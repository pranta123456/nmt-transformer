{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599be6b4-1648-4a6c-8dbb-8bef1c991fcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from de-core-news-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c363bad8-ca47-4f01-8cc2-8fc44a2a92b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Pranta/transformer\n"
     ]
    }
   ],
   "source": [
    "cd transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee437a5-d732-444f-a3e3-33007130f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/neurologic/neuro_env/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/fsx/neurologic/neuro_env/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from model.transformer import Transformer\n",
    "from data import *\n",
    "from config import *\n",
    "from utils.bleu import combined_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef7f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8228a92-22ad-4d22-92f7-f4bb7dd70d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/neurologic/pranta_root/transformer/model/transformer.py:61: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    d_model=d_model,\n",
    "                    d_hidden=d_hidden,\n",
    "                    n_layers=n_layers,\n",
    "                    h=n_heads,\n",
    "                    max_len=max_len,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=torch.device(\"cuda:3\")).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1d0969-69b8-4845-9730-2b61f774355d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54552112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52f8df2-0c2a-481a-b9e6-691c06fe1d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0aab03-5a0f-42d0-85bb-18d26fec5918",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(8015, 512)\n",
       "      (pos_encoding): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_T): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(6192, 512)\n",
       "      (pos_encoding): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadSelfAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_T): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm()\n",
       "        (enc_dec_attention): MultiHeadSelfAttention(\n",
       "          (W_Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_K): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_V): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_T): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm()\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=6192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1609e2f-b5bf-4b28-b0a0-fb356ed8caa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 betas=(0.9, 0.98),\n",
    "                 eps=1e-9)\n",
    "\n",
    "def rate(step, d_model, factor, warmup_steps):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "          d_model ** (-0.5) * min(step ** (-0.5), warmup_steps * step ** (-1.5))\n",
    "      )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                              lr_lambda=lambda step : rate(step, d_model=512, factor=1.0, warmup_steps=4000))\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, dec_voc_size, padding_idx, smoothing=0.1):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.size = dec_voc_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.eps = smoothing\n",
    "        self.conf = 1 - smoothing\n",
    "        self.criterion = nn.CrossEntropyLoss() #nn.KLDivLoss(reduction='sum')\n",
    "        self.true_dist = None\n",
    "            \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.eps/(self.size - 2))\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), self.conf)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "              true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37837c01-91a4-4ba3-b718-d2a9ab3e36d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss = LabelSmoothing(dec_voc_size, en_vocab['<pad>'])\n",
    "Loss = nn.CrossEntropyLoss(ignore_index = trg_pad_idx, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60614876-368b-4783-8fe3-f1b15e46ebd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(iterator):\n",
    "        src = batch[0].T.to(device)\n",
    "        trg = batch[1].T.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # print('step :', round((i / len(iterator)) *  100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator), lr\n",
    "\n",
    "# val_batch_size=6\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            src = batch[0].T.to(device)\n",
    "            trg = batch[1].T.to(device)\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg_reshape = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg_reshape)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # total_bleu = []\n",
    "            # for j in range(val_batch_size):\n",
    "            #     try:\n",
    "            #         trg_words = idx_to_word(trg[j], trg_itos)\n",
    "            #         output_words = output[j].max(dim=1)[1]      # Greedy Decoding\n",
    "            #         output_words = idx_to_word(output_words, trg_itos)\n",
    "            #         bleu = avg_bleu(trg_words.split(), output_words.split())\n",
    "            #         total_bleu.append(bleu)\n",
    "            #     except:\n",
    "            #       pass\n",
    "\n",
    "            # total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            # batch_bleu.append(total_bleu)\n",
    "\n",
    "    # epoch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2850a3b6-ac6e-43be-8035-b59d6a749c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def8bf3d-8cb0-40ec-9ad3-65c23a844629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_last_5_checkpoints(step):\n",
    "    if step in range(459, 500, 10):\n",
    "        torch.save(model.state_dict(), f'./saved_model/model_epoch_{step+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e367c6-f600-4df1-9fd4-f91014f4cc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def idx_to_words(x, itos):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        word = itos[i.item()]\n",
    "        if '<' not in word:\n",
    "            words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15634477-329b-40e5-a8e7-4e5038a24dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(total_epoch, best_loss):\n",
    "    train_losses, test_losses = [], []\n",
    "    for step in range(total_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss, lr = train(model, train_iter, optimizer, Loss, clip)\n",
    "        valid_loss = evaluate(model, val_iter, Loss)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # if step > 3:\n",
    "        #     scheduler.step()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "        # bleus.append(bleu)\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), './saved_model/best_model.pt')\n",
    "\n",
    "        f = open('./result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "\n",
    "        # f = open('/content/drive/MyDrive/transformer/result/bleu.txt', 'w')\n",
    "        # f.write(str(bleus))\n",
    "        # f.close()\n",
    "\n",
    "        f = open('./result/test_loss.txt', 'w')\n",
    "        f.write(str(test_losses))\n",
    "        f.close()\n",
    "        \n",
    "        save_last_5_checkpoints(step)\n",
    "\n",
    "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | lr: {lr}')\n",
    "        print(f'\\tVal Loss: {valid_loss:.3f} ')\n",
    "        # print(f'\\tBLEU Score: {bleu:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e5065-434d-4c91-87e9-e0d10183801f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 1m 25s\n",
      "\tTrain Loss: 7.197 | lr: 2.595171245429374e-06\n",
      "\tVal Loss: 6.677 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 1m 25s\n",
      "\tTrain Loss: 6.597 | lr: 1.8350631859834486e-06\n",
      "\tVal Loss: 6.313 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Time: 1m 25s\n",
      "\tTrain Loss: 6.322 | lr: 1.4983228171418255e-06\n",
      "\tVal Loss: 6.114 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Time: 1m 24s\n",
      "\tTrain Loss: 6.143 | lr: 1.297585622714687e-06\n",
      "\tVal Loss: 5.973 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Time: 1m 24s\n",
      "\tTrain Loss: 6.013 | lr: 1.1605958636065741e-06\n",
      "\tVal Loss: 5.861 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Time: 1m 25s\n",
      "\tTrain Loss: 5.911 | lr: 1.0594742244075163e-06\n",
      "\tVal Loss: 5.773 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Time: 1m 25s\n",
      "\tTrain Loss: 5.829 | lr: 9.808825321474133e-07\n",
      "\tVal Loss: 5.695 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Time: 1m 25s\n",
      "\tTrain Loss: 5.762 | lr: 9.175315929917243e-07\n",
      "\tVal Loss: 5.642 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Time: 1m 25s\n",
      "\tTrain Loss: 5.706 | lr: 8.650570818097915e-07\n",
      "\tVal Loss: 5.592 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 1m 25s\n",
      "\tTrain Loss: 5.660 | lr: 8.206652053732661e-07\n",
      "\tVal Loss: 5.550 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 1m 25s\n",
      "\tTrain Loss: 5.621 | lr: 7.824735716189601e-07\n",
      "\tVal Loss: 5.524 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 1m 25s\n",
      "\tTrain Loss: 5.589 | lr: 7.491614085709127e-07\n",
      "\tVal Loss: 5.493 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Time: 1m 24s\n",
      "\tTrain Loss: 5.561 | lr: 7.197709995388732e-07\n",
      "\tVal Loss: 5.467 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Time: 1m 25s\n",
      "\tTrain Loss: 5.538 | lr: 6.833386108658795e-07\n",
      "\tVal Loss: 5.446 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Time: 1m 24s\n",
      "\tTrain Loss: 5.519 | lr: 6.161566292171235e-07\n",
      "\tVal Loss: 5.432 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Time: 1m 25s\n",
      "\tTrain Loss: 5.503 | lr: 5.593041477218479e-07\n",
      "\tVal Loss: 5.421 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Time: 1m 25s\n",
      "\tTrain Loss: 5.490 | lr: 5.106867992588496e-07\n",
      "\tVal Loss: 5.403 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Time: 1m 25s\n",
      "\tTrain Loss: 5.479 | lr: 4.687262288591184e-07\n",
      "\tVal Loss: 5.398 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Time: 1m 25s\n",
      "\tTrain Loss: 5.470 | lr: 4.322127882322756e-07\n",
      "\tVal Loss: 5.397 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Time: 1m 25s\n",
      "\tTrain Loss: 5.461 | lr: 4.0020547020916355e-07\n",
      "\tVal Loss: 5.382 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Time: 1m 25s\n",
      "\tTrain Loss: 5.443 | lr: 3.2451610891662667e-07\n",
      "\tVal Loss: 5.365 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Time: 1m 24s\n",
      "\tTrain Loss: 5.439 | lr: 3.044466162090564e-07\n",
      "\tVal Loss: 5.361 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Time: 1m 25s\n",
      "\tTrain Loss: 5.434 | lr: 2.8636372363358613e-07\n",
      "\tVal Loss: 5.356 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Time: 1m 25s\n",
      "\tTrain Loss: 5.431 | lr: 2.7000262847499015e-07\n",
      "\tVal Loss: 5.355 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Time: 1m 24s\n",
      "\tTrain Loss: 5.427 | lr: 2.551422421697447e-07\n",
      "\tVal Loss: 5.350 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Time: 1m 25s\n",
      "\tTrain Loss: 5.424 | lr: 2.415966827949294e-07\n",
      "\tVal Loss: 5.351 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Time: 1m 25s\n",
      "\tTrain Loss: 5.421 | lr: 2.2920865733525254e-07\n",
      "\tVal Loss: 5.348 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Time: 1m 25s\n",
      "\tTrain Loss: 5.418 | lr: 2.1784426539623662e-07\n",
      "\tVal Loss: 5.343 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Time: 1m 25s\n",
      "\tTrain Loss: 5.415 | lr: 2.0738888335775054e-07\n",
      "\tVal Loss: 5.342 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Time: 1m 24s\n",
      "\tTrain Loss: 5.413 | lr: 1.9774387779994057e-07\n",
      "\tVal Loss: 5.339 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Time: 1m 25s\n",
      "\tTrain Loss: 5.410 | lr: 1.8882396121545433e-07\n",
      "\tVal Loss: 5.337 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Time: 1m 25s\n",
      "\tTrain Loss: 5.409 | lr: 1.805550494091928e-07\n",
      "\tVal Loss: 5.335 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Time: 1m 25s\n",
      "\tTrain Loss: 5.407 | lr: 1.7287251387773315e-07\n",
      "\tVal Loss: 5.330 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Time: 1m 25s\n",
      "\tTrain Loss: 5.404 | lr: 1.6571974747314012e-07\n",
      "\tVal Loss: 5.332 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 74.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Time: 1m 25s\n",
      "\tTrain Loss: 5.403 | lr: 1.5904698029173232e-07\n",
      "\tVal Loss: 5.328 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Time: 1m 25s\n",
      "\tTrain Loss: 5.345 | lr: 1.546218769887548e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176 | Time: 1m 25s\n",
      "\tTrain Loss: 5.346 | lr: 1.533059505523041e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177 | Time: 1m 25s\n",
      "\tTrain Loss: 5.345 | lr: 1.520085843455401e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178 | Time: 1m 24s\n",
      "\tTrain Loss: 5.345 | lr: 1.507294139341474e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 62.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179 | Time: 1m 25s\n",
      "\tTrain Loss: 5.345 | lr: 1.4946808403272287e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 | Time: 1m 25s\n",
      "\tTrain Loss: 5.345 | lr: 1.4822424822561613e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:24<00:00,  3.42it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 56.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181 | Time: 1m 27s\n",
      "\tTrain Loss: 5.344 | lr: 1.469975686977803e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.47it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182 | Time: 1m 25s\n",
      "\tTrain Loss: 5.345 | lr: 1.4578771597522263e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:03<00:00, 47.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183 | Time: 1m 26s\n",
      "\tTrain Loss: 5.344 | lr: 1.4459436867466094e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.4341721326201183e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.47it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 60.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Time: 1m 26s\n",
      "\tTrain Loss: 5.345 | lr: 1.4225594381935311e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.4111026182001858e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.3997987591150016e-08\n",
      "\tVal Loss: 5.259 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 188 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.3886450170584598e-08\n",
      "\tVal Loss: 5.260 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.377638615772574e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.47it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 59.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Time: 1m 26s\n",
      "\tTrain Loss: 5.344 | lr: 1.3667768446660118e-08\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.3560570569256557e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192 | Time: 1m 25s\n",
      "\tTrain Loss: 5.344 | lr: 1.3454766676920128e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.335033152295994e-08\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.3247240445546929e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:55<00:00,  2.51it/s]\n",
      "100%|██████████| 169/169 [00:03<00:00, 53.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Time: 1m 58s\n",
      "\tTrain Loss: 5.343 | lr: 1.3145469351239005e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.3044994699051845e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.2945793485054601e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2847843227470668e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2751121952264486e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.2655608179196196e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.2561280908326692e-08\n",
      "\tVal Loss: 5.259 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2468119606956375e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203 | Time: 1m 25s\n",
      "\tTrain Loss: 5.343 | lr: 1.2376104196981613e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2285215042653546e-08\n",
      "\tVal Loss: 5.258 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2195432938724593e-08\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.210673909896846e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.47it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.2019115145060246e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.1932543095803597e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 69.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209 | Time: 1m 25s\n",
      "\tTrain Loss: 5.341 | lr: 1.1847005356692511e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 68.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210 | Time: 1m 25s\n",
      "\tTrain Loss: 5.342 | lr: 1.1762484709795832e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:53<00:00,  2.56it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211 | Time: 1m 57s\n",
      "\tTrain Loss: 5.342 | lr: 1.1678964303952994e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212 | Time: 3m 12s\n",
      "\tTrain Loss: 5.340 | lr: 1.1596427645270024e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:07<00:00,  1.55it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 213 | Time: 3m 12s\n",
      "\tTrain Loss: 5.342 | lr: 1.1514858587905207e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 | Time: 3m 12s\n",
      "\tTrain Loss: 5.341 | lr: 1.1434241325134312e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:06<00:00,  1.56it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215 | Time: 3m 10s\n",
      "\tTrain Loss: 5.341 | lr: 1.1354560380685626e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216 | Time: 3m 12s\n",
      "\tTrain Loss: 5.342 | lr: 1.127580060033542e-08\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:07<00:00,  1.55it/s]\n",
      "100%|██████████| 169/169 [00:05<00:00, 33.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217 | Time: 3m 12s\n",
      "\tTrain Loss: 5.341 | lr: 1.1197947143754895e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:07<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218 | Time: 3m 12s\n",
      "\tTrain Loss: 5.341 | lr: 1.112098547659992e-08\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:05<00:00, 33.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219 | Time: 3m 13s\n",
      "\tTrain Loss: 5.340 | lr: 1.1044901362835297e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 | Time: 3m 12s\n",
      "\tTrain Loss: 5.341 | lr: 1.0969680857285547e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:08<00:00,  1.54it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 36.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221 | Time: 3m 12s\n",
      "\tTrain Loss: 5.340 | lr: 1.0895310298404538e-08\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [03:07<00:00,  1.55it/s]\n",
      "100%|██████████| 169/169 [00:04<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222 | Time: 3m 12s\n",
      "\tTrain Loss: 5.340 | lr: 1.0821776301256572e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:47<00:00,  2.70it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 | Time: 1m 49s\n",
      "\tTrain Loss: 5.340 | lr: 1.0749065750701837e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Time: 1m 25s\n",
      "\tTrain Loss: 5.341 | lr: 1.0677165794779367e-08\n",
      "\tVal Loss: 5.255 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0606063838280967e-08\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0535747536509737e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0466204789217122e-08\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228 | Time: 1m 25s\n",
      "\tTrain Loss: 5.341 | lr: 1.0397423734712626e-08\n",
      "\tVal Loss: 5.257 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.032939274414053e-08\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 230 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.026210041591817e-08\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 231 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0195535570330565e-08\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.01296872442763e-08\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 233 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0064544686159837e-08\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 1.0000097350925563e-08\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235 | Time: 1m 24s\n",
      "\tTrain Loss: 5.340 | lr: 9.936334895229027e-09\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 236 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.873247172741047e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.810824229580465e-09\n",
      "\tVal Loss: 5.254 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.749056299871504e-09\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 9.687933801421837e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.627447331517554e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241 | Time: 1m 25s\n",
      "\tTrain Loss: 5.340 | lr: 9.567587662831467e-09\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.50834573944117e-09\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.449712672953508e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.39167973873219e-09\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.334238372225389e-09\n",
      "\tVal Loss: 5.253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 9.277380165390247e-09\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.22109686321141e-09\n",
      "\tVal Loss: 5.248 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 9.165380360310708e-09\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 9.110222697645237e-09\n",
      "\tVal Loss: 5.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 9.055616059291211e-09\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 9.001552769311038e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 252 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 8.948025288701088e-09\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.895026212417827e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 | Time: 1m 25s\n",
      "\tTrain Loss: 5.339 | lr: 8.842548266479965e-09\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 255 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.790584305144419e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.739127308153874e-09\n",
      "\tVal Loss: 5.248 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.688170378053925e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.637706737577728e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.587729727096222e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.538232802132043e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.48920953093528e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262 | Time: 1m 24s\n",
      "\tTrain Loss: 5.337 | lr: 8.440653592119336e-09\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 263 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.392558772355139e-09\n",
      "\tVal Loss: 5.252 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.344918964122087e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 265 | Time: 1m 25s\n",
      "\tTrain Loss: 5.338 | lr: 8.297728163514109e-09\n",
      "\tVal Loss: 5.248 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266 | Time: 1m 24s\n",
      "\tTrain Loss: 5.337 | lr: 8.250980468099261e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.20467007483142e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.158791278012539e-09\n",
      "\tVal Loss: 5.251 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.113338467304128e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.068306125786543e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 8.023688828064782e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 272 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.979481238419525e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.935678109002113e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.892274278072369e-09\n",
      "\tVal Loss: 5.250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 | Time: 1m 24s\n",
      "\tTrain Loss: 5.337 | lr: 7.849264668277967e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 69.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.806644284974346e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 69.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.764408214583957e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.72255162299389e-09\n",
      "\tVal Loss: 5.244 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 69.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.68106975399076e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 68.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.639957927731912e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 69.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.599211539251964e-09\n",
      "\tVal Loss: 5.249 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 282 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.558826057003707e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.518797021432547e-09\n",
      "\tVal Loss: 5.244 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.479120043583489e-09\n",
      "\tVal Loss: 5.243 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.4397908037398956e-09\n",
      "\tVal Loss: 5.245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.400805050093151e-09\n",
      "\tVal Loss: 5.248 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 287 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.362158597442418e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.323847325923725e-09\n",
      "\tVal Loss: 5.248 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Time: 1m 25s\n",
      "\tTrain Loss: 5.337 | lr: 7.2858671797676094e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.248214166084583e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 68.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 291 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.210884353677703e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 60.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 292 | Time: 1m 26s\n",
      "\tTrain Loss: 5.336 | lr: 7.173873871881552e-09\n",
      "\tVal Loss: 5.245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.137178909426952e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Time: 1m 25s\n",
      "\tTrain Loss: 5.335 | lr: 7.100795713330762e-09\n",
      "\tVal Loss: 5.240 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 71.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 7.064720587810109e-09\n",
      "\tVal Loss: 5.245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296 | Time: 1m 25s\n",
      "\tTrain Loss: 5.335 | lr: 7.0289498932204444e-09\n",
      "\tVal Loss: 5.244 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 6.9934800450168276e-09\n",
      "\tVal Loss: 5.243 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298 | Time: 1m 25s\n",
      "\tTrain Loss: 5.336 | lr: 6.9583075127378376e-09\n",
      "\tVal Loss: 5.244 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.52it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Time: 1m 24s\n",
      "\tTrain Loss: 5.335 | lr: 6.923428819011562e-09\n",
      "\tVal Loss: 5.243 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 | Time: 1m 24s\n",
      "\tTrain Loss: 5.336 | lr: 6.888840538583105e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.50it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301 | Time: 1m 25s\n",
      "\tTrain Loss: 5.335 | lr: 6.8545392973630735e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 302 | Time: 1m 24s\n",
      "\tTrain Loss: 5.337 | lr: 6.820521771496524e-09\n",
      "\tVal Loss: 5.247 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 73.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 | Time: 1m 25s\n",
      "\tTrain Loss: 5.335 | lr: 6.786784686451883e-09\n",
      "\tVal Loss: 5.246 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "100%|██████████| 169/169 [00:02<00:00, 72.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Time: 1m 24s\n",
      "\tTrain Loss: 5.335 | lr: 6.753324816129306e-09\n",
      "\tVal Loss: 5.245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 162/290 [00:45<00:37,  3.42it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run(total_epoch=500, best_loss=inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f25b789-c9e4-4a11-ba8e-7ecd3ab123e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./result/test_loss.txt') as g:\n",
    "    data = g.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24fc42c-d653-4a69-896e-f0327234862f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c517fa3-fbbc-4100-9cfc-6358d92349ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"epoch\" : [i for i in range(1, 101)],\n",
    "        \"Loss\" : [float(i) for i in data[1:-1].split(\",\")]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17d7d3b-096b-44a3-882c-32dcd861f88d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='epoch'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cross Entropy without label smoothing\n",
    "df.plot(x=\"epoch\", y=\"Loss\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e110331c-ebda-48ef-a728-1416b0b6cffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./logs/test_run\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb091cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4193ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines = data.split(\"\\n\")\n",
    "train_loss, val_loss = [], []\n",
    "for idx in range(0, len(lines[:-1]), 8):\n",
    "    candidate = lines[idx: idx + 8]\n",
    "    tmp_train_loss = 0.0\n",
    "    tmp_val_loss = 0.0\n",
    "    for l in candidate:\n",
    "        tmp = l.split()\n",
    "        tmp_train_loss += float(tmp[3])\n",
    "        tmp_val_loss += float(tmp[5])\n",
    "    train_loss.append(tmp_train_loss / 8)\n",
    "    val_loss.append(tmp_val_loss / 8)\n",
    "\n",
    "plt.plot(list(range(1, 1001)), train_loss, label=\"train_loss\")\n",
    "plt.plot(list(range(1, 1001)), val_loss, label=\"val_loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53c790-093d-49df-9a76-3232b33c5f1f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_state_dict(model_path):\n",
    "    state_dict = torch.load(model_path)\n",
    "    clean_state_dict = {k.replace('module.', ''):v for k, v in state_dict.items()}\n",
    "    return clean_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925c3a1b-aca6-462a-9309-48d8f91df4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = modified_state_dict('./saved_model/final_model.pt')\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7434333d-6ebd-4739-8649-f9a736a1cda4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 63.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU : 5.423783096773601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu_scores = []\n",
    "model.eval()\n",
    "for batch in tqdm(test_iter):\n",
    "    src = batch[0].T.to(device)\n",
    "    trg = batch[1].T.to(device)\n",
    "    \n",
    "    outputs = model(src, trg[:, :-1])\n",
    "    output_indices = outputs[0].max(dim=1)[1]\n",
    "    candidate_words = idx_to_words(output_indices, trg_itos)\n",
    "    reference_words = idx_to_words(trg[0], trg_itos)\n",
    "    bleu_score = combined_bleu_score(reference_words.split(), candidate_words.split())\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "print(f\"BLEU : {sum(bleu_scores)/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "543b18a7-0e87-4b72-83ae-fea17380cd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ein Mann in einer schwarzen Jacke spielt in der Öffentlichkeit Gitarre . \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_words = idx_to_words(src[0], src_itos)\n",
    "src_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c5acfe-622a-4b76-a2e6-2b341fa04722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A girl wearing black athletic clothing with the number on her jersey runs across some grass . \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_words = idx_to_words(trg[0], trg_itos)\n",
    "trg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27a8630-3cc3-42f6-8723-f0507c50c920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A girl in a shorts shorts is a ball of on her head is down the grass . \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65fe5ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 26,  33,  18,  22, 118,  35,  36,  22,  49,  14,  76,  22,  14,  15,\n",
       "          3], device='cuda:1')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad99cd9-e51a-418d-a51d-2d53e7b17233",
   "metadata": {},
   "source": [
    "## Checkpoint Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b52cee9-c0e2-4edd-8552-639ed0db334d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint1 = torch.load('./saved_model/model_epoch_60.pt')\n",
    "checkpoint2 = torch.load('./saved_model/model_epoch_70.pt')\n",
    "checkpoint3 = torch.load('./saved_model/model_epoch_80.pt')\n",
    "checkpoint4 = torch.load('./saved_model/model_epoch_90.pt')\n",
    "checkpoint5 = torch.load('./saved_model/model_epoch_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d680775-90b2-49ac-aa2a-99ac02d8fd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoints = [\n",
    "    checkpoint1,\n",
    "    checkpoint2,\n",
    "    checkpoint3,\n",
    "    checkpoint4,\n",
    "    checkpoint5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9057e4d-7194-4379-82ca-f79f9f721a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_names = [i for i in checkpoints[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dceb39c-2231-4bf9-a679-dcc8adf981c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkpoint_averaging(checkpoints):\n",
    "    n = len(checkpoints)\n",
    "    for l in checkpoints[0]:\n",
    "        for i in range(1, n):\n",
    "            checkpoints[0][l] += checkpoints[i][l]\n",
    "        checkpoints[0][l] /= 5 \n",
    "            \n",
    "    return checkpoints[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc4c2832-b035-488c-a9be-a77ad2b58bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_checkpoint = checkpoint_averaging(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e432c832-a1ee-4237-b689-38f9044f32b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.emb.token_emb.weight',\n",
       "              tensor([[ 0.1312,  0.1490, -0.1369,  ...,  0.1302, -0.1701, -0.1484],\n",
       "                      [ 0.0220, -0.0051, -0.0232,  ..., -0.0183,  0.0242,  0.0103],\n",
       "                      [ 0.1497,  0.1667,  0.1270,  ...,  0.1277, -0.1487,  0.1565],\n",
       "                      ...,\n",
       "                      [ 0.0176, -0.0148,  0.0084,  ...,  0.0236, -0.0034,  0.0160],\n",
       "                      [ 0.0059, -0.0002, -0.0263,  ..., -0.0255,  0.0024,  0.0092],\n",
       "                      [-0.0128, -0.0261, -0.0141,  ..., -0.0227,  0.0129,  0.0123]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.attention.W_Q.weight',\n",
       "              tensor([[-0.1981,  0.1201, -0.0984,  ..., -0.1208,  0.1695, -0.1017],\n",
       "                      [ 0.2132, -0.0798,  0.1896,  ...,  0.0898,  0.2055,  0.2268],\n",
       "                      [ 0.1804,  0.1908,  0.0882,  ...,  0.1052, -0.1688,  0.1715],\n",
       "                      ...,\n",
       "                      [-0.0897,  0.0778, -0.0967,  ..., -0.0970,  0.2061, -0.1986],\n",
       "                      [ 0.1998,  0.2258,  0.1321,  ...,  0.1688,  0.0949,  0.0804],\n",
       "                      [-0.0884, -0.0956, -0.1556,  ..., -0.1541,  0.0997, -0.1243]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.attention.W_K.weight',\n",
       "              tensor([[-0.2028,  0.1881, -0.2038,  ...,  0.1517,  0.1252, -0.1162],\n",
       "                      [-0.1386,  0.0987, -0.2155,  ...,  0.1916,  0.1441, -0.1370],\n",
       "                      [-0.1251,  0.1697, -0.2225,  ..., -0.1987, -0.1257, -0.1166],\n",
       "                      ...,\n",
       "                      [ 0.2213, -0.1570,  0.2007,  ...,  0.1893, -0.1004, -0.1865],\n",
       "                      [ 0.2198, -0.2166,  0.1309,  ...,  0.1332, -0.1437, -0.1768],\n",
       "                      [ 0.1580,  0.1377, -0.1413,  ..., -0.2238,  0.1651,  0.1955]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.attention.W_V.weight',\n",
       "              tensor([[ 0.1551,  0.1052,  0.1039,  ...,  0.1704, -0.1592,  0.1859],\n",
       "                      [-0.1725, -0.1118,  0.2084,  ..., -0.1209,  0.1877, -0.2108],\n",
       "                      [-0.2174, -0.1943, -0.1354,  ..., -0.2203,  0.0950, -0.1258],\n",
       "                      ...,\n",
       "                      [ 0.2125,  0.1235,  0.2039,  ...,  0.1836, -0.1841,  0.1721],\n",
       "                      [ 0.1377, -0.1730,  0.1933,  ...,  0.1507, -0.1011,  0.0779],\n",
       "                      [ 0.1131, -0.2171,  0.1291,  ..., -0.1630,  0.2128, -0.1269]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.attention.W_T.weight',\n",
       "              tensor([[-0.1387,  0.0777,  0.1957,  ..., -0.1184,  0.1634,  0.1377],\n",
       "                      [-0.1090,  0.0753,  0.1293,  ..., -0.0904, -0.1550,  0.1389],\n",
       "                      [-0.0766,  0.1180,  0.0814,  ..., -0.0902,  0.1653,  0.1778],\n",
       "                      ...,\n",
       "                      [ 0.1386, -0.2103, -0.0865,  ...,  0.1300, -0.1660, -0.0788],\n",
       "                      [ 0.2037, -0.2007, -0.1961,  ...,  0.1644, -0.1742, -0.1427],\n",
       "                      [ 0.1587, -0.1054, -0.1148,  ...,  0.1827, -0.1956, -0.0924]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.layer_norm1.gamma',\n",
       "              tensor([0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1521, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1521, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8481, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8479, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1522, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1522, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1514, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1521, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8478, 0.8478, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8478,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.layer_norm1.beta',\n",
       "              tensor([ 0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1519,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1506, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1521,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1522, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1521, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1522, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.ffn.layer1.weight',\n",
       "              tensor([[-0.1162,  0.1522, -0.1789,  ..., -0.1569, -0.1104,  0.1127],\n",
       "                      [-0.1210,  0.1185, -0.1715,  ..., -0.1387, -0.1415,  0.1430],\n",
       "                      [ 0.1232,  0.1132,  0.1547,  ...,  0.1456,  0.1876, -0.1056],\n",
       "                      ...,\n",
       "                      [ 0.1519,  0.1195,  0.1141,  ...,  0.1252,  0.1376, -0.1368],\n",
       "                      [-0.1347,  0.1370, -0.1874,  ..., -0.1317, -0.1918,  0.1170],\n",
       "                      [-0.1845,  0.1787, -0.1298,  ..., -0.1156, -0.1823,  0.1633]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.ffn.layer1.bias',\n",
       "              tensor([ 0.1166,  0.1100, -0.1899,  ..., -0.1504,  0.1917,  0.1736],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.ffn.layer2.weight',\n",
       "              tensor([[ 0.1479, -0.1310, -0.1068,  ...,  0.1688, -0.1093, -0.1279],\n",
       "                      [ 0.1738, -0.1160, -0.1478,  ..., -0.1065,  0.1726,  0.1587],\n",
       "                      [ 0.1118,  0.1775,  0.1622,  ...,  0.1128,  0.1253,  0.1556],\n",
       "                      ...,\n",
       "                      [-0.1608, -0.1989, -0.1690,  ..., -0.1329, -0.1081, -0.1959],\n",
       "                      [-0.1310, -0.1641, -0.1202,  ..., -0.1091, -0.1614, -0.1507],\n",
       "                      [-0.1382, -0.1885, -0.1575,  ..., -0.1725, -0.1080, -0.1675]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.ffn.layer2.bias',\n",
       "              tensor([-0.1476, -0.1519,  0.1634, -0.1461,  0.1311, -0.1374, -0.1683, -0.1470,\n",
       "                       0.1588, -0.1380, -0.1490,  0.1455,  0.1435, -0.1432, -0.1319, -0.1416,\n",
       "                       0.1493,  0.1718,  0.1592, -0.1475,  0.1548,  0.1735,  0.1655,  0.1396,\n",
       "                      -0.1409, -0.1466, -0.1570, -0.1735,  0.1428,  0.1621, -0.1657, -0.1526,\n",
       "                       0.1604,  0.1635,  0.1734, -0.1630, -0.1591, -0.1627, -0.1560,  0.1680,\n",
       "                       0.1574, -0.1651, -0.1714,  0.1605,  0.1530, -0.1522,  0.1590, -0.1433,\n",
       "                       0.1598,  0.1683,  0.1613,  0.1545, -0.1379, -0.1699,  0.1647,  0.1741,\n",
       "                      -0.1475, -0.1355,  0.1543,  0.1485, -0.1736,  0.1354,  0.1494, -0.1358,\n",
       "                       0.1505,  0.1605,  0.1655,  0.1443,  0.1590,  0.1438,  0.1710, -0.1544,\n",
       "                       0.1407, -0.1660,  0.1314,  0.1516, -0.1515, -0.1598, -0.1645,  0.1541,\n",
       "                      -0.1466, -0.1652,  0.1508,  0.1319, -0.1416,  0.1445, -0.1554, -0.1370,\n",
       "                       0.1665, -0.1527,  0.1566, -0.1384, -0.1418,  0.1488, -0.1543, -0.1390,\n",
       "                      -0.1558,  0.1595, -0.1506, -0.1365,  0.1381, -0.1312, -0.1341, -0.1459,\n",
       "                      -0.1352, -0.1432,  0.1598,  0.1382,  0.1349, -0.1707,  0.1506,  0.1562,\n",
       "                      -0.1582, -0.1376, -0.1587, -0.1539,  0.1685, -0.1411,  0.1729,  0.1362,\n",
       "                       0.1436,  0.1585, -0.1331, -0.1306, -0.1419,  0.1369,  0.1468,  0.1348,\n",
       "                       0.1605, -0.1523,  0.1601,  0.1654,  0.1459, -0.1714,  0.1640, -0.1735,\n",
       "                       0.1525,  0.1702,  0.1636,  0.1424,  0.1516, -0.1687,  0.1354,  0.1383,\n",
       "                       0.1355, -0.1609, -0.1457,  0.1652, -0.1513, -0.1329, -0.1663, -0.1557,\n",
       "                      -0.1463,  0.1496, -0.1454,  0.1513,  0.1378, -0.1443,  0.1425,  0.1406,\n",
       "                      -0.1698,  0.1575, -0.1744,  0.1472, -0.1315,  0.1412,  0.1593,  0.1599,\n",
       "                       0.1547, -0.1732, -0.1559, -0.1482, -0.1313,  0.1343,  0.1716, -0.1366,\n",
       "                      -0.1594, -0.1677,  0.1383, -0.1732,  0.1597, -0.1427, -0.1622, -0.1643,\n",
       "                       0.1694, -0.1672, -0.1598, -0.1568, -0.1318, -0.1720, -0.1656,  0.1361,\n",
       "                      -0.1341,  0.1678, -0.1680, -0.1483, -0.1440,  0.1333,  0.1374,  0.1441,\n",
       "                      -0.1394,  0.1460,  0.1381,  0.1346,  0.1468, -0.1656, -0.1554, -0.1463,\n",
       "                      -0.1566, -0.1684,  0.1706,  0.1522,  0.1723,  0.1352,  0.1434, -0.1716,\n",
       "                      -0.1645, -0.1511, -0.1457, -0.1687,  0.1586, -0.1641, -0.1613,  0.1683,\n",
       "                       0.1552, -0.1685, -0.1573,  0.1372, -0.1547, -0.1620,  0.1320, -0.1318,\n",
       "                       0.1687,  0.1667,  0.1460, -0.1426,  0.1524, -0.1440,  0.1339, -0.1498,\n",
       "                       0.1475,  0.1578, -0.1526, -0.1585,  0.1610,  0.1381,  0.1352,  0.1716,\n",
       "                      -0.1347, -0.1564, -0.1554, -0.1710,  0.1721, -0.1389, -0.1390,  0.1390,\n",
       "                      -0.1725, -0.1689,  0.1395,  0.1349,  0.1593,  0.1712, -0.1436,  0.1461,\n",
       "                      -0.1370,  0.1432,  0.1430,  0.1635,  0.1379,  0.1334,  0.1564, -0.1519,\n",
       "                      -0.1483,  0.1304,  0.1704, -0.1537,  0.1700, -0.1602, -0.1421,  0.1443,\n",
       "                       0.1545, -0.1339,  0.1710, -0.1409, -0.1672, -0.1521, -0.1516,  0.1403,\n",
       "                       0.1489, -0.1390,  0.1626, -0.1525,  0.1313,  0.1598, -0.1694,  0.1475,\n",
       "                      -0.1501,  0.1492,  0.1315, -0.1544, -0.1632,  0.1624, -0.1369, -0.1673,\n",
       "                       0.1376, -0.1634,  0.1633, -0.1330, -0.1338, -0.1385,  0.1632,  0.1589,\n",
       "                      -0.1409, -0.1416,  0.1590, -0.1699, -0.1471, -0.1704, -0.1621,  0.1367,\n",
       "                       0.1666, -0.1489, -0.1561, -0.1516,  0.1675,  0.1321,  0.1567, -0.1613,\n",
       "                       0.1567, -0.1575, -0.1731, -0.1574,  0.1692, -0.1442, -0.1660, -0.1325,\n",
       "                      -0.1351, -0.1481,  0.1352,  0.1654,  0.1544,  0.1339,  0.1465,  0.1468,\n",
       "                      -0.1351,  0.1489,  0.1442, -0.1559,  0.1610,  0.1693, -0.1587,  0.1327,\n",
       "                       0.1457,  0.1459,  0.1324,  0.1496,  0.1378,  0.1489,  0.1361, -0.1477,\n",
       "                       0.1440,  0.1472,  0.1442, -0.1472,  0.1315,  0.1694,  0.1511,  0.1737,\n",
       "                      -0.1403, -0.1596,  0.1692,  0.1489, -0.1529, -0.1633,  0.1593, -0.1622,\n",
       "                      -0.1732,  0.1544, -0.1586,  0.1529, -0.1432,  0.1624,  0.1409,  0.1708,\n",
       "                       0.1723,  0.1391, -0.1460, -0.1670, -0.1553,  0.1350, -0.1369,  0.1590,\n",
       "                      -0.1351, -0.1650, -0.1325,  0.1434,  0.1620,  0.1581, -0.1531,  0.1395,\n",
       "                      -0.1589, -0.1312, -0.1654, -0.1684, -0.1696,  0.1582, -0.1675, -0.1448,\n",
       "                       0.1369, -0.1348,  0.1385, -0.1679,  0.1461,  0.1551,  0.1421, -0.1450,\n",
       "                       0.1558,  0.1446, -0.1353,  0.1541,  0.1446, -0.1440, -0.1498,  0.1350,\n",
       "                      -0.1467,  0.1486, -0.1610, -0.1657, -0.1359,  0.1738, -0.1378,  0.1327,\n",
       "                       0.1415, -0.1342, -0.1626,  0.1508, -0.1551,  0.1734,  0.1485,  0.1365,\n",
       "                      -0.1686, -0.1734, -0.1455,  0.1720,  0.1303,  0.1332, -0.1466,  0.1514,\n",
       "                       0.1688,  0.1645, -0.1632,  0.1424, -0.1544, -0.1399, -0.1491,  0.1581,\n",
       "                       0.1634,  0.1476,  0.1567, -0.1666, -0.1345,  0.1642,  0.1434, -0.1735,\n",
       "                      -0.1532,  0.1439,  0.1602, -0.1743,  0.1462, -0.1561, -0.1693, -0.1378,\n",
       "                      -0.1589,  0.1492,  0.1590, -0.1372,  0.1405, -0.1380,  0.1515,  0.1497,\n",
       "                      -0.1314, -0.1666,  0.1644, -0.1549,  0.1660, -0.1718,  0.1557,  0.1361,\n",
       "                       0.1588, -0.1433,  0.1512,  0.1384,  0.1508, -0.1725,  0.1653, -0.1334,\n",
       "                      -0.1543,  0.1607, -0.1427,  0.1397,  0.1516,  0.1533, -0.1496, -0.1397,\n",
       "                      -0.1492,  0.1727,  0.1618,  0.1486, -0.1372, -0.1319, -0.1594, -0.1685],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.layer_norm2.gamma',\n",
       "              tensor([1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1522, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8478, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8478,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1485, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1522, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1522, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1520, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8479, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.0.layer_norm2.beta',\n",
       "              tensor([-0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1525, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1522,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1520, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.attention.W_Q.weight',\n",
       "              tensor([[ 0.0829, -0.1526,  0.1262,  ...,  0.1871,  0.1278, -0.0906],\n",
       "                      [-0.0785,  0.1149, -0.2224,  ..., -0.1876, -0.1504,  0.1064],\n",
       "                      [ 0.1210, -0.1710,  0.2036,  ...,  0.1509,  0.2168, -0.1299],\n",
       "                      ...,\n",
       "                      [ 0.1279, -0.0947,  0.1503,  ...,  0.1411,  0.1199, -0.2184],\n",
       "                      [ 0.2070,  0.1500,  0.1022,  ...,  0.1890,  0.0952,  0.1752],\n",
       "                      [ 0.1910,  0.0937,  0.1866,  ...,  0.1851,  0.1905, -0.0778]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.attention.W_K.weight',\n",
       "              tensor([[-0.1893, -0.2284,  0.2184,  ..., -0.1514, -0.1010,  0.1524],\n",
       "                      [-0.2175,  0.1365, -0.1663,  ...,  0.1261,  0.1801, -0.2118],\n",
       "                      [-0.1952, -0.2214,  0.1309,  ..., -0.1799, -0.1142,  0.1420],\n",
       "                      ...,\n",
       "                      [ 0.1832, -0.2169,  0.1009,  ..., -0.1965, -0.1532, -0.1131],\n",
       "                      [ 0.1530,  0.1372, -0.1847,  ...,  0.1327,  0.1739,  0.1823],\n",
       "                      [ 0.1622,  0.2167, -0.0893,  ...,  0.2104,  0.2148, -0.1994]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.attention.W_V.weight',\n",
       "              tensor([[ 0.1664,  0.0833,  0.1314,  ...,  0.1636,  0.1163, -0.1715],\n",
       "                      [-0.2157,  0.2097, -0.1271,  ..., -0.1223, -0.1761,  0.1913],\n",
       "                      [-0.1106,  0.2069, -0.2281,  ..., -0.1725, -0.1491,  0.1405],\n",
       "                      ...,\n",
       "                      [ 0.1745,  0.1541,  0.1483,  ...,  0.0805,  0.1317, -0.0995],\n",
       "                      [-0.1240,  0.1197, -0.1264,  ..., -0.1472, -0.1982,  0.0776],\n",
       "                      [-0.1389, -0.1545,  0.1191,  ...,  0.2101, -0.1543,  0.1138]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.attention.W_T.weight',\n",
       "              tensor([[ 0.2200, -0.1013,  0.1308,  ..., -0.0941,  0.0988, -0.2228],\n",
       "                      [ 0.1845,  0.0838,  0.1133,  ...,  0.0808, -0.0771,  0.1240],\n",
       "                      [-0.1301, -0.1690,  0.2060,  ..., -0.1428,  0.0998, -0.0974],\n",
       "                      ...,\n",
       "                      [ 0.2059,  0.0944, -0.1383,  ...,  0.1776, -0.2226, -0.0839],\n",
       "                      [ 0.2219,  0.1657, -0.1646,  ...,  0.2270, -0.1935, -0.0964],\n",
       "                      [ 0.0868,  0.1768, -0.1793,  ...,  0.1991, -0.1461,  0.1832]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.layer_norm1.gamma',\n",
       "              tensor([0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8472, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1521,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1524, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8480, 0.8478, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8476, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8483, 1.1523, 0.8477, 1.1524, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8475, 0.8477, 0.8477, 0.8476, 0.8477, 1.1525, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8479, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8474, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8478, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1560, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8476, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1526, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8480, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1524, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8481, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8480, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8482, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8476, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1524, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8510, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8478, 0.8476, 0.8477, 1.1523, 0.8478, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1517,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.0951, 1.1523, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.layer_norm1.beta',\n",
       "              tensor([ 0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1524, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1525,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1524,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1522,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1522,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1524,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1522, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1525,  0.1523, -0.1523, -0.1523,  0.1523,  0.1520, -0.1523,  0.1523,\n",
       "                       0.1520, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1517,  0.1524, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1522,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.ffn.layer1.weight',\n",
       "              tensor([[-0.0300,  0.0299, -0.0008,  ...,  0.0050,  0.0218,  0.0156],\n",
       "                      [ 0.1884,  0.1466, -0.1282,  ..., -0.1471,  0.1422, -0.1496],\n",
       "                      [ 0.1966,  0.1195,  0.1863,  ...,  0.1333,  0.1369, -0.1602],\n",
       "                      ...,\n",
       "                      [ 0.1582,  0.1327,  0.1384,  ...,  0.1971,  0.1080, -0.1337],\n",
       "                      [ 0.1046,  0.1573, -0.1260,  ..., -0.1110,  0.1244, -0.1252],\n",
       "                      [ 0.1162,  0.1682, -0.1494,  ..., -0.2001,  0.1801, -0.1287]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.ffn.layer1.bias',\n",
       "              tensor([-0.0263, -0.1380, -0.1642,  ..., -0.1619, -0.1465, -0.1367],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.ffn.layer2.weight',\n",
       "              tensor([[-0.0280,  0.1210,  0.1841,  ..., -0.1339,  0.1255,  0.1676],\n",
       "                      [-0.0109,  0.1107, -0.1399,  ..., -0.1789, -0.1872, -0.1365],\n",
       "                      [ 0.0279, -0.1317,  0.1536,  ...,  0.1944,  0.1261,  0.1228],\n",
       "                      ...,\n",
       "                      [ 0.0190, -0.1692, -0.1191,  ..., -0.1301, -0.1629, -0.1737],\n",
       "                      [-0.0175, -0.1967, -0.1226,  ..., -0.1482, -0.1413, -0.1560],\n",
       "                      [-0.0216,  0.1312, -0.1146,  ..., -0.1111, -0.1626, -0.1658]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.ffn.layer2.bias',\n",
       "              tensor([ 0.1499, -0.1526,  0.1494, -0.1608, -0.1526, -0.1438, -0.1381, -0.1645,\n",
       "                       0.1741, -0.1560, -0.1448, -0.1558,  0.1579, -0.1713,  0.1611,  0.1303,\n",
       "                       0.1459,  0.1378,  0.1477, -0.1310, -0.1714,  0.1603, -0.1472, -0.1416,\n",
       "                       0.1468,  0.1648,  0.1457, -0.1678, -0.1517,  0.1558,  0.1571, -0.1352,\n",
       "                       0.1590,  0.1369,  0.1676, -0.1550,  0.1432, -0.1350, -0.1414,  0.1474,\n",
       "                       0.1640, -0.1738, -0.1716,  0.1649, -0.1724, -0.1650,  0.1678, -0.1645,\n",
       "                       0.1666,  0.1596, -0.1549,  0.1397, -0.1504,  0.1500,  0.1668,  0.1377,\n",
       "                       0.1467, -0.1312,  0.1621,  0.1522,  0.1483, -0.1708,  0.1564,  0.1438,\n",
       "                       0.1637,  0.1496,  0.1514, -0.1429, -0.1454,  0.1352,  0.1395, -0.1478,\n",
       "                       0.1645, -0.1410,  0.1648,  0.1701,  0.1571, -0.1316, -0.1635, -0.1625,\n",
       "                       0.1343, -0.1380,  0.1680,  0.1466, -0.1318,  0.1415, -0.1633, -0.1661,\n",
       "                       0.1713,  0.1436,  0.1350, -0.1558,  0.1559,  0.1325, -0.1659,  0.1452,\n",
       "                      -0.1411,  0.1368, -0.1717, -0.1685,  0.1321,  0.1323,  0.1679, -0.1692,\n",
       "                       0.1578, -0.1455,  0.1588,  0.1734, -0.1591,  0.1555,  0.1615,  0.1398,\n",
       "                       0.1418, -0.1364, -0.1601,  0.1564,  0.1337, -0.1621, -0.1417,  0.1604,\n",
       "                       0.1703,  0.1442,  0.1617,  0.1337,  0.1364,  0.1629,  0.1692, -0.1628,\n",
       "                       0.1460, -0.1700,  0.1717, -0.1525,  0.1568, -0.1457, -0.1420, -0.1429,\n",
       "                       0.1480,  0.1429, -0.1546,  0.1507, -0.1658, -0.1333,  0.1650, -0.1499,\n",
       "                       0.1647, -0.1304, -0.1527,  0.1600, -0.1607,  0.1679, -0.1664,  0.1405,\n",
       "                       0.1537, -0.1694, -0.1645, -0.1570,  0.1606,  0.1532,  0.1565,  0.1349,\n",
       "                       0.1562, -0.1584, -0.1347,  0.1514,  0.1440,  0.1308,  0.1350, -0.1347,\n",
       "                       0.1684,  0.1466, -0.1442, -0.1465, -0.1438,  0.1520, -0.1475,  0.1368,\n",
       "                       0.1531,  0.1655,  0.1698,  0.1548,  0.1435, -0.1549,  0.1648,  0.1392,\n",
       "                       0.1573, -0.1548, -0.1346, -0.1442, -0.1733, -0.1328,  0.1738,  0.1623,\n",
       "                       0.1657,  0.1567,  0.1486, -0.1358, -0.1505, -0.1473, -0.1468, -0.1494,\n",
       "                       0.1375, -0.1625, -0.1673,  0.1595,  0.1433, -0.1537,  0.1607, -0.1625,\n",
       "                      -0.1423, -0.1360,  0.1343,  0.1572,  0.1512, -0.1624, -0.1574, -0.1390,\n",
       "                       0.1342, -0.1320, -0.1693, -0.1647, -0.1553,  0.1350,  0.1373,  0.1313,\n",
       "                      -0.1364, -0.1587, -0.1591,  0.1553, -0.1667, -0.1528,  0.1484, -0.1545,\n",
       "                      -0.1308,  0.1343,  0.1712, -0.1714,  0.1403, -0.1531,  0.1442, -0.1656,\n",
       "                      -0.1418, -0.1663, -0.1392, -0.1417,  0.1692,  0.1719,  0.1490, -0.1614,\n",
       "                       0.1672, -0.1533, -0.1646, -0.1667, -0.1452, -0.1412, -0.1564, -0.1314,\n",
       "                      -0.1453, -0.1303,  0.1418, -0.1429, -0.1469, -0.1709, -0.1730,  0.1474,\n",
       "                       0.1478,  0.1379,  0.1742,  0.1690,  0.1323,  0.1469,  0.1358, -0.1528,\n",
       "                      -0.1714,  0.1717,  0.1536, -0.1351, -0.1437, -0.1592, -0.1438, -0.1322,\n",
       "                       0.1411, -0.1343,  0.1607, -0.1673,  0.1672, -0.1343, -0.1371,  0.1627,\n",
       "                      -0.1720,  0.1316, -0.1395, -0.1410,  0.1422, -0.1366, -0.1568,  0.1327,\n",
       "                      -0.1389,  0.1524,  0.1424, -0.1699, -0.1434,  0.1513, -0.1716,  0.1741,\n",
       "                      -0.1687, -0.1508,  0.1710,  0.1720, -0.1514, -0.1551, -0.1490,  0.1434,\n",
       "                      -0.1673, -0.1546,  0.1729, -0.1334, -0.1553, -0.1330, -0.1445, -0.1522,\n",
       "                       0.1436,  0.1510, -0.1559,  0.1563,  0.1437,  0.1682, -0.1455,  0.1485,\n",
       "                       0.1399, -0.1631,  0.1735, -0.1538,  0.1664, -0.1497, -0.1389, -0.1726,\n",
       "                       0.1403, -0.1382,  0.1310, -0.1387,  0.1389, -0.1695,  0.1619,  0.1344,\n",
       "                      -0.1522, -0.1694,  0.1315, -0.1678, -0.1560, -0.1402, -0.1412, -0.1482,\n",
       "                       0.1306, -0.1417,  0.1465,  0.1737,  0.1654, -0.1667, -0.1448, -0.1628,\n",
       "                       0.1616,  0.1488, -0.1686, -0.1565,  0.1382, -0.1305,  0.1423,  0.1670,\n",
       "                      -0.1675, -0.1680,  0.1468,  0.1515,  0.1696,  0.1663,  0.1700, -0.1494,\n",
       "                      -0.1548, -0.1401,  0.1541,  0.1659,  0.1390,  0.1467, -0.1694,  0.1554,\n",
       "                       0.1372,  0.1341, -0.1619, -0.1598, -0.1706,  0.1380, -0.1601,  0.1388,\n",
       "                      -0.1546,  0.1740, -0.1640, -0.1522,  0.1673, -0.1486,  0.1665, -0.1535,\n",
       "                       0.1332,  0.1730,  0.1589, -0.1635,  0.1705,  0.1690, -0.1507, -0.1462,\n",
       "                      -0.1416,  0.1373,  0.1703, -0.1318,  0.1636,  0.1622, -0.1712, -0.1354,\n",
       "                       0.1661,  0.1713,  0.1418,  0.1677,  0.1448, -0.1628, -0.1638,  0.1465,\n",
       "                      -0.1312, -0.1539, -0.1318, -0.1688, -0.1553,  0.1374,  0.1690,  0.1654,\n",
       "                      -0.1319, -0.1450, -0.1653,  0.1466, -0.1389,  0.1634, -0.1588,  0.1406,\n",
       "                      -0.1677,  0.1563,  0.1563,  0.1730,  0.1713,  0.1601, -0.1347,  0.1329,\n",
       "                       0.1466,  0.1707,  0.1705,  0.1395,  0.1562,  0.1738, -0.1404,  0.1479,\n",
       "                       0.1435,  0.1678,  0.1697,  0.1663, -0.1454,  0.1311,  0.1695,  0.1375,\n",
       "                      -0.1483,  0.1542,  0.1378, -0.1347, -0.1474, -0.1341,  0.1596, -0.1600,\n",
       "                      -0.1531,  0.1515,  0.1315, -0.1705,  0.1720,  0.1502,  0.1634, -0.1507,\n",
       "                      -0.1343, -0.1463,  0.1582, -0.1322, -0.1641,  0.1644, -0.1407, -0.1646,\n",
       "                       0.1452, -0.1680,  0.1589,  0.1414,  0.1362, -0.1353, -0.1350, -0.1341,\n",
       "                      -0.1311,  0.1676, -0.1373,  0.1698,  0.1580,  0.1472, -0.1419, -0.1711,\n",
       "                      -0.1399,  0.1559, -0.1385,  0.1470, -0.1353, -0.1329, -0.1611, -0.1346],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.layer_norm2.gamma',\n",
       "              tensor([0.8476, 1.1523, 0.8477, 1.1519, 1.1523, 1.1523, 1.1523, 1.1524, 0.8476,\n",
       "                      1.1524, 1.1522, 1.1524, 0.8476, 0.8474, 0.8476, 1.1524, 0.8479, 0.8477,\n",
       "                      0.8476, 1.1524, 1.1524, 0.8477, 1.1522, 1.1524, 1.1516, 0.8480, 1.1525,\n",
       "                      0.8477, 0.8477, 1.1518, 1.1523, 0.8479, 0.8477, 0.8478, 0.8480, 0.8483,\n",
       "                      1.1524, 1.1523, 1.1524, 0.8476, 0.8478, 1.1525, 0.8476, 0.8476, 0.8477,\n",
       "                      1.1523, 1.1524, 0.8478, 1.1524, 0.8478, 0.8478, 0.8477, 0.8477, 1.1520,\n",
       "                      0.8480, 1.1523, 1.1522, 1.1524, 0.8477, 1.1522, 1.1530, 1.1524, 1.1523,\n",
       "                      0.8476, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1522, 1.1524, 0.8477, 0.8476, 1.1518, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8476, 0.8476, 0.8477, 1.1522, 0.8477, 0.8478,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1524, 1.1524, 1.1523, 1.1523, 0.8477, 0.8476,\n",
       "                      1.1526, 0.8477, 0.8477, 1.1524, 1.1524, 0.8477, 1.1522, 1.1523, 0.8468,\n",
       "                      0.8477, 1.1524, 1.1524, 1.1505, 1.1523, 0.8476, 0.8475, 0.8477, 0.8476,\n",
       "                      1.1524, 1.1523, 1.1530, 1.1523, 0.8477, 0.8477, 0.8477, 1.1524, 0.8476,\n",
       "                      1.1523, 1.1525, 0.8478, 1.1524, 1.1523, 1.1523, 1.1540, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1522, 1.1524, 0.8482, 1.1523, 1.1521,\n",
       "                      1.1522, 0.8477, 1.1515, 0.8477, 1.1458, 0.8482, 1.1521, 1.1523, 1.1522,\n",
       "                      0.8477, 0.8478, 1.1520, 1.1523, 0.8475, 1.1523, 1.1523, 1.1519, 1.1525,\n",
       "                      1.1523, 0.8480, 0.8478, 0.8482, 0.8477, 1.1525, 1.1522, 0.8477, 1.1523,\n",
       "                      1.1524, 0.8477, 0.8478, 0.8477, 1.1523, 1.1523, 1.1525, 1.1523, 0.8475,\n",
       "                      0.8472, 0.8477, 0.8476, 0.8477, 1.1523, 1.1522, 0.8477, 1.1523, 1.1522,\n",
       "                      0.8476, 0.8476, 1.1523, 0.8476, 1.1523, 1.1523, 0.8476, 1.1523, 0.8476,\n",
       "                      1.1523, 0.9084, 0.8477, 1.1523, 0.8487, 0.8475, 0.8476, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1524, 0.8477, 0.8477, 1.1524, 0.8477,\n",
       "                      1.1524, 0.8478, 1.1522, 0.8473, 1.1523, 1.1523, 0.8479, 1.1525, 0.8477,\n",
       "                      0.8501, 0.8477, 1.1522, 1.1523, 1.1523, 1.1521, 0.8476, 0.8475, 1.1526,\n",
       "                      1.1468, 1.1523, 0.8476, 1.1522, 0.8476, 0.8479, 0.8478, 1.1524, 0.8476,\n",
       "                      0.8482, 1.1521, 0.8480, 1.1524, 1.1549, 1.1521, 1.1523, 0.8541, 1.1523,\n",
       "                      0.8476, 0.8488, 1.1521, 0.8477, 0.8477, 0.8477, 1.1524, 1.1523, 0.8477,\n",
       "                      0.8478, 1.1524, 1.1523, 1.1521, 1.1523, 0.8476, 0.8478, 1.1523, 1.1524,\n",
       "                      1.1522, 0.8477, 0.8474, 0.8477, 0.8477, 1.1523, 1.1523, 0.8478, 1.1522,\n",
       "                      0.8478, 0.8477, 1.1523, 0.8477, 1.1523, 0.8476, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8476, 0.8476, 0.8474, 1.1524, 0.8477, 0.8484, 0.8478, 1.1524, 1.1524,\n",
       "                      0.8478, 0.8477, 0.8477, 1.1523, 0.8477, 1.1517, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1522, 1.1524, 0.8477, 1.1524, 1.1523, 0.8476, 1.1523, 1.1523,\n",
       "                      0.8476, 1.1521, 0.8477, 0.8477, 1.1522, 0.8477, 1.1520, 0.8477, 0.8476,\n",
       "                      1.1522, 1.1523, 1.1523, 0.8476, 1.1520, 0.8476, 1.1515, 1.1524, 0.8474,\n",
       "                      0.8476, 0.8474, 0.8477, 1.1523, 1.1523, 0.8475, 0.8477, 0.8477, 0.8486,\n",
       "                      0.8482, 0.8478, 0.8478, 1.1527, 0.8478, 0.8477, 0.8478, 1.1523, 0.8476,\n",
       "                      0.8478, 0.8477, 0.8476, 0.8477, 0.8479, 1.1523, 1.1512, 0.8477, 0.8489,\n",
       "                      0.8477, 1.1524, 1.1524, 1.1523, 0.8473, 0.8479, 1.1523, 1.1524, 1.1579,\n",
       "                      0.8514, 1.1523, 1.1524, 0.8476, 1.1523, 0.8474, 0.8477, 0.8478, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8476, 1.1524, 1.1523, 1.1523, 0.8477, 1.1523, 1.1585,\n",
       "                      1.1521, 0.8477, 0.8477, 0.8480, 0.8479, 0.8476, 0.8483, 0.8477, 0.8478,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1522, 1.1524, 0.8477, 1.1525, 0.8476, 1.1513,\n",
       "                      1.1524, 1.1521, 0.8479, 1.1524, 1.1522, 0.8477, 0.8477, 1.1525, 1.1515,\n",
       "                      0.8479, 1.1520, 1.1524, 1.1523, 1.1523, 1.1522, 0.8478, 0.8476, 0.8476,\n",
       "                      1.1524, 0.8478, 1.1524, 1.1514, 0.8478, 0.8485, 1.1518, 0.8476, 1.1519,\n",
       "                      1.1529, 1.1519, 1.1523, 1.1520, 1.1521, 1.1522, 0.8477, 0.8476, 1.1524,\n",
       "                      1.1523, 0.8479, 1.1523, 1.1523, 0.8477, 0.8477, 0.8480, 1.1524, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8476, 1.1523, 1.1522, 0.8478, 0.8478,\n",
       "                      0.8477, 0.8477, 1.1522, 0.8478, 0.8481, 0.8476, 0.8478, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8476, 1.1523, 1.1522, 0.8476, 1.1523, 0.8480, 1.1523, 0.8480,\n",
       "                      0.8478, 1.1523, 0.8477, 1.1526, 0.8479, 0.8477, 0.8476, 1.1522, 1.1524,\n",
       "                      1.1524, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1524, 0.8464, 1.1522,\n",
       "                      0.8477, 1.1524, 0.8477, 0.8483, 1.1523, 1.1523, 1.1522, 1.1522, 0.8476,\n",
       "                      1.1522, 0.8478, 0.8475, 1.1524, 1.1524, 0.8390, 1.1523, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.1.layer_norm2.beta',\n",
       "              tensor([ 0.1523, -0.1523,  0.1524, -0.1523, -0.1523, -0.1523, -0.1523, -0.1524,\n",
       "                       0.1523, -0.1523, -0.1524, -0.1523,  0.1523, -0.1523,  0.1524,  0.1524,\n",
       "                       0.1524,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1524,\n",
       "                       0.1522,  0.1523,  0.1526, -0.1521, -0.1523,  0.1523,  0.1523, -0.1524,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1524,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1522, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1524,\n",
       "                       0.1524,  0.1523, -0.1524,  0.1523, -0.1523,  0.1526,  0.1524,  0.1523,\n",
       "                       0.1521, -0.1523,  0.1524,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1524, -0.1523,  0.1523,  0.1523, -0.1524,\n",
       "                       0.1523, -0.1524,  0.1523,  0.1523,  0.1525, -0.1523, -0.1523, -0.1524,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1524,  0.1523, -0.1522, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1526,  0.1523, -0.1521, -0.1523,  0.1524,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1524, -0.1523,  0.1523,  0.1523, -0.1523,  0.1522,  0.1523,  0.1523,\n",
       "                      -0.1522, -0.1523, -0.1523,  0.1523,  0.1523, -0.1524, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1524,  0.1523,  0.1523,  0.1524,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1525, -0.1524,  0.1523, -0.1524,  0.1521, -0.1523, -0.1523, -0.1524,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1522,  0.1519, -0.1524,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1521, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1524, -0.1522,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1522, -0.1524, -0.1523,  0.1524,  0.1523,  0.1524,  0.1524, -0.1525,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1522, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1522, -0.1525,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1521,  0.1486, -0.1523, -0.1520, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1529, -0.1523, -0.1525,\n",
       "                       0.1524, -0.1523, -0.1524,  0.1522,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1524,  0.1524,  0.1523,  0.1522, -0.1523, -0.1523, -0.1524,\n",
       "                       0.1523, -0.1524, -0.1523, -0.1523, -0.1523,  0.1524,  0.1524,  0.1523,\n",
       "                      -0.1523, -0.1529, -0.1523,  0.1523, -0.1523, -0.1523,  0.1522, -0.1523,\n",
       "                      -0.1523, -0.1534,  0.1523, -0.1523,  0.1523, -0.1523,  0.1524, -0.1525,\n",
       "                      -0.1523, -0.1524,  0.1521, -0.1526,  0.1523,  0.1523,  0.1523, -0.1524,\n",
       "                       0.1523, -0.1523, -0.1524, -0.1522, -0.1523, -0.1525, -0.1522, -0.1523,\n",
       "                       0.1528, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1524,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1524, -0.1522, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1524, -0.1523,  0.1523,\n",
       "                      -0.1524,  0.1521, -0.1522, -0.1523,  0.1523, -0.1524, -0.1524,  0.1523,\n",
       "                      -0.1525,  0.1524,  0.1523, -0.1523, -0.1523,  0.1524, -0.1521,  0.1524,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1524, -0.1524, -0.1523,\n",
       "                       0.1524,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1524, -0.1522,  0.1524,  0.1547,  0.1523, -0.1523, -0.1524, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1524, -0.1523,  0.1524, -0.1522,  0.1526,  0.1524,\n",
       "                      -0.1523, -0.1521,  0.1524, -0.1523, -0.1524, -0.1523, -0.1523, -0.1525,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1524,  0.1523, -0.1521, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1524, -0.1524,  0.1520,  0.1524,\n",
       "                      -0.1524, -0.1524,  0.1523,  0.1523,  0.1523,  0.1526,  0.1524, -0.1523,\n",
       "                      -0.1524, -0.1523,  0.1524,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1524,  0.1523, -0.1525, -0.1523, -0.1523,  0.1524, -0.1526,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1524, -0.1524,  0.1523, -0.1523,  0.1524, -0.1528,\n",
       "                       0.1523,  0.1524,  0.1524, -0.1523,  0.1524,  0.1523, -0.1523, -0.1524,\n",
       "                      -0.1523,  0.1521,  0.1524, -0.1523,  0.1523,  0.1523, -0.1524, -0.1520,\n",
       "                       0.1523,  0.1524,  0.1526,  0.1523,  0.1524, -0.1524, -0.1523,  0.1523,\n",
       "                      -0.1524, -0.1524, -0.1523, -0.1524, -0.1524,  0.1523,  0.1524,  0.1523,\n",
       "                       0.1530, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1525,  0.1523,\n",
       "                      -0.1523,  0.1524,  0.1524,  0.1274,  0.1523,  0.1523, -0.1523,  0.1525,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1524, -0.1523,  0.1523,  0.1524,  0.1524,\n",
       "                      -0.1522,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1520,  0.1523,  0.1524, -0.1523,  0.1524,  0.1524,  0.1523, -0.1524,\n",
       "                      -0.1523, -0.1524,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1524,\n",
       "                      -0.1524,  0.1523, -0.1523,  0.1523, -0.1521,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1522,  0.1524, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1524],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.attention.W_Q.weight',\n",
       "              tensor([[ 0.1699,  0.2275,  0.2171,  ...,  0.1034,  0.1215, -0.1030],\n",
       "                      [-0.1802, -0.2045,  0.1592,  ..., -0.1419, -0.2177,  0.0834],\n",
       "                      [-0.1565, -0.1217, -0.1154,  ..., -0.0807, -0.2095,  0.1388],\n",
       "                      ...,\n",
       "                      [-0.2071, -0.1938, -0.1071,  ..., -0.0908, -0.1214,  0.1756],\n",
       "                      [ 0.1448,  0.1585, -0.1483,  ..., -0.0805,  0.1624, -0.1117],\n",
       "                      [ 0.1269,  0.1453,  0.1727,  ...,  0.1604,  0.1866, -0.2157]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.attention.W_K.weight',\n",
       "              tensor([[-0.1470,  0.0850, -0.1127,  ...,  0.1310,  0.2079,  0.1645],\n",
       "                      [-0.0954,  0.2178, -0.2163,  ...,  0.1339, -0.1131,  0.0978],\n",
       "                      [-0.1290, -0.1901, -0.1776,  ...,  0.0893,  0.1898,  0.1558],\n",
       "                      ...,\n",
       "                      [-0.0906,  0.1804, -0.1867,  ...,  0.0847,  0.1212, -0.2007],\n",
       "                      [-0.0963,  0.0872, -0.2117,  ...,  0.1263,  0.2142, -0.1882],\n",
       "                      [ 0.1278, -0.1719, -0.2248,  ...,  0.1076, -0.1437,  0.1050]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.attention.W_V.weight',\n",
       "              tensor([[-0.1894,  0.0765, -0.1283,  ..., -0.1797, -0.1274,  0.1950],\n",
       "                      [ 0.1337,  0.1102,  0.1624,  ...,  0.1578,  0.2198, -0.0814],\n",
       "                      [-0.2241, -0.0870, -0.1590,  ..., -0.0795, -0.0968,  0.0827],\n",
       "                      ...,\n",
       "                      [-0.1805, -0.1923, -0.1458,  ...,  0.1725, -0.1906,  0.1235],\n",
       "                      [-0.0997, -0.2001, -0.1274,  ...,  0.1557, -0.2268,  0.1023],\n",
       "                      [ 0.0852,  0.0864,  0.0826,  ..., -0.1070,  0.1106, -0.1275]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.attention.W_T.weight',\n",
       "              tensor([[-0.1178, -0.2287, -0.1460,  ...,  0.1989,  0.0983, -0.1190],\n",
       "                      [-0.0806, -0.1553, -0.1407,  ...,  0.1531,  0.1324, -0.1458],\n",
       "                      [-0.1414, -0.0889, -0.2217,  ...,  0.1975,  0.2098, -0.2025],\n",
       "                      ...,\n",
       "                      [ 0.2127,  0.1284,  0.0828,  ...,  0.1709, -0.0922,  0.1698],\n",
       "                      [ 0.2041,  0.1394,  0.2042,  ..., -0.2036, -0.1321,  0.2143],\n",
       "                      [-0.1860, -0.1706, -0.2211,  ...,  0.0723,  0.1624, -0.0956]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.layer_norm1.gamma',\n",
       "              tensor([1.1524, 0.8477, 0.8478, 0.8478, 1.1509, 1.1523, 0.8477, 1.1515, 0.8478,\n",
       "                      1.1523, 0.8476, 1.1522, 1.1523, 0.8476, 0.8477, 0.8476, 1.1524, 0.8472,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1522, 0.8473, 1.1523, 0.8488,\n",
       "                      1.1540, 1.1525, 0.8477, 0.8469, 0.8477, 1.1515, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1522, 1.1524, 0.8476, 1.1506, 0.8477, 1.1526, 0.8477, 1.1524,\n",
       "                      0.8477, 0.8476, 1.1524, 0.8484, 0.8477, 0.8472, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1516, 1.1524, 1.1523, 0.8477, 0.8477, 1.1523, 1.1524, 0.8477,\n",
       "                      0.8477, 1.1521, 0.8477, 1.1520, 0.8477, 0.8476, 0.8476, 0.8478, 0.8479,\n",
       "                      1.1526, 1.1523, 1.1522, 1.1523, 1.1524, 1.1754, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8475, 1.1524, 1.1525, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1494, 1.1519, 0.8476, 1.1523, 0.8478, 0.8477, 1.1534,\n",
       "                      1.1526, 0.8477, 1.1522, 0.8477, 0.8480, 0.8465, 0.8477, 0.8487, 1.1522,\n",
       "                      0.8477, 0.8476, 1.1523, 0.8477, 1.1524, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1524, 1.1517, 1.1522, 0.8480, 0.8476, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1524, 1.1528, 0.8477, 1.1518, 1.1523, 1.1522, 0.8478, 0.8477, 1.1524,\n",
       "                      1.1524, 0.8477, 1.1523, 1.1518, 1.1527, 1.1523, 1.1524, 1.1523, 1.1523,\n",
       "                      1.1524, 1.1532, 0.8481, 0.8477, 1.1523, 0.8449, 1.1517, 1.1523, 0.8476,\n",
       "                      0.8476, 1.1565, 0.8474, 0.8477, 1.1523, 0.8476, 1.1523, 1.1522, 0.8478,\n",
       "                      1.1523, 1.1524, 1.1523, 0.8477, 1.1525, 1.1523, 1.1523, 0.8477, 0.8476,\n",
       "                      1.1525, 0.8478, 1.1514, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8455,\n",
       "                      0.8471, 0.8478, 0.8477, 0.8477, 0.8478, 1.1523, 1.1524, 0.8477, 0.8479,\n",
       "                      0.8478, 1.1523, 1.1525, 0.8490, 1.1663, 1.1520, 0.8477, 1.1523, 1.1525,\n",
       "                      1.1523, 1.1526, 0.8458, 1.1523, 1.1523, 0.8483, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8476, 1.1523, 1.1524, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1520, 1.1521, 1.1519, 1.1511, 1.1515, 1.1523, 1.1524,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1525, 1.1172, 1.1523, 1.1523, 1.1522,\n",
       "                      1.1528, 1.1524, 0.8477, 1.1523, 1.1523, 1.1524, 0.8476, 0.8477, 1.1529,\n",
       "                      0.8461, 1.1523, 1.1523, 0.8478, 0.8479, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8474, 1.1524, 1.1514, 1.1520, 0.8477, 1.1523, 0.8473, 1.1524,\n",
       "                      0.8477, 1.1512, 0.8467, 1.1523, 1.1524, 1.1526, 0.8476, 0.8476, 1.1523,\n",
       "                      1.1522, 1.1489, 0.8474, 0.8477, 1.1523, 1.1527, 0.8475, 0.8476, 1.1522,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8478, 1.1523, 0.8477, 0.8477, 1.1532, 0.8477,\n",
       "                      1.1525, 0.8484, 1.1523, 0.8492, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8476, 1.1523, 0.8477, 0.8476, 1.1523, 1.1524, 0.8478,\n",
       "                      1.1523, 0.8479, 1.1522, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1528,\n",
       "                      0.8477, 0.8474, 1.1523, 0.8488, 1.1497, 0.8477, 1.1524, 0.8479, 1.1523,\n",
       "                      1.1524, 1.1523, 1.1523, 0.8477, 1.1523, 0.8476, 1.1523, 1.1533, 1.1531,\n",
       "                      0.8477, 0.8478, 1.1523, 1.1523, 0.8481, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1521, 0.8477, 1.1539, 1.1539, 1.1523, 0.8477,\n",
       "                      0.8476, 1.1523, 0.8476, 1.1522, 0.8476, 0.8477, 1.1523, 1.1521, 0.8496,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1526, 1.1523, 1.1523, 1.1535, 0.8476, 0.8475,\n",
       "                      1.1523, 0.8484, 0.8477, 1.1523, 1.1523, 1.1524, 0.8477, 0.8475, 0.8479,\n",
       "                      0.8477, 0.8486, 0.8473, 0.8477, 0.8474, 0.8477, 0.8477, 1.1523, 1.1524,\n",
       "                      1.1524, 0.8477, 0.8477, 0.8476, 0.8477, 0.8476, 0.8477, 0.8476, 0.8476,\n",
       "                      0.8476, 1.1523, 0.8476, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 1.1521,\n",
       "                      0.8476, 0.8477, 0.8476, 1.1522, 0.8475, 1.1523, 0.8477, 1.1524, 1.1524,\n",
       "                      1.1524, 1.1523, 0.8477, 1.1524, 1.1522, 1.1528, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1692, 0.8477, 0.8483, 1.1523, 1.1521, 0.8477, 0.8477, 0.8477, 1.1524,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8474, 0.8477, 0.8476, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8481, 0.8477, 0.8480, 1.1522, 1.1523,\n",
       "                      0.8481, 0.8477, 0.8476, 0.8477, 0.8477, 1.1523, 0.8480, 0.8478, 0.8476,\n",
       "                      1.1525, 0.8477, 1.1524, 0.8477, 0.8477, 0.8476, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1524, 0.8477, 1.1522, 0.8476, 0.8477, 1.1526, 0.8456, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1522, 1.1524, 0.8477, 0.8477, 1.1522, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 0.8482, 0.8476,\n",
       "                      0.8477, 1.1525, 0.8477, 0.8477, 1.1523, 1.1523, 1.1522, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1526, 1.1523, 1.1523, 1.1511, 0.8477, 1.1522, 1.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.layer_norm1.beta',\n",
       "              tensor([ 0.1523,  0.1523,  0.1522,  0.1524, -0.1525, -0.1523, -0.1523, -0.1524,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1524, -0.1524,  0.1523,  0.1524, -0.1524,\n",
       "                       0.1524, -0.1526,  0.1523, -0.1525, -0.1523,  0.1523, -0.1523, -0.1522,\n",
       "                      -0.1524,  0.1524, -0.1522, -0.1504,  0.1519,  0.1523, -0.1525, -0.1523,\n",
       "                      -0.1530,  0.1523,  0.1524, -0.1523, -0.1523, -0.1502, -0.1524,  0.1523,\n",
       "                       0.1525, -0.1523, -0.1523,  0.1523, -0.1524, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1525, -0.1524, -0.1524, -0.1523,  0.1523,  0.1526,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1524, -0.1522, -0.1523,  0.1523,\n",
       "                       0.1524,  0.1523, -0.1526, -0.1523, -0.1523,  0.1523,  0.1523, -0.1522,\n",
       "                       0.1524, -0.1524,  0.1523,  0.1523,  0.1523,  0.1522,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1524,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1524,  0.1529,  0.1524,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1520, -0.1524,  0.1523, -0.1524, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1522,  0.1523, -0.1523, -0.1525,  0.1524,  0.1523,\n",
       "                      -0.1528, -0.1524, -0.1523, -0.1523,  0.1523, -0.1524, -0.1522, -0.1521,\n",
       "                       0.1524,  0.1523,  0.1523,  0.1524,  0.1523,  0.1523,  0.1524,  0.1516,\n",
       "                      -0.1523, -0.1525,  0.1524, -0.1523, -0.1523, -0.1523, -0.1523, -0.1524,\n",
       "                       0.1523,  0.1523, -0.1522,  0.1522,  0.1523, -0.1524,  0.1523, -0.1523,\n",
       "                       0.1524, -0.1523,  0.1517,  0.1523,  0.1523,  0.1525, -0.1525,  0.1522,\n",
       "                       0.1523, -0.1523, -0.1522, -0.1520, -0.1522,  0.1524,  0.1523,  0.1523,\n",
       "                       0.1525,  0.1522, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1522, -0.1522, -0.1522, -0.1535, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1524,  0.1506,  0.1525, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1522,  0.1534, -0.1524, -0.1523,  0.1524,\n",
       "                       0.1524, -0.1525,  0.1523, -0.1523, -0.1523, -0.1524, -0.1523, -0.1524,\n",
       "                      -0.1481,  0.1523, -0.1524,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1524,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1524, -0.1523,  0.1525,  0.1526,  0.1524,  0.1525, -0.1522,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1508, -0.1523,\n",
       "                      -0.1524,  0.1522, -0.1524, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1517, -0.1528,  0.1524,  0.1523, -0.1523, -0.1524,\n",
       "                       0.1522, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1525,\n",
       "                       0.1524, -0.1523,  0.1524, -0.1524, -0.1524, -0.1523,  0.1524,  0.1523,\n",
       "                       0.1523,  0.1524,  0.1522,  0.1523, -0.1521,  0.1524,  0.1523, -0.1525,\n",
       "                       0.1526,  0.1523, -0.1523, -0.1524,  0.1525, -0.1523, -0.1524, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1524, -0.1524,  0.1523, -0.1523, -0.1521,  0.1523,\n",
       "                       0.1521, -0.1527,  0.1523,  0.1518,  0.1522, -0.1523, -0.1523,  0.1524,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1524,\n",
       "                       0.1527, -0.1523,  0.1524, -0.1520, -0.1524, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1524, -0.1523, -0.1527,  0.1524,  0.1521,  0.1524,\n",
       "                       0.1523,  0.1523, -0.1525, -0.1524,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1524, -0.1523,  0.1374, -0.1523,  0.1523, -0.1523, -0.1522,  0.1523,\n",
       "                       0.1524,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1524,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1525,  0.1525, -0.1524,  0.1522, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1524, -0.1523, -0.1523,  0.1524,  0.1514,\n",
       "                       0.1524,  0.1523, -0.1523,  0.1522,  0.1523, -0.1524, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1520,  0.1523, -0.1523,  0.1524, -0.1523, -0.1523,\n",
       "                      -0.1525, -0.1528,  0.1523,  0.1523,  0.1524,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1521,  0.1523, -0.1523, -0.1524, -0.1522,  0.1524, -0.1524,  0.1521,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1524,\n",
       "                       0.1523,  0.1523,  0.1532, -0.1523,  0.1525,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1524, -0.1523, -0.1524, -0.1523,  0.1524,  0.1523,  0.1524, -0.1523,\n",
       "                       0.1524,  0.1524, -0.1523,  0.1524,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1524, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1524,  0.1523, -0.1524, -0.1523, -0.1523,  0.1523, -0.1524,  0.1524,\n",
       "                      -0.1523,  0.1524, -0.1523, -0.1524,  0.1523,  0.1525, -0.1525,  0.1520,\n",
       "                       0.1523, -0.1523, -0.1522,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1525,  0.1523,  0.1523,  0.1523, -0.1524,  0.1524,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1524,\n",
       "                      -0.1524, -0.1523,  0.1534,  0.1524,  0.1523,  0.1523,  0.1524, -0.1523,\n",
       "                      -0.1524,  0.1524,  0.1523, -0.1523,  0.1522,  0.1524, -0.1523, -0.1523,\n",
       "                       0.1524, -0.1523,  0.1523,  0.1524,  0.1524, -0.1522,  0.1524, -0.1521,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1525,  0.1524, -0.1523,  0.1524,\n",
       "                      -0.1523, -0.1520, -0.1524,  0.1524, -0.1522, -0.1523, -0.1523,  0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.ffn.layer1.weight',\n",
       "              tensor([[ 0.1751, -0.1477, -0.1772,  ...,  0.1108, -0.1874,  0.1151],\n",
       "                      [ 0.1163, -0.1477, -0.1249,  ...,  0.1762, -0.1600,  0.1419],\n",
       "                      [ 0.1063,  0.1239, -0.1263,  ...,  0.1444, -0.1216,  0.1711],\n",
       "                      ...,\n",
       "                      [-0.1827,  0.1798, -0.1463,  ...,  0.1946,  0.1625, -0.1425],\n",
       "                      [ 0.1867, -0.1667, -0.1457,  ...,  0.1524, -0.1679,  0.1993],\n",
       "                      [ 0.1189, -0.1443, -0.1149,  ..., -0.1054,  0.1841,  0.1079]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.ffn.layer1.bias',\n",
       "              tensor([ 0.1334,  0.1505,  0.1375,  ..., -0.1359,  0.1652,  0.1274],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.ffn.layer2.weight',\n",
       "              tensor([[ 0.1935,  0.1614,  0.1549,  ...,  0.1252,  0.1913,  0.1574],\n",
       "                      [ 0.1597,  0.1220,  0.1390,  ...,  0.0695,  0.1126,  0.1128],\n",
       "                      [ 0.1730,  0.1478,  0.1939,  ..., -0.1292,  0.1961,  0.1521],\n",
       "                      ...,\n",
       "                      [-0.1389, -0.1648, -0.1273,  ..., -0.2018, -0.1464, -0.1170],\n",
       "                      [-0.1878, -0.1402, -0.1923,  ...,  0.1662, -0.1227, -0.1977],\n",
       "                      [ 0.1661,  0.1828,  0.1255,  ...,  0.1391,  0.1471,  0.2003]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.ffn.layer2.bias',\n",
       "              tensor([ 0.1358,  0.1339,  0.1721,  0.1364, -0.1525, -0.1356, -0.1339, -0.1342,\n",
       "                      -0.1577, -0.1692, -0.1382, -0.1331, -0.1524,  0.1659,  0.1335, -0.1724,\n",
       "                       0.1554, -0.1372,  0.1415,  0.1445, -0.1737,  0.1449, -0.1369,  0.1707,\n",
       "                      -0.1381,  0.1430, -0.1714,  0.1579,  0.1329,  0.1360,  0.1612,  0.1360,\n",
       "                      -0.1320,  0.1607,  0.1668, -0.1534, -0.1534, -0.1676, -0.1532,  0.1391,\n",
       "                       0.1433, -0.1553, -0.1545, -0.1587, -0.1599, -0.1587, -0.1568, -0.1389,\n",
       "                       0.1486,  0.1341,  0.1321,  0.1533,  0.1678, -0.1597,  0.1421,  0.1562,\n",
       "                       0.1684, -0.1649,  0.1629,  0.1637, -0.1574, -0.1490, -0.1708,  0.1341,\n",
       "                       0.1370, -0.1539,  0.1394, -0.1588, -0.1575,  0.1711,  0.1586, -0.1304,\n",
       "                       0.1498, -0.1730,  0.1698,  0.1320, -0.1512, -0.1466,  0.1475, -0.1681,\n",
       "                       0.1580, -0.1569,  0.1532, -0.1371, -0.1614,  0.1608,  0.1382, -0.1732,\n",
       "                       0.1556,  0.1731,  0.1385, -0.1530, -0.1588, -0.1717,  0.1393,  0.1431,\n",
       "                      -0.1454,  0.1721,  0.1571, -0.1555,  0.1367, -0.1629, -0.1497, -0.1466,\n",
       "                       0.1458,  0.1506, -0.1694,  0.1351, -0.1540, -0.1645,  0.1431,  0.1659,\n",
       "                      -0.1680, -0.1565, -0.1524, -0.1729,  0.1735, -0.1490, -0.1507, -0.1612,\n",
       "                       0.1631,  0.1345,  0.1502,  0.1521,  0.1412,  0.1725,  0.1427,  0.1678,\n",
       "                      -0.1405,  0.1572,  0.1522, -0.1547,  0.1330, -0.1717, -0.1464, -0.1479,\n",
       "                       0.1371,  0.1501,  0.1597,  0.1380,  0.1633, -0.1701,  0.1349, -0.1592,\n",
       "                       0.1709,  0.1470,  0.1697,  0.1463,  0.1736,  0.1514, -0.1641,  0.1365,\n",
       "                       0.1335, -0.1471, -0.1656,  0.1677, -0.1581, -0.1391,  0.1601,  0.1310,\n",
       "                       0.1501,  0.1533, -0.1648, -0.1346, -0.1358,  0.1446, -0.1461, -0.1563,\n",
       "                       0.1669,  0.1630, -0.1535,  0.1513,  0.1534,  0.1463, -0.1629, -0.1644,\n",
       "                       0.1510,  0.1453,  0.1584,  0.1385, -0.1664, -0.1469,  0.1401,  0.1547,\n",
       "                       0.1526, -0.1617,  0.1407,  0.1417,  0.1364, -0.1336, -0.1385,  0.1591,\n",
       "                       0.1645,  0.1599,  0.1567, -0.1601, -0.1472,  0.1413, -0.1553,  0.1473,\n",
       "                       0.1665,  0.1677, -0.1397,  0.1569,  0.1308, -0.1596,  0.1519, -0.1571,\n",
       "                      -0.1314, -0.1710,  0.1720,  0.1592,  0.1447,  0.1696, -0.1571,  0.1349,\n",
       "                       0.1609, -0.1505,  0.1478,  0.1582, -0.1615, -0.1743, -0.1497,  0.1632,\n",
       "                       0.1523,  0.1656, -0.1710,  0.1369, -0.1501, -0.1664, -0.1579, -0.1331,\n",
       "                      -0.1495, -0.1496, -0.1462, -0.1621,  0.1392,  0.1700,  0.1432,  0.1512,\n",
       "                       0.1625, -0.1590, -0.1570, -0.1361,  0.1330,  0.1423,  0.1488, -0.1571,\n",
       "                       0.1420, -0.1416, -0.1703, -0.1331, -0.1481, -0.1642, -0.1541, -0.1440,\n",
       "                      -0.1687, -0.1632,  0.1418,  0.1329, -0.1555, -0.1341,  0.1524,  0.1439,\n",
       "                       0.1432,  0.1326, -0.1445,  0.1471, -0.1320,  0.1728,  0.1636, -0.1622,\n",
       "                      -0.1628,  0.1502,  0.1740, -0.1540,  0.1441, -0.1454, -0.1312, -0.1406,\n",
       "                      -0.1347, -0.1544,  0.1697, -0.1342,  0.1503, -0.1454, -0.1628,  0.1364,\n",
       "                       0.1513, -0.1711,  0.1406, -0.1562,  0.1684, -0.1468, -0.1710,  0.1609,\n",
       "                      -0.1551,  0.1385, -0.1392, -0.1513, -0.1722,  0.1650, -0.1711,  0.1445,\n",
       "                       0.1466, -0.1424,  0.1721, -0.1584, -0.1423, -0.1305, -0.1602,  0.1699,\n",
       "                      -0.1530, -0.1344, -0.1643, -0.1654,  0.1388,  0.1425,  0.1460, -0.1634,\n",
       "                       0.1339,  0.1311, -0.1426, -0.1464,  0.1408,  0.1330, -0.1650,  0.1552,\n",
       "                       0.1430, -0.1305, -0.1684, -0.1639,  0.1366, -0.1321,  0.1572,  0.1585,\n",
       "                       0.1322,  0.1608, -0.1439, -0.1646,  0.1717, -0.1712, -0.1533,  0.1421,\n",
       "                      -0.1605,  0.1497,  0.1532,  0.1541,  0.1416, -0.1321,  0.1441, -0.1417,\n",
       "                      -0.1384, -0.1505, -0.1336, -0.1508, -0.1351, -0.1430,  0.1531, -0.1541,\n",
       "                       0.1672,  0.1310, -0.1588,  0.1442,  0.1341, -0.1449, -0.1619,  0.1573,\n",
       "                      -0.1442,  0.1305,  0.1478,  0.1644, -0.1657,  0.1445, -0.1438, -0.1470,\n",
       "                       0.1530,  0.1622,  0.1460, -0.1566,  0.1351,  0.1726, -0.1335,  0.1480,\n",
       "                       0.1623,  0.1737, -0.1603, -0.1648, -0.1433,  0.1342,  0.1627,  0.1507,\n",
       "                      -0.1448,  0.1504, -0.1695, -0.1357,  0.1393, -0.1595,  0.1534,  0.1567,\n",
       "                       0.1601,  0.1411,  0.1621, -0.1527,  0.1676,  0.1507, -0.1725, -0.1513,\n",
       "                       0.1661, -0.1634, -0.1430, -0.1679,  0.1562,  0.1405, -0.1617, -0.1448,\n",
       "                       0.1469, -0.1732, -0.1483, -0.1464,  0.1390, -0.1736,  0.1428,  0.1438,\n",
       "                      -0.1696, -0.1310, -0.1520, -0.1521, -0.1473, -0.1404,  0.1394,  0.1521,\n",
       "                      -0.1383,  0.1695, -0.1715, -0.1533, -0.1395,  0.1557,  0.1634,  0.1600,\n",
       "                      -0.1494,  0.1636, -0.1528,  0.1657,  0.1654,  0.1666, -0.1535,  0.1738,\n",
       "                       0.1468,  0.1407, -0.1417,  0.1421,  0.1467, -0.1521, -0.1315,  0.1596,\n",
       "                      -0.1416,  0.1699,  0.1707,  0.1553, -0.1352,  0.1709,  0.1591,  0.1654,\n",
       "                      -0.1461,  0.1553,  0.1306, -0.1587, -0.1390,  0.1320, -0.1429,  0.1356,\n",
       "                      -0.1713, -0.1361, -0.1741,  0.1327,  0.1399, -0.1383,  0.1374, -0.1401,\n",
       "                      -0.1535,  0.1437, -0.1515, -0.1563,  0.1425,  0.1704,  0.1712, -0.1669,\n",
       "                       0.1328,  0.1389,  0.1570,  0.1453,  0.1532, -0.1441,  0.1509,  0.1352,\n",
       "                       0.1740,  0.1687, -0.1677,  0.1672,  0.1472,  0.1492, -0.1538, -0.1396,\n",
       "                      -0.1636,  0.1566, -0.1624,  0.1713,  0.1351, -0.1305, -0.1602,  0.1547],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.layer_norm2.gamma',\n",
       "              tensor([1.1510, 1.1379, 0.8551, 0.8492, 1.1518, 1.1518, 0.8308, 0.8534, 1.1609,\n",
       "                      1.1517, 0.8477, 1.1404, 1.1515, 0.8473, 0.8531, 0.8485, 1.1500, 0.9437,\n",
       "                      0.8427, 1.1227, 1.1833, 0.8497, 1.1511, 1.1526, 0.8499, 1.1501, 1.0790,\n",
       "                      0.8330, 1.1502, 1.0910, 0.8636, 0.8722, 0.8519, 1.1538, 0.8512, 1.1517,\n",
       "                      0.8554, 1.1528, 0.8883, 0.8492, 1.1377, 0.8471, 1.1526, 1.1526, 1.1562,\n",
       "                      1.1497, 0.8523, 0.8494, 1.1583, 0.8480, 1.1497, 0.8514, 0.8584, 1.1488,\n",
       "                      0.8393, 1.1456, 1.0454, 1.1512, 1.1650, 0.8472, 1.1471, 1.1073, 0.8467,\n",
       "                      1.1434, 1.1445, 0.8466, 0.8366, 0.8477, 0.8488, 0.8487, 0.8502, 0.8548,\n",
       "                      0.8587, 1.1520, 1.1472, 1.1516, 0.8645, 1.1500, 0.8484, 1.1501, 0.8524,\n",
       "                      1.1527, 0.8723, 0.8483, 0.8473, 1.1474, 1.1514, 0.8487, 0.8488, 0.8494,\n",
       "                      1.1651, 1.1588, 0.8471, 0.8504, 0.8494, 1.1525, 0.8507, 0.8418, 1.1375,\n",
       "                      0.8510, 1.1567, 1.1485, 0.8466, 1.1472, 0.8500, 0.8492, 1.1484, 0.8650,\n",
       "                      0.8464, 0.8499, 0.9046, 0.8491, 1.1515, 1.1523, 0.8481, 1.1553, 0.8473,\n",
       "                      1.1535, 1.1431, 0.8507, 1.1525, 1.1483, 0.8527, 0.8485, 1.1527, 0.8484,\n",
       "                      1.1427, 0.8039, 0.8499, 0.8801, 1.1509, 1.1487, 1.1519, 0.8959, 1.1504,\n",
       "                      1.1475, 0.8477, 1.1518, 0.8499, 1.1491, 1.1469, 1.1180, 1.1524, 1.1539,\n",
       "                      1.1519, 1.1571, 0.8472, 0.8493, 1.1525, 0.8481, 0.8847, 0.8362, 0.8501,\n",
       "                      0.8480, 1.1466, 1.1542, 0.8485, 0.8797, 0.8516, 1.1487, 1.2021, 0.8516,\n",
       "                      1.1305, 0.8497, 1.1478, 0.8507, 1.1450, 1.1524, 1.1502, 0.8466, 0.8525,\n",
       "                      0.8483, 1.1540, 0.8564, 0.8511, 1.1964, 0.8488, 1.1508, 1.1516, 1.1483,\n",
       "                      0.8628, 0.8509, 0.8480, 0.8484, 0.8491, 1.1767, 1.1501, 1.1551, 1.1495,\n",
       "                      0.8628, 1.1523, 1.1534, 0.8206, 0.8717, 1.1480, 0.8482, 1.1451, 0.8586,\n",
       "                      1.1520, 1.1522, 0.8516, 1.1498, 1.1669, 0.8712, 0.8508, 0.8485, 1.1936,\n",
       "                      0.8629, 1.1509, 0.8707, 0.8485, 1.1497, 1.1449, 1.1522, 1.1522, 1.1519,\n",
       "                      1.1524, 0.8488, 1.1527, 1.1479, 1.1439, 0.7995, 1.1481, 1.1529, 1.1504,\n",
       "                      1.1517, 0.8482, 1.1521, 1.1516, 1.1490, 1.1517, 1.1495, 1.1992, 1.1540,\n",
       "                      1.1774, 1.1434, 1.1439, 1.1535, 1.1487, 1.1565, 1.1354, 0.8493, 1.1501,\n",
       "                      1.1493, 1.1513, 1.1499, 0.8489, 0.8530, 0.8521, 0.8481, 1.1482, 1.1522,\n",
       "                      0.8584, 0.8805, 1.1510, 0.8450, 1.1553, 0.8507, 1.1485, 0.8445, 1.1468,\n",
       "                      0.8529, 1.1441, 0.8529, 1.1285, 1.1507, 1.1507, 0.8481, 0.8498, 1.1253,\n",
       "                      1.1579, 1.2004, 1.1469, 0.8531, 0.8469, 1.1456, 0.8468, 0.8496, 1.1538,\n",
       "                      0.8480, 1.1518, 1.1435, 0.8544, 1.1478, 0.8476, 0.8477, 1.1402, 0.8471,\n",
       "                      1.1496, 1.1487, 1.1511, 1.1513, 0.8489, 0.8502, 0.8455, 1.1527, 1.1417,\n",
       "                      0.8406, 1.1526, 0.8503, 1.1514, 1.1375, 0.8822, 1.1522, 1.1908, 0.8483,\n",
       "                      1.1514, 1.1465, 1.1505, 0.8476, 0.8484, 1.1510, 0.8488, 0.9329, 0.8468,\n",
       "                      0.8490, 0.8480, 1.1743, 1.0893, 0.8285, 0.8499, 1.1433, 0.8505, 1.1513,\n",
       "                      0.8484, 1.1419, 1.1486, 1.1481, 1.1513, 0.8485, 1.1512, 1.1512, 1.1498,\n",
       "                      0.8495, 1.1494, 1.1514, 1.1506, 1.1470, 1.1503, 0.8478, 0.8493, 1.1514,\n",
       "                      0.8491, 0.8661, 0.8500, 1.1512, 0.8507, 1.1626, 0.8591, 0.8611, 1.1512,\n",
       "                      0.8506, 0.8490, 1.1281, 1.1396, 0.8181, 0.8520, 1.1533, 1.1486, 1.1468,\n",
       "                      0.8486, 0.8685, 1.1521, 1.1959, 0.8517, 1.1192, 1.1726, 0.8472, 0.8481,\n",
       "                      1.1565, 0.8492, 0.8550, 1.1522, 0.8278, 0.8589, 1.1357, 1.1519, 1.1503,\n",
       "                      0.8479, 1.1455, 0.8561, 0.8497, 1.1537, 0.8524, 0.8492, 1.1513, 1.1510,\n",
       "                      0.8499, 1.0351, 0.8535, 0.8032, 0.8481, 1.1016, 0.8472, 0.8482, 0.8498,\n",
       "                      0.8501, 1.1529, 0.8494, 1.1312, 1.1518, 0.8469, 0.8514, 0.8481, 1.1707,\n",
       "                      0.8612, 0.8489, 1.1818, 0.8471, 1.1535, 1.1501, 0.8484, 1.1521, 0.8505,\n",
       "                      0.8292, 1.1509, 0.8462, 0.9470, 1.1501, 0.8514, 0.8507, 0.8480, 1.1514,\n",
       "                      0.8497, 0.8494, 1.1271, 1.1518, 1.1485, 1.1404, 0.8491, 0.8486, 1.1537,\n",
       "                      1.1476, 0.8484, 1.1508, 1.1483, 0.8486, 0.8478, 0.8379, 0.8488, 1.1514,\n",
       "                      1.1517, 1.1357, 0.8460, 1.1509, 0.8441, 0.8710, 0.8431, 1.1528, 0.8479,\n",
       "                      0.8470, 0.8553, 1.1472, 1.1304, 0.8523, 1.1520, 0.8479, 0.8475, 0.8502,\n",
       "                      1.0929, 0.8490, 0.8497, 0.8477, 0.8696, 0.8084, 1.1513, 1.1519, 0.8405,\n",
       "                      1.1517, 1.1409, 0.8528, 0.8471, 1.1525, 0.8482, 1.1516, 0.8682, 0.8490,\n",
       "                      1.1553, 1.1513, 0.8490, 1.1804, 1.1582, 1.1522, 1.1437, 1.1453, 0.8752,\n",
       "                      1.1460, 1.1439, 1.1661, 0.8556, 0.8513, 1.1475, 1.1512, 0.8602, 0.8499,\n",
       "                      0.8453, 0.8588, 0.8541, 0.8509, 1.1498, 0.8491, 1.1521, 0.8557, 0.8563,\n",
       "                      0.8498, 0.8507, 1.1544, 1.1495, 1.0588, 0.8509, 0.8665, 1.1496],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.2.layer_norm2.beta',\n",
       "              tensor([ 0.1519,  0.1511,  0.1552,  0.1533, -0.1514, -0.1514, -0.1491, -0.1538,\n",
       "                      -0.1578, -0.1518, -0.1350, -0.1502, -0.1511,  0.1522,  0.1574, -0.1535,\n",
       "                       0.1524, -0.1611,  0.1518,  0.1217, -0.1599,  0.1541, -0.1504,  0.1896,\n",
       "                      -0.1539,  0.1515, -0.1516,  0.1492,  0.1483,  0.1504,  0.1601,  0.1780,\n",
       "                      -0.1554,  0.1531,  0.1946, -0.1517, -0.1559, -0.1635, -0.1530,  0.1540,\n",
       "                       0.1506, -0.1523, -0.1528, -0.1526, -0.1527, -0.1515, -0.1545, -0.1531,\n",
       "                       0.1721,  0.1530,  0.1500,  0.1753,  0.1603, -0.1503,  0.1510,  0.1446,\n",
       "                       0.1475, -0.1508,  0.1525,  0.1525, -0.1508, -0.1391, -0.1513,  0.1519,\n",
       "                       0.1495, -0.1389,  0.1319, -0.1523, -0.1533,  0.1534,  0.1530, -0.1613,\n",
       "                       0.1807, -0.1518,  0.1486,  0.1517, -0.1883, -0.1477,  0.1532, -0.1511,\n",
       "                       0.1558, -0.1522,  0.1537, -0.1528, -0.1523,  0.1513,  0.1517, -0.1540,\n",
       "                       0.1539,  0.1540,  0.1546, -0.1531, -0.1522, -0.1542,  0.1538,  0.1523,\n",
       "                      -0.1542,  0.1484,  0.1401, -0.1539,  0.1527, -0.1239, -0.1519, -0.1508,\n",
       "                       0.1596,  0.1628, -0.1517,  0.1548, -0.1524, -0.1614,  0.1669,  0.1552,\n",
       "                      -0.1515, -0.1523, -0.1528, -0.1466,  0.1524, -0.1532, -0.1431, -0.1537,\n",
       "                       0.1523,  0.1487,  0.1531,  0.1531,  0.1525,  0.1536,  0.1512,  0.1414,\n",
       "                      -0.1528,  0.1577,  0.1526, -0.1510,  0.1498, -0.1560, -0.1497, -0.1510,\n",
       "                       0.1522,  0.1524,  0.1533,  0.1505,  0.1341, -0.1436,  0.1525, -0.1518,\n",
       "                       0.1523,  0.1791,  0.1524,  0.1534,  0.1522,  0.1603, -0.1559,  0.1454,\n",
       "                       0.1540, -0.1526, -0.1484,  0.1532, -0.1528,  0.0289,  0.1533,  0.1505,\n",
       "                       0.1599,  0.1543, -0.1508, -0.1538, -0.1447,  0.1528, -0.1502, -0.1526,\n",
       "                       0.1513,  0.1528, -0.1541,  0.1528,  0.1617,  0.1714, -0.1540, -0.1773,\n",
       "                       0.1540,  0.1491,  0.1505,  0.1503, -0.1572, -0.1553,  0.1520,  0.1529,\n",
       "                       0.1537, -0.1564,  0.1512, -0.1452,  0.1519, -0.1545, -0.1523,  0.1514,\n",
       "                       0.1494,  0.1572,  0.1503, -0.1527, -0.1448,  0.1577, -0.1519,  0.1525,\n",
       "                       0.1546,  0.1445, -0.1528,  0.1596,  0.1557, -0.1530,  0.1634, -0.1541,\n",
       "                      -0.1520, -0.1559,  0.1523,  0.1512,  0.1481,  0.1526, -0.1518,  0.1513,\n",
       "                       0.1522, -0.1531,  0.1525,  0.1477, -0.1389, -0.1471, -0.1517,  0.1529,\n",
       "                       0.1513,  0.1518, -0.1530,  0.1524, -0.1516, -0.1500,  0.1813, -0.1517,\n",
       "                      -0.1592, -0.1622, -0.1687, -0.1485,  0.1508,  0.1612,  0.1512,  0.1526,\n",
       "                       0.1420, -0.1531, -0.1421, -0.1517,  0.1514,  0.1507,  0.1528, -0.1555,\n",
       "                       0.1528, -0.1535, -0.1521, -0.1508, -0.1542, -0.1568, -0.1511, -0.1520,\n",
       "                      -0.1533, -0.1546,  0.1495,  0.1497, -0.1513, -0.1580,  0.1481,  0.1537,\n",
       "                       0.1467,  0.1510, -0.1513,  0.1532, -0.1538,  0.1368,  0.1543, -0.1664,\n",
       "                      -0.1483,  0.1547,  0.1511, -0.1480,  0.1500, -0.1529, -0.1525, -0.1519,\n",
       "                      -0.1522, -0.1511,  0.1537, -0.1446,  0.1520, -0.1525, -0.1457,  0.1511,\n",
       "                       0.1514, -0.1516,  0.1521, -0.1514,  0.1631, -0.1544, -0.1525,  0.1529,\n",
       "                      -0.1494,  0.1518, -0.1524, -0.1545, -0.1519,  0.1507, -0.1618,  0.1532,\n",
       "                       0.1542, -0.1529,  0.1511, -0.1499, -0.1520, -0.1527, -0.1531,  0.1525,\n",
       "                      -0.1532, -0.1536, -0.1645, -0.1531,  0.1558,  0.1639,  0.1454, -0.1455,\n",
       "                       0.1562,  0.1505, -0.1524, -0.1519,  0.1525,  0.1488, -0.1512,  0.1515,\n",
       "                       0.1521, -0.1539, -0.1512, -0.1501,  0.1484, -0.1551,  0.1480,  0.1515,\n",
       "                       0.1501,  0.1481, -0.1498, -0.1524,  0.1543, -0.1519, -0.1525,  0.1549,\n",
       "                      -0.1550,  0.1440,  0.1549,  0.1530,  0.1576, -0.1541,  0.1515, -0.1525,\n",
       "                      -0.1530, -0.1520, -0.1515, -0.1463, -0.1567, -0.1527,  0.1495, -0.1515,\n",
       "                       0.1550,  0.1531, -0.1522,  0.1581,  0.1531, -0.1491, -0.1551,  0.1531,\n",
       "                      -0.1525,  0.1528,  0.1561,  0.1535, -0.1529,  0.1529, -0.1566, -0.1508,\n",
       "                       0.1516,  0.1497,  0.1530, -0.1303,  0.1563,  0.1531, -0.1518,  0.1553,\n",
       "                       0.1543,  0.1501, -0.1518, -0.1536, -0.1465,  0.1545,  0.1143, -0.1217,\n",
       "                      -0.1481,  0.1522, -0.1533, -0.1540,  0.1535, -0.1529,  0.1536,  0.1458,\n",
       "                       0.1521,  0.1524,  0.1731, -0.1532,  0.1624,  0.1753, -0.1534, -0.1547,\n",
       "                       0.1521,  0.1447, -0.1518, -0.1528,  0.1516,  0.1533, -0.1292, -0.1516,\n",
       "                       0.1504, -0.1584, -0.1509, -0.1558,  0.1533, -0.1534,  0.1500,  0.1544,\n",
       "                      -0.1533, -0.1503, -0.1522, -0.1471, -0.1510, -0.1541,  0.1531,  0.1525,\n",
       "                      -0.1501,  0.1531, -0.1507, -0.1513, -0.1535,  0.1525,  0.1033,  0.1530,\n",
       "                      -0.1517,  0.1456, -0.1509,  0.1507,  0.1509,  0.0561, -0.1593,  0.1297,\n",
       "                       0.1532,  0.1614, -0.1537,  0.1532,  0.1519, -0.1470, -0.1568,  0.1521,\n",
       "                      -0.1529,  0.1511,  0.1530,  0.1501, -0.1561,  0.1528,  0.1526,  0.1556,\n",
       "                      -0.1502,  0.1511,  0.1518, -0.1497, -0.1521,  0.1402, -0.1540,  0.1524,\n",
       "                      -0.1522, -0.1531, -0.1353,  0.1609,  0.1539, -0.1508,  0.1517, -0.1538,\n",
       "                      -0.1564,  0.1541, -0.1559, -0.1511,  0.1469,  0.1606,  0.1491, -0.1482,\n",
       "                       0.1553,  0.1559,  0.1545,  0.1472,  0.1525, -0.1603,  0.1549,  0.1298,\n",
       "                       0.1563,  0.1553, -0.1537,  0.1502,  0.1529,  0.1447, -0.1551, -0.1579,\n",
       "                      -0.1532,  0.1548, -0.1542,  0.1499,  0.1387, -0.1526, -0.1561,  0.1509],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.attention.W_Q.weight',\n",
       "              tensor([[ 0.1974,  0.2238,  0.0796,  ..., -0.1435, -0.1415, -0.0982],\n",
       "                      [ 0.1108,  0.1730,  0.1962,  ..., -0.1072, -0.1008, -0.2390],\n",
       "                      [-0.1044,  0.1086,  0.2138,  ..., -0.1208, -0.1812, -0.1273],\n",
       "                      ...,\n",
       "                      [-0.0810,  0.1180, -0.1811,  ...,  0.1050,  0.1279,  0.1993],\n",
       "                      [ 0.2434,  0.1914, -0.1008,  ...,  0.0419,  0.0834,  0.2056],\n",
       "                      [ 0.1278, -0.1994,  0.0899,  ...,  0.1433, -0.1900, -0.1659]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.attention.W_K.weight',\n",
       "              tensor([[ 0.1322,  0.1433,  0.2231,  ...,  0.1988, -0.1227, -0.1662],\n",
       "                      [-0.1068,  0.1871, -0.1307,  ..., -0.2062,  0.1147,  0.0819],\n",
       "                      [ 0.1540,  0.1369,  0.0758,  ..., -0.1171, -0.1221, -0.0780],\n",
       "                      ...,\n",
       "                      [ 0.1261, -0.2058,  0.1812,  ..., -0.1915,  0.1871,  0.1278],\n",
       "                      [-0.2272, -0.0791,  0.1490,  ...,  0.0913, -0.2010,  0.1189],\n",
       "                      [ 0.1377, -0.0834,  0.2110,  ..., -0.0782,  0.1991, -0.1559]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.attention.W_V.weight',\n",
       "              tensor([[ 0.1645,  0.1538, -0.0889,  ...,  0.1376,  0.2236,  0.2014],\n",
       "                      [-0.2253, -0.1241, -0.1113,  ...,  0.1395,  0.1336,  0.2236],\n",
       "                      [-0.1939, -0.1593, -0.0778,  ...,  0.2260,  0.1943,  0.0912],\n",
       "                      ...,\n",
       "                      [-0.1000, -0.1323, -0.1756,  ..., -0.1093, -0.1513,  0.1645],\n",
       "                      [ 0.1726, -0.2081,  0.2021,  ..., -0.1557, -0.0081,  0.1608],\n",
       "                      [ 0.2105,  0.1915, -0.1367,  ..., -0.1281,  0.1009, -0.0536]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.attention.W_T.weight',\n",
       "              tensor([[-0.1248,  0.1438, -0.1431,  ...,  0.1924,  0.2213, -0.1228],\n",
       "                      [-0.2491,  0.0870, -0.1630,  ..., -0.0133,  0.0921,  0.1187],\n",
       "                      [-0.1984,  0.1943,  0.1293,  ..., -0.1532, -0.1308,  0.0978],\n",
       "                      ...,\n",
       "                      [-0.1123,  0.2302,  0.1689,  ...,  0.1783,  0.0842, -0.1867],\n",
       "                      [-0.1388,  0.1099,  0.1255,  ...,  0.0972,  0.1061, -0.0999],\n",
       "                      [ 0.1326, -0.2055, -0.1067,  ..., -0.1043, -0.1984,  0.1500]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.layer_norm1.gamma',\n",
       "              tensor([1.1500, 0.8798, 1.1659, 0.8411, 1.1585, 1.1533, 0.8983, 1.1516, 0.8528,\n",
       "                      1.1530, 1.1471, 1.1515, 1.1566, 0.8452, 0.8060, 1.1533, 0.8454, 0.8472,\n",
       "                      1.1543, 1.1578, 1.1568, 0.8446, 1.1538, 0.8526, 0.8368, 1.1517, 0.9360,\n",
       "                      0.8440, 1.1531, 0.8490, 0.8462, 1.1496, 1.1789, 1.1516, 1.1594, 1.1518,\n",
       "                      1.1527, 0.8441, 1.1559, 0.8465, 0.8834, 0.8471, 1.1771, 1.1761, 1.1528,\n",
       "                      0.8493, 1.1530, 0.8455, 1.1526, 0.8472, 0.8398, 1.1573, 1.1511, 1.1612,\n",
       "                      1.1567, 0.8542, 0.8490, 1.1487, 0.8418, 0.8460, 0.8462, 0.8518, 1.1534,\n",
       "                      0.8463, 1.1490, 0.8441, 0.8482, 0.8478, 1.1513, 0.8414, 0.8467, 1.1201,\n",
       "                      0.8474, 1.1523, 1.1555, 1.1527, 1.1534, 0.8107, 0.8473, 0.8482, 0.8462,\n",
       "                      1.1563, 1.1508, 1.1464, 0.8472, 1.1573, 1.1552, 1.1673, 1.1693, 0.8454,\n",
       "                      1.1618, 1.1521, 1.1558, 0.8449, 1.1540, 0.8488, 1.1671, 0.8469, 1.1531,\n",
       "                      0.8439, 0.8458, 1.1536, 1.1546, 1.1538, 1.1492, 1.1489, 1.1409, 1.1509,\n",
       "                      0.8474, 1.1532, 0.8466, 0.9260, 1.1579, 1.1524, 0.8474, 1.1465, 0.8450,\n",
       "                      1.1537, 1.1540, 1.1515, 0.8030, 1.1536, 0.8472, 0.8052, 1.1533, 0.8471,\n",
       "                      1.1533, 0.8510, 0.8549, 1.1518, 1.1677, 0.8436, 1.1524, 1.1609, 0.8529,\n",
       "                      0.8463, 0.8476, 1.1533, 0.8453, 0.8067, 1.1538, 1.1536, 0.8457, 1.1524,\n",
       "                      1.1523, 1.1548, 0.8441, 0.8394, 1.1441, 0.9082, 0.8872, 0.8531, 0.8420,\n",
       "                      0.8491, 0.8214, 1.1529, 1.1502, 1.1530, 0.8474, 1.1553, 1.1533, 1.1574,\n",
       "                      0.8439, 1.1716, 0.8384, 1.1532, 0.8444, 1.1586, 1.1527, 0.8029, 0.8468,\n",
       "                      0.8431, 1.1550, 1.1543, 0.8455, 1.2008, 0.8386, 1.1534, 1.1528, 0.8556,\n",
       "                      0.8477, 0.8476, 0.8479, 0.8465, 0.8460, 1.1562, 1.1531, 1.1535, 0.8431,\n",
       "                      1.1519, 1.1531, 1.1541, 0.8460, 0.8544, 1.1444, 1.1560, 0.8423, 1.1181,\n",
       "                      1.1533, 1.1365, 1.1535, 0.8512, 0.8462, 0.8425, 0.8486, 1.1547, 0.8408,\n",
       "                      1.1532, 1.1530, 1.1529, 0.8484, 1.1518, 0.8483, 1.1531, 1.1549, 1.1535,\n",
       "                      1.1531, 0.8448, 1.1503, 1.1534, 0.8450, 1.1525, 1.1499, 1.1485, 1.1548,\n",
       "                      1.1533, 0.8486, 1.1534, 1.1525, 1.1575, 1.1526, 1.1553, 1.1532, 1.1467,\n",
       "                      1.1542, 0.8457, 1.1557, 0.8531, 0.8500, 0.8075, 1.1375, 0.8471, 1.1534,\n",
       "                      1.1538, 0.8480, 1.1542, 0.8368, 0.8464, 0.8450, 1.1907, 1.1541, 0.8380,\n",
       "                      0.8863, 1.1311, 0.8349, 1.1555, 1.1549, 0.8529, 1.1440, 1.1542, 0.8341,\n",
       "                      1.1545, 1.1542, 1.1372, 0.8562, 0.8488, 1.1517, 0.8465, 1.1612, 0.8494,\n",
       "                      1.1563, 1.1525, 0.8474, 1.1534, 1.1531, 0.8580, 0.8491, 0.8462, 0.8459,\n",
       "                      0.8465, 1.1524, 1.1540, 0.8458, 0.8838, 0.8476, 0.8474, 0.8448, 0.8453,\n",
       "                      0.8514, 1.1607, 1.1602, 1.1534, 0.8738, 0.8404, 0.8413, 1.1485, 0.9151,\n",
       "                      0.8477, 1.1540, 0.8406, 1.1576, 1.1533, 1.1528, 0.8388, 1.1530, 1.1413,\n",
       "                      1.1615, 1.1150, 0.8305, 0.8470, 0.8469, 0.8565, 0.8376, 1.1546, 0.8440,\n",
       "                      0.8482, 1.1423, 1.1540, 0.8449, 1.1640, 0.8310, 0.8583, 1.1544, 1.1528,\n",
       "                      0.8420, 0.8460, 1.1539, 1.1532, 1.1536, 1.1532, 1.1517, 1.1535, 1.1518,\n",
       "                      0.8469, 0.8472, 1.1536, 1.1517, 1.1533, 1.1360, 0.8464, 0.8479, 1.1532,\n",
       "                      1.1526, 0.8503, 1.1531, 1.1901, 0.8443, 1.1535, 0.8068, 0.8408, 0.8368,\n",
       "                      1.1550, 0.8467, 1.1551, 0.9297, 1.1617, 1.1592, 1.1531, 0.8425, 1.1615,\n",
       "                      0.8464, 1.1522, 0.8481, 1.1617, 0.8477, 1.1664, 1.1519, 1.1523, 0.8276,\n",
       "                      0.8409, 1.1828, 0.8459, 1.1616, 0.8511, 1.1535, 0.8401, 1.1527, 0.8513,\n",
       "                      0.8431, 1.1543, 0.8465, 1.1571, 0.8449, 0.8454, 0.8510, 1.1751, 1.1540,\n",
       "                      1.1686, 0.8447, 0.8210, 0.8570, 0.8461, 0.9322, 0.8438, 0.8487, 0.8483,\n",
       "                      0.8365, 1.1530, 1.1748, 1.1584, 0.8105, 0.8526, 0.8050, 0.8464, 0.8401,\n",
       "                      0.8392, 0.8275, 0.8386, 1.1532, 0.8461, 1.1557, 0.8466, 1.1618, 0.8437,\n",
       "                      1.1575, 0.8497, 1.1490, 0.8536, 1.1578, 1.1465, 1.1510, 0.8469, 1.1519,\n",
       "                      1.1657, 0.8447, 1.1554, 1.1544, 1.1550, 0.8437, 0.8456, 1.1898, 1.1106,\n",
       "                      1.1536, 0.8459, 1.1540, 1.1437, 0.8447, 1.1593, 1.1540, 0.8454, 0.8470,\n",
       "                      1.1561, 1.1534, 0.8470, 0.8823, 1.1417, 1.1533, 0.8512, 1.1536, 0.8441,\n",
       "                      0.8448, 0.8463, 0.8419, 0.8512, 0.8100, 1.1535, 1.1543, 0.8510, 0.8456,\n",
       "                      1.1575, 1.1534, 1.1224, 0.8457, 0.8503, 1.1545, 1.1522, 0.9668, 0.8458,\n",
       "                      1.1532, 0.8510, 1.1626, 0.8478, 0.8326, 0.8469, 1.1524, 0.8468, 0.8447,\n",
       "                      1.1543, 1.1427, 0.8459, 0.8473, 1.1577, 1.1536, 1.1381, 0.8451, 1.1540,\n",
       "                      1.1539, 0.8625, 0.8705, 0.8586, 0.8392, 1.0975, 1.1532, 0.8413, 0.8484,\n",
       "                      1.1087, 1.1954, 1.1527, 0.8456, 0.8696, 1.1443, 0.8130, 0.8475, 0.8467,\n",
       "                      1.1510, 1.1569, 1.1520, 0.8462, 0.8981, 0.8441, 0.8260, 1.1576],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.layer_norm1.beta',\n",
       "              tensor([-0.1432,  0.1379, -0.1789,  0.1391, -0.1542, -0.1542, -0.1692, -0.1562,\n",
       "                       0.1473, -0.1533,  0.1582, -0.1564, -0.1537,  0.1503, -0.1934,  0.1549,\n",
       "                       0.1534, -0.1504,  0.1567, -0.1426,  0.1562,  0.1488, -0.1538,  0.1425,\n",
       "                      -0.1539,  0.1542,  0.0991,  0.1504,  0.1541,  0.1507,  0.1490,  0.1557,\n",
       "                      -0.1334,  0.1582,  0.1563, -0.1693,  0.1554, -0.1504, -0.1531,  0.1499,\n",
       "                       0.1525, -0.1510, -0.1938,  0.0585, -0.1530,  0.1489, -0.1611, -0.1497,\n",
       "                      -0.1539,  0.1516, -0.1484, -0.1594,  0.1558, -0.1500,  0.1544, -0.1412,\n",
       "                       0.1508, -0.1590,  0.1507,  0.1509, -0.1514, -0.1422,  0.1539,  0.1512,\n",
       "                       0.1585,  0.1505,  0.1515, -0.1510,  0.1577,  0.1512,  0.1509, -0.0798,\n",
       "                       0.1509, -0.1532, -0.1589,  0.1537,  0.1549,  0.1321,  0.1506, -0.1498,\n",
       "                       0.1504, -0.1534,  0.1550,  0.1125, -0.1504,  0.1526,  0.1529,  0.1880,\n",
       "                       0.1061,  0.1512, -0.1567, -0.1542, -0.1547, -0.1498, -0.1551, -0.1466,\n",
       "                       0.1747, -0.1494, -0.1552, -0.1496,  0.1597, -0.1543,  0.1559, -0.1533,\n",
       "                      -0.1543,  0.1546, -0.1574, -0.1586, -0.1515,  0.1544,  0.1497,  0.1252,\n",
       "                      -0.1584, -0.1538, -0.1508,  0.1707,  0.1485, -0.1540, -0.1546,  0.1544,\n",
       "                       0.1715,  0.1546,  0.1512, -0.1787,  0.1540,  0.1504,  0.1535,  0.1476,\n",
       "                      -0.1488, -0.1548, -0.1437, -0.1514,  0.1557,  0.1561,  0.1468, -0.1516,\n",
       "                       0.1510,  0.1552,  0.1484,  0.1912,  0.1540, -0.1544,  0.1503, -0.1546,\n",
       "                       0.1542,  0.1539,  0.1446,  0.1507,  0.1705, -0.1467,  0.1504, -0.1335,\n",
       "                       0.1513,  0.1527,  0.1493,  0.1540,  0.1573,  0.1538,  0.1503,  0.1533,\n",
       "                       0.1539,  0.1537, -0.1514, -0.1642, -0.1616,  0.1532,  0.1480, -0.1532,\n",
       "                       0.1529, -0.1576, -0.1511,  0.1433,  0.1568,  0.1681, -0.1450, -0.1368,\n",
       "                       0.1394,  0.1536,  0.1560,  0.1505, -0.1457,  0.1493,  0.1509,  0.1513,\n",
       "                       0.1514,  0.1549,  0.1535, -0.1569,  0.1502, -0.1524, -0.1531,  0.1553,\n",
       "                       0.1497, -0.1498,  0.1859, -0.1549,  0.1475, -0.1699, -0.1535,  0.1536,\n",
       "                       0.1568, -0.1427, -0.1511,  0.1496,  0.1500, -0.1569,  0.1404, -0.1514,\n",
       "                      -0.1534,  0.1550,  0.1501,  0.1559, -0.1452,  0.1562, -0.1572,  0.1557,\n",
       "                       0.1533, -0.1511,  0.1568,  0.1541, -0.1504, -0.1533, -0.1741,  0.1574,\n",
       "                       0.1553,  0.1531, -0.1512,  0.1553, -0.1548, -0.1525,  0.1560, -0.1531,\n",
       "                      -0.1542,  0.1383, -0.1552, -0.1507,  0.1524, -0.1504, -0.1455, -0.1719,\n",
       "                      -0.1767, -0.1502, -0.1534, -0.1544,  0.1497,  0.1545, -0.1451, -0.1515,\n",
       "                       0.1500, -0.1546, -0.1536,  0.1462,  0.1572,  0.1199,  0.1557,  0.1542,\n",
       "                      -0.1541, -0.1458,  0.1521, -0.1530, -0.1426, -0.1245,  0.1551,  0.1795,\n",
       "                      -0.1436,  0.1494, -0.1555,  0.1510,  0.1538,  0.1514,  0.1570, -0.1535,\n",
       "                      -0.1492, -0.1596, -0.1538, -0.1513,  0.1464, -0.1512, -0.1509, -0.1473,\n",
       "                      -0.1550, -0.1536,  0.1512,  0.0771,  0.1474, -0.1486, -0.0687,  0.1513,\n",
       "                       0.1497, -0.1534,  0.1535, -0.1548,  0.1701, -0.1519,  0.1511,  0.1555,\n",
       "                       0.1122,  0.1509, -0.1555, -0.1467, -0.1534,  0.1536,  0.1525, -0.1547,\n",
       "                       0.1553,  0.1641,  0.1539, -0.1500, -0.1510, -0.1514, -0.1513, -0.1418,\n",
       "                      -0.1459,  0.1517, -0.1480, -0.1483, -0.1539, -0.1542,  0.1476, -0.1545,\n",
       "                       0.1436,  0.1450,  0.1533, -0.1553,  0.1504, -0.1491, -0.1525,  0.1542,\n",
       "                       0.1541,  0.1558, -0.1542, -0.1545,  0.1555, -0.1467, -0.1478,  0.1535,\n",
       "                      -0.1544, -0.1590, -0.1799, -0.1509,  0.1504, -0.1544, -0.1542,  0.1504,\n",
       "                       0.1543, -0.0930,  0.1503, -0.1540,  0.1674, -0.1461,  0.1512, -0.1575,\n",
       "                       0.1506, -0.1552, -0.1510, -0.1576,  0.1580, -0.1535,  0.1487, -0.1547,\n",
       "                       0.1509,  0.1556,  0.1485,  0.1544,  0.1502, -0.1532, -0.1562,  0.1545,\n",
       "                      -0.1476,  0.1482, -0.1637,  0.1514, -0.1682,  0.1475, -0.1533, -0.1505,\n",
       "                       0.1543, -0.1150,  0.1504, -0.1549,  0.1439,  0.1535,  0.1471,  0.1497,\n",
       "                       0.1487, -0.1376, -0.1539, -0.1537, -0.1493,  0.1484, -0.1399, -0.1508,\n",
       "                       0.1761,  0.1495, -0.1502, -0.1478,  0.1462, -0.1541, -0.1483, -0.1539,\n",
       "                       0.1482,  0.1276, -0.1673, -0.1490, -0.1512,  0.1472, -0.1478, -0.1518,\n",
       "                      -0.1562, -0.1493, -0.1531, -0.1511,  0.1549,  0.1514,  0.1559,  0.1490,\n",
       "                      -0.1614, -0.1456, -0.1529,  0.1548,  0.1554, -0.1483,  0.1856,  0.1509,\n",
       "                      -0.1511, -0.1532, -0.1539, -0.1536, -0.1511, -0.1494,  0.1588,  0.1102,\n",
       "                      -0.1537,  0.1500, -0.1572, -0.1561, -0.1485, -0.1636,  0.1534,  0.1494,\n",
       "                      -0.1505,  0.1551, -0.1558,  0.1463, -0.1473, -0.1630,  0.1543,  0.1546,\n",
       "                       0.1533,  0.1509, -0.1502,  0.1513,  0.1510,  0.1448,  0.1920,  0.1534,\n",
       "                       0.1617, -0.1551,  0.1515, -0.1573,  0.1592,  0.1447,  0.1507,  0.1563,\n",
       "                      -0.1532, -0.1505, -0.1402,  0.1536, -0.1536, -0.1552, -0.1552,  0.1414,\n",
       "                      -0.1477, -0.1511, -0.1527, -0.1521,  0.1514,  0.1526,  0.1544, -0.1474,\n",
       "                      -0.1512, -0.1578, -0.1585,  0.1557,  0.1514,  0.1537, -0.1571,  0.1399,\n",
       "                       0.1480,  0.1478,  0.1505, -0.1452,  0.1584, -0.1500,  0.1470,  0.0966,\n",
       "                      -0.1466, -0.1611, -0.1513, -0.0454, -0.1671, -0.0733, -0.1503, -0.1506,\n",
       "                      -0.1515, -0.1618, -0.1552,  0.1512,  0.1483, -0.1520, -0.1512,  0.1542],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.ffn.layer1.weight',\n",
       "              tensor([[-0.1860,  0.1919,  0.1055,  ...,  0.1070, -0.1958, -0.1714],\n",
       "                      [-0.1836,  0.1069,  0.1589,  ...,  0.1807, -0.1680, -0.1930],\n",
       "                      [-0.1731, -0.1127,  0.1051,  ..., -0.1511, -0.1077, -0.1802],\n",
       "                      ...,\n",
       "                      [-0.1136,  0.1714, -0.1313,  ...,  0.1036, -0.1548,  0.1544],\n",
       "                      [ 0.1357, -0.1729, -0.1445,  ..., -0.1463, -0.1069,  0.1108],\n",
       "                      [ 0.0329,  0.0182, -0.0122,  ...,  0.0442, -0.0202,  0.0147]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.ffn.layer1.bias',\n",
       "              tensor([-0.1706, -0.1361, -0.1747,  ..., -0.1819,  0.1235, -0.0416],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.ffn.layer2.weight',\n",
       "              tensor([[ 0.1593,  0.1440,  0.1851,  ...,  0.1549,  0.2125,  0.1429],\n",
       "                      [ 0.1407,  0.1470,  0.1257,  ...,  0.1230,  0.1552, -0.0589],\n",
       "                      [ 0.1093,  0.1730,  0.1157,  ...,  0.1434,  0.1416, -0.0462],\n",
       "                      ...,\n",
       "                      [-0.1408, -0.1609, -0.1349,  ..., -0.1303, -0.1095,  0.0362],\n",
       "                      [-0.1881, -0.1395, -0.1378,  ..., -0.1434, -0.1289,  0.0112],\n",
       "                      [ 0.1705,  0.1176,  0.1552,  ...,  0.1643,  0.1559,  0.0018]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.ffn.layer2.bias',\n",
       "              tensor([ 0.1588,  0.1673,  0.1644,  0.1703, -0.1570, -0.1373, -0.1497, -0.1448,\n",
       "                       0.1620, -0.1690,  0.1729,  0.1430, -0.1431,  0.1368, -0.1712,  0.1362,\n",
       "                       0.1556, -0.1387,  0.1567, -0.1307,  0.1540,  0.1597, -0.1316,  0.1598,\n",
       "                       0.1713,  0.1699, -0.1553,  0.1371,  0.1452,  0.1645,  0.1508,  0.1691,\n",
       "                      -0.1635, -0.1682, -0.1332, -0.1682,  0.1369, -0.1663, -0.1547, -0.1333,\n",
       "                       0.1514, -0.1429, -0.1369, -0.1310, -0.1384,  0.1594, -0.1420, -0.1631,\n",
       "                      -0.1729,  0.1669, -0.1739,  0.1470,  0.1635, -0.1681,  0.1339,  0.1549,\n",
       "                       0.1548,  0.1604,  0.1504,  0.1428, -0.1467, -0.1672,  0.1464,  0.1525,\n",
       "                       0.1369,  0.1438,  0.1707, -0.1476,  0.1533,  0.1657,  0.1494, -0.1561,\n",
       "                       0.1528, -0.1621, -0.1376,  0.1676,  0.1313, -0.1582,  0.1417, -0.1668,\n",
       "                       0.1420, -0.1628,  0.1431,  0.1692, -0.1667,  0.1353,  0.1339,  0.1341,\n",
       "                       0.1505,  0.1413, -0.1403, -0.1463, -0.1573, -0.1576, -0.1550,  0.1670,\n",
       "                       0.1634, -0.1415, -0.1540, -0.1675, -0.1377, -0.1326,  0.1427, -0.1390,\n",
       "                      -0.1440,  0.1539,  0.1607,  0.1504, -0.1642,  0.1741,  0.1707, -0.1644,\n",
       "                      -0.1524, -0.1700, -0.1413,  0.1702,  0.1315, -0.1506, -0.1435,  0.1613,\n",
       "                       0.1447,  0.1733,  0.1472, -0.1499,  0.1460,  0.1445,  0.1742,  0.1347,\n",
       "                      -0.1307, -0.1439, -0.1655, -0.1325,  0.1379,  0.1375, -0.1444, -0.1590,\n",
       "                       0.1646,  0.1416,  0.1324,  0.1581, -0.1600, -0.1538,  0.1508, -0.1604,\n",
       "                       0.1737,  0.1337,  0.1604,  0.1709,  0.1346, -0.1427,  0.1660, -0.1321,\n",
       "                       0.1325,  0.1308,  0.1325,  0.1342,  0.1482,  0.1741,  0.1609,  0.1717,\n",
       "                       0.1699,  0.1581, -0.1541, -0.1547,  0.1743,  0.1327,  0.1343, -0.1304,\n",
       "                       0.1547, -0.1427, -0.1711, -0.1413,  0.1530, -0.1330,  0.1742, -0.1521,\n",
       "                       0.1570,  0.1329,  0.1689,  0.1447, -0.1332,  0.1676,  0.1312,  0.1535,\n",
       "                       0.1646,  0.1506,  0.1644, -0.1741,  0.1605, -0.1590, -0.1624,  0.1596,\n",
       "                       0.1725,  0.1532, -0.1733, -0.1675,  0.1644,  0.1420, -0.1735,  0.1428,\n",
       "                       0.1437,  0.1549, -0.1688,  0.1552, -0.1723, -0.1342, -0.1445, -0.1604,\n",
       "                      -0.1396,  0.1633,  0.1719,  0.1525, -0.1352,  0.1454, -0.1674,  0.1305,\n",
       "                       0.1352, -0.1440,  0.1489,  0.1587, -0.1727, -0.1453,  0.1683,  0.1306,\n",
       "                       0.1651,  0.1685, -0.1572,  0.1658, -0.1476, -0.1534,  0.1339, -0.1417,\n",
       "                      -0.1460, -0.1744, -0.1701, -0.1449,  0.1491, -0.1596, -0.1523, -0.1359,\n",
       "                       0.1610, -0.1450, -0.1473, -0.1540,  0.1684,  0.1468,  0.1368, -0.1631,\n",
       "                       0.1631, -0.1587, -0.1350, -0.1359,  0.1626,  0.1727,  0.1549,  0.1469,\n",
       "                      -0.1597, -0.1338,  0.1653,  0.1393, -0.1581, -0.1680,  0.1516, -0.1324,\n",
       "                      -0.1491,  0.1627, -0.1685,  0.1693,  0.1687,  0.1453,  0.1496, -0.1583,\n",
       "                      -0.1567, -0.1631, -0.1547, -0.1591, -0.1580, -0.1640, -0.1544, -0.1584,\n",
       "                       0.1570, -0.1512,  0.1308,  0.1401,  0.1336, -0.1392,  0.1701,  0.1421,\n",
       "                       0.1496, -0.1378,  0.1401, -0.1710,  0.1458, -0.1665,  0.1369,  0.1547,\n",
       "                       0.1684,  0.1679, -0.1552, -0.1327, -0.1503,  0.1423,  0.1589, -0.1582,\n",
       "                       0.1392,  0.1604,  0.1379, -0.1729,  0.1745, -0.1323, -0.1312, -0.1730,\n",
       "                       0.1666,  0.1595, -0.1513,  0.1332, -0.1563, -0.1630,  0.1579, -0.1346,\n",
       "                       0.1587, -0.1635,  0.1681, -0.1592,  0.1743, -0.1404, -0.1631,  0.1643,\n",
       "                       0.1711,  0.1331, -0.1322, -0.1518,  0.1661, -0.1312, -0.1743,  0.1468,\n",
       "                      -0.1715,  0.1725,  0.1712, -0.1463,  0.1314, -0.1334, -0.1436,  0.1693,\n",
       "                       0.1430, -0.1442,  0.1663, -0.1536,  0.1709,  0.1320,  0.1692, -0.1451,\n",
       "                       0.1374, -0.1416, -0.1727, -0.1466, -0.1705, -0.1608,  0.1711, -0.1641,\n",
       "                       0.1405,  0.1736,  0.1519, -0.1706,  0.1593, -0.1604, -0.1581,  0.1399,\n",
       "                      -0.1501, -0.1493,  0.1382,  0.1359, -0.1393,  0.1601, -0.1469, -0.1311,\n",
       "                       0.1740,  0.1396,  0.1649, -0.1593, -0.1484,  0.1736,  0.1493,  0.1736,\n",
       "                      -0.1444, -0.1622, -0.1481, -0.1694, -0.1724,  0.1640, -0.1604, -0.1718,\n",
       "                       0.1538,  0.1339, -0.1337, -0.1407,  0.1477, -0.1554, -0.1623, -0.1687,\n",
       "                       0.1716, -0.1551, -0.1402, -0.1615, -0.1426,  0.1613, -0.1674, -0.1386,\n",
       "                      -0.1407, -0.1399, -0.1623, -0.1504,  0.1496,  0.1316,  0.1567,  0.1398,\n",
       "                       0.1359, -0.1520, -0.1443,  0.1319,  0.1360, -0.1732,  0.1329,  0.1618,\n",
       "                      -0.1447, -0.1458, -0.1738, -0.1706, -0.1612, -0.1364,  0.1545,  0.1735,\n",
       "                      -0.1435,  0.1471, -0.1718, -0.1412,  0.1563, -0.1551,  0.1465,  0.1726,\n",
       "                      -0.1330,  0.1616, -0.1522, -0.1530, -0.1321, -0.1725,  0.1342, -0.1666,\n",
       "                       0.1515,  0.1307, -0.1341,  0.1462,  0.1565,  0.1435, -0.1386,  0.1322,\n",
       "                       0.1528, -0.1368,  0.1419, -0.1713,  0.1541,  0.1308,  0.1365, -0.1625,\n",
       "                      -0.1736, -0.1462,  0.1417,  0.1649, -0.1498, -0.1693,  0.1403,  0.1451,\n",
       "                      -0.1420, -0.1640,  0.1390, -0.1734,  0.1456,  0.1723,  0.1480, -0.1684,\n",
       "                      -0.1331, -0.1572, -0.1578,  0.1693,  0.1614,  0.1380,  0.1490,  0.1593,\n",
       "                      -0.1369,  0.1543,  0.1460, -0.1583,  0.1584, -0.1549, -0.1578, -0.1401,\n",
       "                      -0.1476, -0.1346, -0.1308,  0.1515,  0.1505,  0.1400, -0.1481, -0.1355,\n",
       "                      -0.1572, -0.1463, -0.1667,  0.1433, -0.1434, -0.1577, -0.1445,  0.1346],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.layer_norm2.gamma',\n",
       "              tensor([1.1726, 0.8410, 0.8266, 0.8246, 1.1586, 1.1562, 0.8365, 1.1584, 1.1781,\n",
       "                      1.1568, 1.1680, 0.8162, 1.1601, 0.8401, 1.1934, 1.1619, 0.8687, 0.8308,\n",
       "                      1.1654, 0.8284, 1.1605, 0.8341, 1.1574, 0.8418, 0.8478, 1.1577, 0.8234,\n",
       "                      0.8410, 1.1566, 1.1638, 0.8423, 1.1604, 0.8343, 0.8415, 0.8440, 1.1579,\n",
       "                      1.1689, 1.1706, 0.8384, 1.1570, 1.1542, 0.8424, 1.1681, 1.1624, 1.1559,\n",
       "                      0.8403, 1.1440, 0.8388, 1.1567, 0.8448, 0.8384, 0.8231, 1.1585, 0.8340,\n",
       "                      1.1667, 1.1599, 0.8343, 0.8385, 0.8227, 0.8381, 0.8421, 0.8414, 1.1570,\n",
       "                      0.8420, 1.1630, 0.8335, 1.1804, 0.8436, 1.1579, 0.8348, 0.8409, 0.8331,\n",
       "                      0.8351, 1.1571, 1.1255, 1.1555, 1.1711, 1.1578, 0.8393, 0.8334, 0.8383,\n",
       "                      1.1655, 1.1592, 0.8465, 0.8439, 0.8358, 0.8376, 1.1529, 0.8371, 0.8416,\n",
       "                      1.1595, 1.1567, 1.1532, 0.8416, 1.1633, 1.1635, 1.1608, 0.8429, 1.1633,\n",
       "                      0.8428, 0.8925, 1.1613, 1.1574, 0.8017, 0.8422, 1.1583, 0.8391, 0.8338,\n",
       "                      0.8365, 1.1566, 0.8373, 1.1627, 1.1637, 1.1561, 0.8429, 1.1557, 0.8388,\n",
       "                      1.1568, 1.1686, 1.1578, 1.1697, 1.1634, 0.8650, 1.1588, 1.1623, 0.8388,\n",
       "                      1.1591, 0.8440, 0.8375, 1.1600, 1.1586, 0.8293, 1.1642, 0.8483, 0.8506,\n",
       "                      0.8427, 0.8423, 1.1708, 0.8395, 1.1734, 0.8473, 1.1578, 0.8349, 1.1604,\n",
       "                      1.1606, 1.1597, 0.8404, 0.8277, 1.1582, 0.8393, 0.8352, 0.8351, 0.8268,\n",
       "                      0.8353, 1.0698, 1.1613, 1.1573, 1.1563, 0.8398, 1.1596, 1.1671, 1.1591,\n",
       "                      0.8421, 0.8335, 0.8387, 1.1556, 0.8392, 1.1714, 1.1549, 0.8296, 0.8338,\n",
       "                      1.1613, 1.1594, 0.8293, 1.1651, 0.8332, 0.8295, 1.1586, 1.1563, 1.1596,\n",
       "                      0.8398, 0.8337, 0.8383, 0.8421, 1.0801, 1.1660, 1.1593, 1.1574, 1.1514,\n",
       "                      0.8429, 1.1568, 0.8311, 0.8434, 1.1690, 0.8386, 1.1631, 0.8375, 0.8287,\n",
       "                      1.1570, 0.8323, 1.1629, 1.1567, 0.8144, 0.8319, 1.1515, 1.1601, 1.1577,\n",
       "                      0.8376, 1.1574, 1.1584, 0.8409, 0.8122, 0.8345, 1.1604, 1.1663, 1.1573,\n",
       "                      0.8378, 0.8433, 1.1571, 1.1590, 0.8344, 1.1561, 0.8211, 1.1583, 1.0893,\n",
       "                      1.1550, 0.8407, 1.1642, 1.1581, 1.1607, 1.1654, 1.1585, 0.8348, 1.1613,\n",
       "                      1.1644, 0.8327, 0.8446, 0.8384, 0.8422, 0.8199, 1.1341, 0.8252, 1.1574,\n",
       "                      1.1547, 0.8371, 1.1571, 0.8676, 0.8445, 0.8284, 1.1733, 1.1581, 1.1681,\n",
       "                      0.8795, 1.1655, 0.9108, 0.8390, 1.1607, 0.8375, 1.1737, 1.1168, 0.8307,\n",
       "                      0.8428, 1.1571, 0.8229, 0.8361, 0.8409, 1.1566, 0.8414, 0.8255, 0.8366,\n",
       "                      1.1657, 1.1599, 0.8172, 1.1657, 1.1565, 1.1773, 1.1591, 0.8453, 0.8429,\n",
       "                      0.8429, 0.8414, 1.1592, 0.8411, 0.8366, 0.8379, 0.8436, 1.1694, 0.8431,\n",
       "                      0.8393, 0.8332, 1.1664, 1.1592, 1.1817, 1.1584, 0.8426, 1.1594, 1.1616,\n",
       "                      0.8393, 1.1585, 0.8393, 1.1598, 1.1571, 0.8340, 0.9032, 1.1594, 0.8453,\n",
       "                      1.1597, 1.1710, 1.1593, 0.8436, 0.8443, 1.0905, 1.1809, 1.1916, 0.8370,\n",
       "                      1.1660, 1.1659, 1.1561, 0.8279, 1.1749, 0.8397, 1.1563, 0.8411, 1.1553,\n",
       "                      0.8426, 0.8366, 1.1529, 1.1577, 1.1753, 1.1647, 1.1588, 1.1591, 1.1597,\n",
       "                      0.8423, 0.8425, 0.8394, 1.1586, 0.8418, 0.8390, 0.8448, 0.8397, 1.1854,\n",
       "                      1.1616, 0.8283, 1.1687, 1.1382, 0.8181, 1.1609, 0.8576, 1.1679, 1.1706,\n",
       "                      1.1589, 0.8363, 0.8420, 1.1563, 1.1588, 0.8416, 1.1597, 0.8937, 1.1586,\n",
       "                      0.8428, 1.1577, 0.8396, 0.8131, 0.8297, 0.8385, 1.1635, 1.1584, 0.8388,\n",
       "                      1.1635, 0.8255, 0.8430, 1.1586, 0.8345, 1.1612, 0.9197, 1.1579, 0.8577,\n",
       "                      0.8364, 1.1789, 0.8297, 1.1583, 0.8397, 0.8311, 1.1608, 1.1583, 0.8346,\n",
       "                      0.8318, 0.8338, 0.8437, 0.8412, 0.8411, 1.1523, 0.8411, 0.8406, 0.8418,\n",
       "                      0.8316, 1.1564, 0.8303, 1.1629, 1.1429, 1.1709, 1.1755, 0.8393, 0.8379,\n",
       "                      0.8391, 0.8374, 0.8222, 0.8115, 0.8396, 1.1563, 0.8407, 0.8261, 0.8404,\n",
       "                      1.1556, 0.8362, 0.8302, 0.8282, 0.8160, 0.8267, 1.1620, 0.8428, 1.1608,\n",
       "                      1.1527, 0.8413, 1.1609, 1.1567, 1.1579, 0.8387, 0.8416, 0.8342, 0.8290,\n",
       "                      1.1562, 0.9054, 1.1594, 1.1655, 0.8699, 1.1601, 1.1586, 0.8365, 0.8374,\n",
       "                      1.1572, 1.1585, 1.1587, 0.8370, 1.1589, 1.1587, 0.8447, 1.1568, 1.1705,\n",
       "                      0.8396, 0.8449, 0.8361, 1.1155, 0.8430, 1.1575, 1.1690, 0.8401, 0.8450,\n",
       "                      0.8363, 1.1577, 0.7987, 0.8363, 0.8365, 1.1566, 1.1044, 1.1664, 1.2055,\n",
       "                      1.1555, 0.8318, 1.1378, 0.8455, 1.1601, 0.8423, 1.1533, 0.8446, 1.1636,\n",
       "                      0.8321, 0.8581, 0.8224, 0.8438, 1.1572, 1.1581, 1.1687, 0.8430, 1.1612,\n",
       "                      1.1672, 0.8427, 0.8106, 1.1220, 0.8373, 0.8294, 1.1574, 0.8338, 1.1587,\n",
       "                      1.1639, 0.8248, 1.1342, 0.8436, 1.1574, 1.1649, 1.1609, 0.8379, 0.8407,\n",
       "                      0.8307, 1.1639, 1.1554, 0.8432, 1.1488, 0.8426, 1.1602, 1.1607],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.3.layer_norm2.beta',\n",
       "              tensor([ 0.1559,  0.1496,  0.1381,  0.1259, -0.1572, -0.1566, -0.1461, -0.1576,\n",
       "                       0.1604, -0.1567,  0.1632,  0.1330, -0.1569,  0.1456, -0.1575,  0.1604,\n",
       "                       0.1702, -0.1415,  0.1563, -0.1429,  0.1567,  0.1429, -0.1577,  0.1444,\n",
       "                       0.1640,  0.1547, -0.1360,  0.1465,  0.1575,  0.1584,  0.1458,  0.1565,\n",
       "                      -0.1423, -0.1422, -0.1735, -0.1618,  0.1561, -0.1543, -0.1478, -0.1923,\n",
       "                       0.1521, -0.1479, -0.1585, -0.1620, -0.1555,  0.1448, -0.1421, -0.1458,\n",
       "                      -0.1567,  0.1488, -0.1432,  0.1379,  0.1594, -0.1468,  0.1630,  0.1645,\n",
       "                       0.1442,  0.1298,  0.1483,  0.1453, -0.1490, -0.1394,  0.1564,  0.1485,\n",
       "                       0.1648,  0.1466,  0.1653, -0.1461,  0.1605,  0.1421,  0.1483, -0.1315,\n",
       "                       0.1443, -0.1561, -0.1487,  0.1572,  0.1563, -0.1681,  0.1412, -0.1470,\n",
       "                       0.1445, -0.1553,  0.1599, -0.1864, -0.1472,  0.1477,  0.1470,  0.1309,\n",
       "                       0.1356,  0.1484, -0.1571, -0.1561, -0.1557, -0.1461, -0.1630, -0.0705,\n",
       "                       0.1620, -0.1450, -0.1530, -0.1467, -0.1546, -0.1595,  0.1579, -0.1344,\n",
       "                      -0.1241,  0.1556,  0.1349,  0.0964, -0.1438,  0.1618,  0.1407, -0.1579,\n",
       "                      -0.1595, -0.1565, -0.1468,  0.1793,  0.1448, -0.1582, -0.1608,  0.1573,\n",
       "                       0.1614,  0.1570,  0.1526, -0.1638,  0.1549,  0.1422,  0.1564,  0.1473,\n",
       "                      -0.1441, -0.1594, -0.1745, -0.1476,  0.1579, -0.1893, -0.1706, -0.1488,\n",
       "                       0.1470,  0.1626,  0.1405,  0.1592, -0.0929, -0.1569,  0.1492, -0.1578,\n",
       "                       0.1620,  0.1578,  0.1445,  0.1448,  0.1603, -0.1463,  0.1446, -0.1423,\n",
       "                       0.1461,  0.1361,  0.1196,  0.1558,  0.1618,  0.1567,  0.1452,  0.1549,\n",
       "                       0.1588,  0.1567, -0.1476, -0.1394,  0.1493,  0.1555,  0.1436, -0.1554,\n",
       "                       0.1559, -0.1477, -0.1473, -0.1665,  0.1591, -0.1437,  0.1591, -0.1425,\n",
       "                       0.1271,  0.1600,  0.1600,  0.1560, -0.1480,  0.1398,  0.1472,  0.1480,\n",
       "                       0.1465,  0.1556,  0.1529, -0.1596,  0.1541, -0.1474, -0.1562,  0.1437,\n",
       "                       0.1479,  0.1598, -0.1431, -0.1642,  0.1413,  0.1374, -0.1586,  0.1437,\n",
       "                       0.1575,  0.1761, -0.1445,  0.1414, -0.1836, -0.1631, -0.1612, -0.1469,\n",
       "                      -0.1555,  0.1585,  0.1471,  0.1441, -0.1438,  0.1621, -0.1526,  0.1566,\n",
       "                       0.1484, -0.1485,  0.1571,  0.1577, -0.1352, -0.1571,  0.1260,  0.1560,\n",
       "                       0.1426,  0.1559, -0.1472,  0.1797, -0.1593, -0.1557,  0.1636, -0.1554,\n",
       "                      -0.1462, -0.1602, -0.1611, -0.1477,  0.1483, -0.1459, -0.1460, -0.1466,\n",
       "                       0.1503, -0.1471, -0.1568, -0.1570,  0.1499,  0.1585,  0.1660, -0.1485,\n",
       "                       0.1441, -0.1579, -0.1564, -0.1761,  0.1669,  0.1715,  0.1843,  0.1432,\n",
       "                      -0.1582, -0.1392,  0.1536, -0.1134, -0.1416, -0.1460,  0.1570, -0.1373,\n",
       "                      -0.1346,  0.1446, -0.1590,  0.1480,  0.1431,  0.1352,  0.1622, -0.1574,\n",
       "                      -0.1359, -0.1591, -0.1565, -0.1649, -0.1581, -0.1480, -0.1478, -0.1443,\n",
       "                       0.1050, -0.1564,  0.1483,  0.1330,  0.1411, -0.1352,  0.1675,  0.1478,\n",
       "                       0.1448, -0.1447,  0.1589, -0.1557,  0.1695, -0.1572,  0.1485,  0.1579,\n",
       "                      -0.1811,  0.1475, -0.1638, -0.1440, -0.1554,  0.1578,  0.1454, -0.1663,\n",
       "                       0.1570,  0.0178,  0.1553, -0.1678,  0.1780, -0.1483, -0.1466, -0.1439,\n",
       "                       0.1860,  0.1615, -0.1378,  0.1782, -0.1534, -0.1575,  0.1456, -0.1583,\n",
       "                       0.1453, -0.1592,  0.1475, -0.1567,  0.1461, -0.1319, -0.1515,  0.1575,\n",
       "                       0.1593,  0.1659, -0.1570, -0.1573,  0.1588, -0.1448, -0.1480,  0.1474,\n",
       "                      -0.1546,  0.1282,  0.1303, -0.1486,  0.1460, -0.1686, -0.1739,  0.1406,\n",
       "                       0.1569, -0.0967,  0.1442, -0.1575,  0.1682,  0.1641,  0.1564, -0.1639,\n",
       "                       0.1459, -0.1477, -0.1560, -0.1573, -0.1417, -0.1570,  0.1621, -0.1590,\n",
       "                       0.1473,  0.1597,  0.1364, -0.1355,  0.1458, -0.1354, -0.1598,  0.1578,\n",
       "                      -0.1452, -0.1822,  0.1384,  0.1481, -0.1595,  0.1364, -0.1564, -0.1640,\n",
       "                       0.1584,  0.1696,  0.1456, -0.1621, -0.1042,  0.1556,  0.1424,  0.1376,\n",
       "                      -0.1709, -0.1562, -0.1451, -0.1485, -0.1492,  0.1445, -0.1462, -0.1472,\n",
       "                       0.1830,  0.1389, -0.1435, -0.1434,  0.1406, -0.1568, -0.1425, -0.1608,\n",
       "                      -0.1785, -0.1608, -0.1585, -0.1426, -0.1476,  0.1421, -0.1422, -0.1481,\n",
       "                      -0.1434, -0.1464, -0.1532, -0.1471,  0.1464,  0.1468,  0.1569,  0.0928,\n",
       "                       0.1422, -0.1468, -0.1504,  0.1403,  0.1690, -0.1460,  0.1795,  0.1537,\n",
       "                      -0.1475, -0.1554, -0.1558, -0.1556, -0.1474, -0.1418,  0.1405,  0.1037,\n",
       "                      -0.1569,  0.1541, -0.1642, -0.1639,  0.1965, -0.1598,  0.1573,  0.1426,\n",
       "                      -0.1457,  0.1577, -0.1582, -0.1774, -0.1099, -0.1620,  0.1583, -0.1346,\n",
       "                       0.1576,  0.1575, -0.1474,  0.1486,  0.1468,  0.1446, -0.1470,  0.1567,\n",
       "                       0.1586, -0.1452,  0.1484, -0.1454,  0.1587,  0.1320,  0.1475, -0.1465,\n",
       "                      -0.1571, -0.1470,  0.1636,  0.1593, -0.1560, -0.1489, -0.1822,  0.1519,\n",
       "                      -0.1620, -0.1472, -0.1327, -0.1497,  0.1555,  0.1422,  0.1542, -0.1372,\n",
       "                      -0.1476, -0.1583, -0.1593,  0.1554,  0.1483,  0.1578,  0.1621,  0.1465,\n",
       "                      -0.1395,  0.0935,  0.1437, -0.1430,  0.1607, -0.1406, -0.1621, -0.1592,\n",
       "                      -0.1406, -0.1336, -0.1486,  0.1579,  0.1573,  0.1602, -0.1432, -0.1476,\n",
       "                      -0.1443, -0.1859, -0.1576,  0.1488, -0.1503, -0.1487, -0.1554,  0.1537],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.attention.W_Q.weight',\n",
       "              tensor([[ 0.1119,  0.0867,  0.1782,  ..., -0.0982,  0.1775,  0.1852],\n",
       "                      [ 0.1514,  0.1662,  0.1514,  ..., -0.1269,  0.2078, -0.2001],\n",
       "                      [ 0.1569,  0.1452, -0.1238,  ...,  0.0993, -0.2423, -0.2125],\n",
       "                      ...,\n",
       "                      [-0.0855,  0.1875, -0.2066,  ...,  0.1880,  0.0336,  0.1153],\n",
       "                      [-0.1501,  0.0433, -0.2137,  ..., -0.1147, -0.0764, -0.1814],\n",
       "                      [-0.2587,  0.0799,  0.1536,  ...,  0.2298, -0.0486, -0.1517]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.attention.W_K.weight',\n",
       "              tensor([[ 0.0828,  0.1281, -0.2273,  ..., -0.1525, -0.0819, -0.2268],\n",
       "                      [-0.0938,  0.1201,  0.1248,  ..., -0.2192,  0.1464,  0.1739],\n",
       "                      [-0.1162,  0.1806, -0.0985,  ..., -0.2039,  0.2215,  0.1806],\n",
       "                      ...,\n",
       "                      [ 0.0988, -0.1143,  0.1290,  ..., -0.1381,  0.1224,  0.0929],\n",
       "                      [ 0.0656,  0.1483,  0.1159,  ..., -0.1527, -0.1750, -0.1961],\n",
       "                      [ 0.2334,  0.1023, -0.0375,  ...,  0.1033, -0.0887, -0.1803]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.attention.W_V.weight',\n",
       "              tensor([[ 0.0931,  0.1549,  0.1594,  ..., -0.0968,  0.1891, -0.1619],\n",
       "                      [-0.2280, -0.1971,  0.2071,  ..., -0.1939,  0.2040, -0.1567],\n",
       "                      [-0.0910,  0.1257,  0.2224,  ..., -0.0915, -0.1636,  0.0522],\n",
       "                      ...,\n",
       "                      [ 0.1048, -0.1739, -0.1979,  ...,  0.1580, -0.1586,  0.1040],\n",
       "                      [ 0.2387, -0.0881, -0.2281,  ...,  0.1718, -0.1024,  0.1244],\n",
       "                      [-0.1642,  0.1264,  0.2060,  ..., -0.0873, -0.0868,  0.0920]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.attention.W_T.weight',\n",
       "              tensor([[-0.1540,  0.1090,  0.0917,  ..., -0.0896, -0.1136, -0.2051],\n",
       "                      [ 0.0910,  0.1095,  0.1189,  ..., -0.2067, -0.1588,  0.1221],\n",
       "                      [-0.0855,  0.1529, -0.0741,  ...,  0.2040,  0.0819,  0.1621],\n",
       "                      ...,\n",
       "                      [ 0.1767,  0.0892,  0.1660,  ..., -0.1503, -0.1699,  0.1298],\n",
       "                      [ 0.1387,  0.1406,  0.1646,  ..., -0.1376, -0.1510,  0.1785],\n",
       "                      [-0.1375, -0.1470, -0.2128,  ...,  0.2148,  0.1889, -0.1809]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.layer_norm1.gamma',\n",
       "              tensor([1.1637, 1.1552, 0.8546, 0.8140, 0.8620, 1.1545, 1.1542, 1.1555, 1.1544,\n",
       "                      1.1551, 1.1847, 1.1531, 1.1438, 0.8422, 0.8455, 1.1553, 1.1555, 0.8491,\n",
       "                      1.1573, 0.8453, 1.1536, 0.8464, 1.1523, 0.8476, 0.8467, 0.8410, 0.9810,\n",
       "                      0.8421, 1.1556, 0.8437, 1.1550, 1.1541, 1.1546, 0.8447, 1.1547, 0.8829,\n",
       "                      1.1613, 1.1541, 1.1570, 1.1544, 0.8443, 0.8913, 0.8513, 1.1538, 1.1555,\n",
       "                      1.1542, 0.8437, 0.8429, 1.1541, 0.8408, 1.1544, 0.8451, 1.1533, 0.8416,\n",
       "                      0.8466, 1.1588, 0.8492, 0.8456, 0.8174, 0.8417, 0.8439, 0.8449, 1.1551,\n",
       "                      0.8432, 0.8454, 1.1538, 1.1609, 0.8425, 0.8582, 1.1570, 1.1544, 0.8440,\n",
       "                      1.1651, 0.8451, 0.8462, 1.1575, 1.1518, 1.1530, 0.8492, 1.1558, 1.1502,\n",
       "                      1.1539, 1.1541, 0.8433, 1.1549, 0.8388, 0.8453, 1.1538, 1.1534, 0.8450,\n",
       "                      1.1548, 0.8397, 1.1549, 0.8349, 1.1560, 1.1552, 0.8586, 0.8460, 0.8469,\n",
       "                      0.8481, 1.1549, 0.8435, 1.1543, 0.8454, 1.1548, 0.8431, 0.8457, 1.1548,\n",
       "                      1.1570, 1.1546, 0.8441, 1.1540, 0.8440, 1.1556, 0.8431, 1.1485, 0.8416,\n",
       "                      1.1528, 0.8454, 0.8390, 0.8474, 0.8609, 1.1566, 1.1550, 1.1536, 0.8530,\n",
       "                      0.8418, 1.1526, 1.1579, 1.1532, 1.1612, 0.8834, 0.8424, 1.1535, 0.8460,\n",
       "                      1.1534, 0.8447, 1.1569, 1.1565, 1.1445, 0.8415, 1.1707, 0.8453, 1.1529,\n",
       "                      1.1539, 1.1613, 1.1547, 0.8439, 1.1545, 0.8380, 1.1304, 0.8711, 0.8441,\n",
       "                      0.8439, 1.1534, 1.1427, 1.1623, 1.1559, 0.8468, 1.1514, 0.8503, 0.8446,\n",
       "                      0.8382, 1.1546, 0.8446, 1.1557, 1.1548, 1.1581, 1.1552, 0.8436, 0.8424,\n",
       "                      1.1531, 1.1474, 0.8436, 1.1537, 0.8372, 1.1536, 0.8402, 1.1471, 1.1555,\n",
       "                      0.8426, 0.8445, 0.8674, 0.8438, 0.8403, 1.1551, 0.8461, 1.1545, 1.1478,\n",
       "                      1.1543, 0.8427, 1.1579, 0.8442, 1.1683, 1.1534, 0.8427, 1.1280, 0.8440,\n",
       "                      0.8482, 1.1571, 1.1716, 0.8439, 0.8446, 1.1184, 0.8451, 0.8528, 0.8463,\n",
       "                      0.8396, 1.1559, 1.1548, 0.8435, 0.8432, 0.8433, 1.1510, 0.8468, 1.1537,\n",
       "                      0.8442, 0.8425, 1.1539, 1.1554, 1.1522, 0.8403, 0.8450, 1.1549, 0.8388,\n",
       "                      1.1546, 1.1554, 0.8438, 1.1541, 1.1541, 0.8468, 1.1547, 1.1551, 0.8415,\n",
       "                      1.1535, 0.8432, 0.8430, 1.1522, 1.1530, 0.8408, 1.1530, 0.8399, 1.1628,\n",
       "                      0.8643, 1.1545, 1.1551, 1.1546, 0.8412, 1.1535, 1.1543, 1.1535, 1.1597,\n",
       "                      1.1519, 0.8547, 0.8477, 0.8485, 1.1569, 0.8439, 0.8481, 1.1523, 1.1488,\n",
       "                      0.8448, 1.1563, 0.8488, 0.8377, 0.8439, 1.1541, 0.8445, 0.8263, 0.8724,\n",
       "                      0.8439, 1.1502, 1.1550, 0.8464, 1.1544, 1.1543, 0.8435, 0.8441, 1.1541,\n",
       "                      0.8446, 0.8557, 0.8570, 1.1547, 0.8548, 0.8469, 1.1515, 0.8456, 0.8440,\n",
       "                      0.8561, 0.8445, 1.1528, 1.1543, 1.1558, 1.1589, 0.8404, 1.1545, 1.1516,\n",
       "                      1.1581, 1.1635, 0.8445, 1.1520, 1.1561, 0.8437, 1.1552, 1.1539, 0.8633,\n",
       "                      0.8438, 0.8743, 0.8444, 0.8436, 0.8446, 1.1683, 1.1174, 1.1650, 1.1527,\n",
       "                      1.1310, 0.8474, 1.1530, 0.8441, 0.8467, 0.8432, 0.8454, 0.8442, 1.1547,\n",
       "                      1.1821, 1.1539, 0.8430, 1.1545, 0.8430, 0.8466, 1.1552, 1.1531, 0.8483,\n",
       "                      0.8436, 0.8447, 0.8461, 0.8427, 1.1536, 0.8475, 0.8441, 0.8414, 1.1547,\n",
       "                      0.8713, 0.8449, 0.8425, 1.1775, 0.8512, 1.1550, 0.8566, 0.8523, 1.1551,\n",
       "                      1.1444, 0.8441, 0.8407, 0.8391, 1.1551, 0.8447, 1.1523, 0.8438, 1.1539,\n",
       "                      1.1491, 1.1552, 1.1538, 0.8432, 1.1496, 1.1548, 1.1556, 1.1692, 1.1468,\n",
       "                      0.8496, 1.1364, 0.8442, 1.1547, 1.1546, 1.1531, 1.1605, 1.1558, 0.8409,\n",
       "                      0.8447, 1.2053, 0.8466, 1.1574, 0.8419, 0.8497, 1.1543, 0.8423, 0.8445,\n",
       "                      0.8662, 0.8549, 1.1563, 0.8437, 1.1546, 0.8892, 0.8470, 1.1547, 0.8425,\n",
       "                      1.1534, 1.1532, 0.8441, 1.1540, 0.8419, 1.1526, 0.8492, 0.8414, 0.8199,\n",
       "                      0.8437, 0.8427, 1.1550, 1.1529, 0.8462, 0.8441, 0.8367, 1.1543, 1.1558,\n",
       "                      1.1529, 0.8585, 1.1565, 0.8442, 0.8379, 1.1523, 1.1480, 0.8463, 1.1623,\n",
       "                      0.8394, 0.8484, 1.1552, 1.1523, 0.8427, 0.8431, 0.8445, 1.1490, 0.8440,\n",
       "                      1.1550, 0.8486, 1.1563, 1.1537, 0.8486, 0.8466, 1.1521, 0.8448, 0.8416,\n",
       "                      1.1541, 1.1521, 1.1500, 1.1548, 1.1537, 1.1559, 0.8531, 1.1622, 0.8654,\n",
       "                      1.1530, 0.8435, 0.8447, 1.1447, 0.8441, 1.1534, 1.1545, 1.1547, 0.8434,\n",
       "                      0.8372, 1.1448, 0.8452, 1.1566, 0.8470, 1.1576, 0.8378, 1.1545, 1.1575,\n",
       "                      1.1556, 1.1518, 1.1533, 0.8449, 1.1946, 0.8438, 1.1317, 1.1559, 0.8473,\n",
       "                      0.8382, 1.1541, 0.8450, 0.8435, 1.1553, 1.1532, 0.8405, 0.8445, 0.8422,\n",
       "                      1.1526, 0.8455, 0.8437, 0.8520, 0.8445, 0.8523, 0.8477, 0.8420, 0.9027,\n",
       "                      0.8594, 0.8434, 1.1570, 0.8432, 1.1555, 1.1559, 0.8401, 0.8506, 1.1555,\n",
       "                      1.2056, 1.1351, 1.1545, 0.8393, 0.8415, 0.8418, 0.8430, 1.1554],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.layer_norm1.beta',\n",
       "              tensor([-0.1279, -0.1555,  0.1292,  0.1647,  0.1380, -0.1564,  0.1602, -0.1566,\n",
       "                       0.1570, -0.1562,  0.1826, -0.1594, -0.1576,  0.1481,  0.1466,  0.1563,\n",
       "                       0.1576,  0.1409,  0.1580,  0.1481,  0.1573,  0.1471, -0.1588,  0.1401,\n",
       "                      -0.1544,  0.1440,  0.1869,  0.1468,  0.1607,  0.1464,  0.1565,  0.1581,\n",
       "                       0.1572, -0.1431,  0.1566, -0.2066,  0.1698, -0.1553, -0.1576, -0.1562,\n",
       "                       0.1473, -0.1574, -0.1561, -0.1581, -0.1554,  0.1577, -0.1452, -0.1473,\n",
       "                      -0.1559,  0.1491,  0.1582,  0.1445, -0.1590, -0.1472, -0.1415,  0.1623,\n",
       "                      -0.1465,  0.1459,  0.1471,  0.1479, -0.1480, -0.1445,  0.1563,  0.1485,\n",
       "                       0.1449,  0.1812,  0.1579, -0.1471, -0.1333, -0.1646,  0.1550, -0.1465,\n",
       "                      -0.1418, -0.1453, -0.1440,  0.1574,  0.1565,  0.1568,  0.1452, -0.1565,\n",
       "                       0.1530, -0.1557,  0.1560,  0.1495,  0.1583,  0.1483,  0.1373,  0.1565,\n",
       "                      -0.1586,  0.1473, -0.1581, -0.1456, -0.1553, -0.1489, -0.1566,  0.1569,\n",
       "                      -0.1423, -0.1463,  0.1459, -0.1302, -0.1561,  0.1466,  0.1570, -0.1462,\n",
       "                      -0.1569,  0.1470,  0.1450, -0.1567, -0.1570,  0.1568,  0.1430, -0.1567,\n",
       "                      -0.1385, -0.1561, -0.1468,  0.1847,  0.1478, -0.1638,  0.1432,  0.1483,\n",
       "                      -0.0854, -0.1409,  0.1582, -0.1566,  0.1574,  0.1323,  0.1469,  0.1571,\n",
       "                       0.1623, -0.1585,  0.1586, -0.1411,  0.1490, -0.1557, -0.1480, -0.1561,\n",
       "                       0.1479,  0.1590, -0.1571, -0.1704,  0.1420, -0.1547,  0.1389, -0.1555,\n",
       "                       0.1605,  0.1735,  0.1560,  0.1474,  0.1573, -0.1484, -0.1465, -0.1632,\n",
       "                      -0.1457, -0.1455, -0.1564,  0.1538,  0.1676,  0.1561,  0.1449, -0.1571,\n",
       "                       0.1564,  0.1453, -0.1470, -0.1563,  0.1470,  0.1553, -0.1703, -0.1562,\n",
       "                       0.1558, -0.1474, -0.1486, -0.1555,  0.1552, -0.1463,  0.1650, -0.1537,\n",
       "                       0.1574, -0.1520,  0.1638,  0.1565, -0.1467,  0.1464,  0.1464,  0.1450,\n",
       "                       0.1474,  0.1524,  0.1468, -0.1571, -0.1538, -0.1592, -0.1478,  0.1596,\n",
       "                       0.1459, -0.1668,  0.1594,  0.0832,  0.1548,  0.1481,  0.1421,  0.1557,\n",
       "                       0.1423,  0.1466, -0.1456, -0.1710,  0.1454,  0.0955,  0.1451, -0.1479,\n",
       "                      -0.1557,  0.1562,  0.1474,  0.1468, -0.1458,  0.1570,  0.1439,  0.1555,\n",
       "                       0.1474, -0.1472,  0.1573,  0.1556,  0.1568, -0.1467,  0.1445,  0.1553,\n",
       "                       0.1527,  0.1563,  0.1594, -0.1436, -0.1552, -0.1561,  0.1455, -0.1558,\n",
       "                      -0.1567, -0.1481, -0.1574,  0.1455,  0.1473, -0.1572,  0.1571, -0.1476,\n",
       "                       0.1590,  0.1471, -0.1651,  0.1428,  0.1564,  0.1563,  0.1551, -0.1494,\n",
       "                       0.1561, -0.1563, -0.1558, -0.1591,  0.1580,  0.1384,  0.1719,  0.1404,\n",
       "                      -0.1584, -0.1470, -0.1520, -0.1604, -0.1419, -0.1459,  0.1377,  0.1457,\n",
       "                       0.1339,  0.1458, -0.1560,  0.1473,  0.1501,  0.1368, -0.1466, -0.1538,\n",
       "                       0.1586,  0.1422, -0.1563,  0.1589, -0.1479, -0.1470,  0.1597, -0.1461,\n",
       "                       0.1321,  0.1364,  0.1558,  0.1313,  0.1414,  0.1609, -0.1473,  0.1468,\n",
       "                       0.1350, -0.1441,  0.1576, -0.1559,  0.1561,  0.0816,  0.1474,  0.1550,\n",
       "                      -0.1701,  0.1567, -0.2245, -0.1464, -0.1553,  0.1576,  0.1474, -0.1553,\n",
       "                       0.1576,  0.1429,  0.1469,  0.1305, -0.1450, -0.1487, -0.1459, -0.1405,\n",
       "                       0.1536,  0.1596,  0.1561,  0.1480,  0.1454, -0.1648,  0.1458, -0.1424,\n",
       "                       0.1468,  0.1469,  0.1468, -0.1557,  0.1264,  0.1577, -0.1470,  0.1568,\n",
       "                       0.1443, -0.1458, -0.1551, -0.1584, -0.1425, -0.1464, -0.1435,  0.1413,\n",
       "                      -0.1481, -0.1711,  0.1331, -0.1473,  0.1476, -0.1607,  0.1308, -0.1454,\n",
       "                       0.1469, -0.1324,  0.1474, -0.1571, -0.1397, -0.1312,  0.1558, -0.1353,\n",
       "                       0.1470, -0.1474, -0.1486, -0.1571, -0.1738, -0.1566, -0.1490, -0.1560,\n",
       "                       0.1528,  0.1593, -0.1667, -0.1468,  0.1341,  0.1671, -0.1584,  0.1887,\n",
       "                       0.1502,  0.1412,  0.1213,  0.1445, -0.1571, -0.1577, -0.1575, -0.1600,\n",
       "                       0.1561,  0.1450,  0.1468, -0.1865,  0.1408,  0.1552,  0.1459,  0.1341,\n",
       "                      -0.1560, -0.1470,  0.1467,  0.1576, -0.1553,  0.1787, -0.1471,  0.1693,\n",
       "                       0.1599,  0.1446,  0.1570, -0.1472, -0.1593, -0.1571, -0.1459, -0.0158,\n",
       "                       0.1457, -0.1644,  0.1554, -0.1455, -0.1368,  0.1470, -0.1479, -0.1578,\n",
       "                       0.1554, -0.1439, -0.1480, -0.1483, -0.1702, -0.1610,  0.1561,  0.1272,\n",
       "                       0.1611, -0.1470,  0.1416,  0.1553,  0.1441, -0.1460,  0.1762,  0.1472,\n",
       "                      -0.1405, -0.1566, -0.1569, -0.1454, -0.1480, -0.1455,  0.1544,  0.1463,\n",
       "                      -0.1565, -0.1551,  0.1624, -0.1638, -0.1440,  0.1456,  0.1588, -0.1467,\n",
       "                      -0.1469,  0.1650, -0.1637,  0.1555,  0.1579, -0.1549,  0.1558,  0.0733,\n",
       "                       0.1634,  0.1390, -0.1548,  0.1473,  0.1478,  0.2039, -0.1481,  0.1553,\n",
       "                       0.1569, -0.1350,  0.1460, -0.1436,  0.1630,  0.1384,  0.1571,  0.1476,\n",
       "                      -0.1595, -0.1479,  0.1573,  0.1612, -0.1560, -0.1436,  0.1563,  0.1437,\n",
       "                       0.1502, -0.1471,  0.1632, -0.1556,  0.1211,  0.1306,  0.1565, -0.1460,\n",
       "                      -0.1475, -0.1604, -0.1566,  0.1470,  0.1472,  0.1439,  0.1568,  0.1449,\n",
       "                      -0.1434,  0.1410,  0.1461, -0.1341, -0.1351, -0.1478,  0.1273,  0.1377,\n",
       "                      -0.1478, -0.1647, -0.1479,  0.1574,  0.1540,  0.1470,  0.1429, -0.1552,\n",
       "                      -0.1323, -0.1710, -0.1563,  0.1473, -0.1462, -0.1478, -0.1474,  0.1553],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.ffn.layer1.weight',\n",
       "              tensor([[ 0.1734, -0.1861, -0.1422,  ...,  0.1496,  0.1712,  0.1133],\n",
       "                      [-0.1164, -0.1346,  0.1761,  ..., -0.1167, -0.1109, -0.1232],\n",
       "                      [ 0.1077, -0.1170, -0.1683,  ...,  0.1156,  0.1453,  0.1697],\n",
       "                      ...,\n",
       "                      [ 0.1551, -0.1126, -0.1131,  ...,  0.1157,  0.1257,  0.1186],\n",
       "                      [ 0.1096, -0.1555, -0.1772,  ...,  0.1967,  0.1102,  0.1369],\n",
       "                      [ 0.1934,  0.1602, -0.1598,  ...,  0.1397,  0.1337,  0.1541]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.ffn.layer1.bias',\n",
       "              tensor([ 0.1433, -0.1632,  0.1294,  ...,  0.1194,  0.1127,  0.1860],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.ffn.layer2.weight',\n",
       "              tensor([[ 0.1379, -0.1766, -0.1336,  ..., -0.1708, -0.1994, -0.0439],\n",
       "                      [-0.1522, -0.1974, -0.1913,  ..., -0.1387, -0.1180, -0.1507],\n",
       "                      [-0.1322, -0.1999, -0.1269,  ..., -0.1332, -0.1103, -0.1775],\n",
       "                      ...,\n",
       "                      [-0.1923, -0.1714, -0.1745,  ..., -0.1072, -0.1543, -0.1055],\n",
       "                      [-0.1766, -0.1751, -0.1688,  ..., -0.1914, -0.1237, -0.1528],\n",
       "                      [-0.1138,  0.1326,  0.1653,  ...,  0.1398,  0.1446,  0.1364]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.ffn.layer2.bias',\n",
       "              tensor([-0.1473, -0.1549, -0.1443,  0.1622,  0.1488, -0.1392, -0.1572, -0.1614,\n",
       "                       0.1586, -0.1620,  0.1539, -0.1717, -0.1324,  0.1376,  0.1704,  0.1502,\n",
       "                       0.1391,  0.1456, -0.1337,  0.1540,  0.1608,  0.1496, -0.1735,  0.1465,\n",
       "                      -0.1641, -0.1609,  0.1649,  0.1664,  0.1318,  0.1596,  0.1674,  0.1677,\n",
       "                       0.1494, -0.1387,  0.1462,  0.1698,  0.1548, -0.1556, -0.1385, -0.1658,\n",
       "                       0.1638, -0.1354, -0.1328, -0.1595, -0.1393,  0.1507, -0.1614, -0.1564,\n",
       "                      -0.1709,  0.1489,  0.1624,  0.1635, -0.1664, -0.1544, -0.1307, -0.1706,\n",
       "                      -0.1340,  0.1322,  0.1582,  0.1599, -0.1669, -0.1644,  0.1645,  0.1323,\n",
       "                      -0.1557,  0.1635,  0.1369, -0.1700, -0.1690, -0.1550,  0.1728, -0.1582,\n",
       "                      -0.1609,  0.1727, -0.1394,  0.1343,  0.1560,  0.1314,  0.1327, -0.1335,\n",
       "                       0.1349, -0.1514,  0.1673,  0.1477,  0.1307,  0.1600,  0.1476,  0.1392,\n",
       "                      -0.1519,  0.1417, -0.1354, -0.1611, -0.1567, -0.1614, -0.1626,  0.1623,\n",
       "                       0.1330, -0.1342,  0.1381, -0.1358, -0.1641,  0.1503,  0.1537, -0.1563,\n",
       "                      -0.1648,  0.1606,  0.1729, -0.1689, -0.1371,  0.1568, -0.1395, -0.1358,\n",
       "                      -0.1741, -0.1386, -0.1518,  0.1418,  0.1484,  0.1401, -0.1560,  0.1451,\n",
       "                       0.1397, -0.1305,  0.1349, -0.1570,  0.1382, -0.1699,  0.1656,  0.1525,\n",
       "                       0.1626, -0.1701,  0.1429, -0.1572,  0.1572, -0.1481, -0.1521, -0.1544,\n",
       "                       0.1558,  0.1531, -0.1436, -0.1549, -0.1674, -0.1359,  0.1449, -0.1392,\n",
       "                      -0.1313,  0.1378,  0.1533,  0.1306,  0.1570, -0.1634, -0.1407, -0.1311,\n",
       "                      -0.1705, -0.1517, -0.1414,  0.1659,  0.1724,  0.1482,  0.1677, -0.1500,\n",
       "                       0.1431, -0.1304, -0.1381, -0.1665,  0.1611,  0.1483, -0.1666, -0.1408,\n",
       "                       0.1402, -0.1607, -0.1644, -0.1418,  0.1398, -0.1342,  0.1326, -0.1643,\n",
       "                       0.1402, -0.1579, -0.1533,  0.1387, -0.1734,  0.1578,  0.1654,  0.1742,\n",
       "                       0.1476,  0.1606,  0.1637, -0.1431, -0.1468, -0.1594, -0.1500,  0.1695,\n",
       "                       0.1620,  0.1640,  0.1737,  0.1324,  0.1622,  0.1425, -0.1346,  0.1684,\n",
       "                       0.1482,  0.1356, -0.1636,  0.1659,  0.1428,  0.1525,  0.1667, -0.1321,\n",
       "                      -0.1323,  0.1733,  0.1326,  0.1330, -0.1617,  0.1384, -0.1524,  0.1541,\n",
       "                       0.1439, -0.1341,  0.1716,  0.1740,  0.1658, -0.1498,  0.1524,  0.1609,\n",
       "                       0.1312,  0.1534,  0.1496, -0.1358, -0.1423, -0.1714,  0.1384, -0.1659,\n",
       "                      -0.1460, -0.1683, -0.1476,  0.1560,  0.1503, -0.1654,  0.1499, -0.1736,\n",
       "                       0.1649,  0.1443, -0.1727,  0.1382,  0.1580,  0.1312,  0.1561, -0.1426,\n",
       "                       0.1373, -0.1314, -0.1639, -0.1508,  0.1450,  0.1474, -0.1652,  0.1642,\n",
       "                      -0.1376,  0.1556,  0.1536, -0.1424,  0.1577, -0.1572, -0.1644,  0.1614,\n",
       "                       0.1563, -0.1362, -0.1690,  0.1399,  0.1397, -0.1314, -0.1416, -0.1429,\n",
       "                       0.1664, -0.1391, -0.1368,  0.1421, -0.1452, -0.1702,  0.1361, -0.1424,\n",
       "                      -0.1600, -0.1611,  0.1493,  0.1533,  0.1391,  0.1354, -0.1590,  0.1603,\n",
       "                       0.1352, -0.1517,  0.1408, -0.1688,  0.1528, -0.1581,  0.1358,  0.1507,\n",
       "                       0.1496,  0.1738, -0.1476, -0.1374, -0.1354,  0.1461,  0.1565, -0.1506,\n",
       "                       0.1476, -0.1510,  0.1482, -0.1670,  0.1494, -0.1525, -0.1695, -0.1394,\n",
       "                       0.1713,  0.1724,  0.1405,  0.1369,  0.1599,  0.1477,  0.1565,  0.1613,\n",
       "                       0.1363,  0.1732,  0.1382, -0.1712,  0.1651,  0.1422, -0.1354,  0.1646,\n",
       "                       0.1723, -0.1356, -0.1585, -0.1433,  0.1486, -0.1684, -0.1452,  0.1337,\n",
       "                      -0.1557,  0.1482, -0.1655, -0.1600,  0.1516, -0.1550, -0.1654, -0.1416,\n",
       "                       0.1462,  0.1479,  0.1390, -0.1400, -0.1425, -0.1348,  0.1394,  0.1383,\n",
       "                       0.1411, -0.1713, -0.1580, -0.1723, -0.1707, -0.1428, -0.1356, -0.1498,\n",
       "                       0.1350, -0.1456,  0.1477, -0.1452, -0.1594,  0.1504, -0.1456, -0.1544,\n",
       "                       0.1540,  0.1699,  0.1399,  0.1500, -0.1454,  0.1539, -0.1346, -0.1637,\n",
       "                       0.1303,  0.1525,  0.1354, -0.1326, -0.1773,  0.1536, -0.1412,  0.1572,\n",
       "                      -0.1471, -0.1652,  0.1453, -0.1742, -0.1432,  0.1682, -0.1341, -0.1318,\n",
       "                       0.1583, -0.1685,  0.1593, -0.1606, -0.1568, -0.1648, -0.1360, -0.1459,\n",
       "                       0.1567, -0.1284,  0.1861,  0.1327, -0.1543,  0.1360, -0.1661, -0.1732,\n",
       "                       0.1634,  0.1436, -0.1624, -0.1451, -0.1730, -0.1521,  0.1535, -0.1363,\n",
       "                       0.1524, -0.1429, -0.1538,  0.1313,  0.1318, -0.1495, -0.1688,  0.1593,\n",
       "                       0.1530, -0.1384, -0.1409, -0.1692, -0.1390, -0.1525,  0.1517,  0.1469,\n",
       "                      -0.1743, -0.1610, -0.1403,  0.1535, -0.1377,  0.1422,  0.1667, -0.1543,\n",
       "                      -0.1412,  0.1455, -0.1567,  0.1622,  0.1531, -0.1441,  0.1740, -0.1573,\n",
       "                      -0.1633,  0.1716, -0.1626,  0.1691,  0.1506, -0.1736, -0.1351,  0.1702,\n",
       "                       0.1392,  0.1585,  0.1560, -0.1573,  0.1467,  0.1617,  0.1579,  0.1657,\n",
       "                      -0.1482, -0.1466,  0.1580,  0.1432, -0.1457, -0.1304,  0.1337,  0.1685,\n",
       "                       0.1371, -0.1669,  0.1324, -0.1404,  0.1542,  0.1565,  0.1304, -0.1313,\n",
       "                      -0.1659,  0.1627, -0.1464,  0.1306,  0.1637,  0.1604,  0.1437, -0.1374,\n",
       "                      -0.1718,  0.1415,  0.1386, -0.1324, -0.1426, -0.1412, -0.1502,  0.1442,\n",
       "                      -0.1446, -0.1655, -0.1743,  0.1401,  0.1530,  0.1598,  0.1719, -0.1573,\n",
       "                      -0.1333,  0.1557, -0.1359,  0.1700,  0.1506, -0.1370, -0.1311,  0.1686],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.layer_norm2.gamma',\n",
       "              tensor([1.1812, 1.1863, 1.1560, 1.1530, 0.9113, 1.1595, 1.1594, 1.1638, 1.2381,\n",
       "                      1.1544, 1.1695, 1.2003, 1.1698, 0.9250, 0.8378, 1.1767, 0.9321, 0.9967,\n",
       "                      0.9894, 0.8088, 1.2221, 0.8093, 1.1870, 0.7865, 1.1493, 0.8372, 1.2193,\n",
       "                      0.7955, 1.1543, 0.7894, 1.1804, 1.0669, 1.1628, 0.8294, 1.1308, 0.8528,\n",
       "                      1.1816, 1.1089, 1.1641, 1.2024, 1.1205, 1.1813, 0.8488, 1.1433, 1.1462,\n",
       "                      1.1585, 0.8569, 0.8317, 1.1710, 1.1387, 1.1655, 0.7910, 1.1315, 1.0920,\n",
       "                      0.8085, 0.8386, 0.8290, 0.8270, 1.1355, 0.8908, 0.8065, 0.8114, 1.1090,\n",
       "                      0.8925, 1.0387, 1.1502, 0.9264, 0.8097, 0.8687, 1.1986, 1.1316, 0.8337,\n",
       "                      1.1649, 1.0207, 0.9103, 1.2091, 1.1592, 1.1442, 1.0076, 0.8986, 0.9669,\n",
       "                      1.2095, 1.1258, 0.9535, 1.2040, 0.8508, 0.8212, 0.9744, 1.2019, 0.8086,\n",
       "                      1.1361, 0.7751, 0.9451, 0.8433, 1.1614, 1.1688, 1.1416, 0.8363, 1.1565,\n",
       "                      0.7921, 1.1687, 0.8392, 1.1613, 0.8238, 1.0737, 0.8271, 0.9482, 0.9589,\n",
       "                      1.1505, 1.1664, 1.1543, 1.1836, 1.0287, 1.2116, 0.9909, 1.1433, 1.1582,\n",
       "                      0.8312, 0.8403, 0.9644, 0.8487, 0.8743, 1.0658, 1.1560, 0.9352, 1.1953,\n",
       "                      0.8247, 1.1067, 1.1782, 0.8101, 0.8353, 0.8107, 0.7953, 1.1120, 0.9139,\n",
       "                      1.1465, 0.8509, 1.1932, 1.1974, 1.1489, 1.0783, 1.2366, 1.1170, 1.0971,\n",
       "                      0.8139, 1.1671, 1.1325, 0.8412, 1.1777, 0.8131, 0.9383, 1.1476, 1.0003,\n",
       "                      0.8480, 1.2220, 1.1489, 1.1844, 0.8359, 1.0047, 1.1188, 1.0749, 1.1088,\n",
       "                      0.8456, 1.1726, 0.7926, 1.2034, 1.1677, 1.1912, 1.1931, 0.7904, 0.8037,\n",
       "                      1.1530, 1.1295, 0.8845, 1.1683, 0.7862, 1.1089, 0.9326, 0.8320, 1.1672,\n",
       "                      0.8474, 0.8687, 1.1626, 0.8699, 0.8180, 1.1041, 0.8428, 1.1729, 0.8278,\n",
       "                      1.0753, 0.8271, 1.1554, 0.8395, 1.1382, 1.1727, 0.8414, 1.1992, 0.8474,\n",
       "                      1.1091, 1.1961, 0.9656, 0.8077, 0.8773, 0.8863, 0.8480, 0.8159, 0.8493,\n",
       "                      1.1539, 1.1932, 1.1351, 0.8581, 1.1006, 0.8302, 1.1582, 1.0178, 1.1866,\n",
       "                      1.1751, 0.8153, 1.1122, 1.1803, 1.2578, 1.1162, 1.1279, 1.1584, 1.1312,\n",
       "                      1.1881, 1.2289, 0.8459, 1.1859, 1.1134, 0.8444, 1.1654, 1.1220, 1.1502,\n",
       "                      1.1680, 0.8108, 0.8170, 1.1525, 1.1735, 0.8312, 1.1498, 1.1858, 0.8524,\n",
       "                      0.8625, 1.1701, 1.1802, 1.1824, 1.1576, 1.1609, 1.1930, 1.1605, 1.1537,\n",
       "                      0.8849, 0.7946, 0.8200, 0.7828, 1.2198, 1.0245, 0.9516, 1.1760, 1.1714,\n",
       "                      0.8276, 1.0977, 0.8069, 0.8106, 1.0477, 1.1848, 0.8118, 0.9008, 1.1600,\n",
       "                      0.8398, 1.1443, 1.2123, 1.1634, 1.1527, 1.1954, 0.8345, 0.8240, 1.1291,\n",
       "                      0.8266, 1.2057, 1.1396, 1.1735, 0.7992, 0.8446, 1.1564, 0.7934, 0.9710,\n",
       "                      0.8738, 0.8531, 1.1711, 1.2131, 1.2099, 1.1565, 0.8614, 1.1663, 0.8180,\n",
       "                      1.1970, 1.0936, 0.7990, 1.1442, 1.2055, 1.1650, 1.1770, 1.1730, 1.0178,\n",
       "                      0.8536, 1.2019, 1.1437, 0.8108, 0.8789, 0.9510, 1.2396, 1.1248, 1.0306,\n",
       "                      1.1624, 0.7892, 1.0636, 0.8252, 0.8490, 0.8268, 0.8017, 0.8499, 1.1997,\n",
       "                      1.0096, 1.1419, 0.8166, 1.1375, 0.8823, 0.8274, 0.8528, 0.9130, 1.1506,\n",
       "                      1.1362, 0.8197, 0.8195, 0.8813, 0.8112, 0.9267, 0.8290, 0.8317, 1.2195,\n",
       "                      1.1605, 0.8168, 0.8266, 1.1675, 0.9854, 1.1554, 0.9194, 0.8202, 1.1559,\n",
       "                      1.2111, 0.8197, 0.8297, 1.1219, 1.1503, 1.2374, 1.1327, 0.7670, 0.9132,\n",
       "                      1.0900, 0.8223, 0.8208, 0.8673, 1.1066, 1.1389, 1.1117, 0.7994, 0.8091,\n",
       "                      0.8419, 1.1788, 1.1595, 1.2143, 0.8840, 1.1974, 0.8867, 1.1803, 0.8567,\n",
       "                      0.8163, 0.8587, 1.1433, 0.8229, 0.8315, 0.8651, 1.1912, 0.8133, 1.0300,\n",
       "                      0.8067, 0.9605, 1.1088, 0.8030, 0.8022, 1.1823, 0.8596, 1.1388, 0.8150,\n",
       "                      1.1861, 1.1495, 0.9098, 1.1795, 1.0292, 1.1030, 0.8535, 1.1082, 1.0054,\n",
       "                      0.8029, 0.8075, 0.8555, 1.1967, 1.0608, 0.8158, 1.0955, 1.2152, 1.1683,\n",
       "                      1.1450, 1.1919, 1.1954, 0.7656, 0.8309, 1.0379, 1.1016, 0.8780, 0.8209,\n",
       "                      0.8790, 1.1224, 1.1736, 1.0923, 0.8536, 0.8302, 0.8137, 1.1811, 0.8092,\n",
       "                      1.1760, 0.8471, 1.1232, 1.1002, 0.8325, 0.8095, 1.1810, 0.9501, 0.8413,\n",
       "                      1.2029, 1.1990, 0.8628, 1.1616, 0.9486, 1.1569, 0.8802, 0.8592, 1.2026,\n",
       "                      1.1767, 0.8233, 0.8062, 1.1130, 0.8042, 0.9226, 1.1814, 1.0965, 0.8318,\n",
       "                      1.1662, 1.0058, 0.8156, 1.2216, 0.8694, 1.1877, 0.8666, 1.1480, 0.8420,\n",
       "                      1.1793, 1.1673, 1.1666, 0.8169, 0.9289, 0.8092, 1.0876, 1.1697, 0.8274,\n",
       "                      1.1422, 1.1626, 0.8132, 0.8039, 0.8549, 1.1943, 0.8969, 0.8332, 1.1496,\n",
       "                      1.1603, 0.8138, 0.8807, 0.8574, 1.0435, 0.8331, 0.8486, 0.8463, 1.1917,\n",
       "                      0.9340, 1.1705, 0.8281, 0.7861, 1.1587, 0.8743, 0.8167, 0.8582, 1.0711,\n",
       "                      0.8479, 0.9162, 1.1639, 0.8939, 0.8515, 0.8280, 0.7914, 1.1893],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.4.layer_norm2.beta',\n",
       "              tensor([-0.1482, -0.1864, -0.1955,  0.1765,  0.1168, -0.1764, -0.1695, -0.1958,\n",
       "                       0.1789, -0.1875,  0.1938, -0.2067, -0.1882,  0.1297,  0.1273,  0.1814,\n",
       "                       0.1557,  0.1670,  0.0606,  0.1166,  0.2008,  0.1212, -0.1852,  0.1083,\n",
       "                      -0.1825, -0.1456,  0.2052,  0.1134,  0.1741,  0.1282,  0.1820,  0.1351,\n",
       "                       0.1810, -0.1223,  0.1824,  0.1444,  0.1863, -0.1435, -0.1822, -0.1849,\n",
       "                       0.1727, -0.1991, -0.1575, -0.1817, -0.1893,  0.1818, -0.1357, -0.1097,\n",
       "                      -0.1816,  0.1797,  0.2269,  0.1015, -0.1729, -0.1709, -0.1044, -0.1248,\n",
       "                      -0.1180,  0.1228,  0.1619,  0.1367, -0.1176, -0.1204,  0.1697,  0.1227,\n",
       "                      -0.1698, -0.1318,  0.1416, -0.1153, -0.1187, -0.1954,  0.1767, -0.1210,\n",
       "                      -0.1791,  0.1297, -0.1368,  0.1909,  0.1736,  0.1770,  0.1577, -0.1357,\n",
       "                       0.1478, -0.1842,  0.1618,  0.1341,  0.1891,  0.1225,  0.1021,  0.1262,\n",
       "                      -0.1829,  0.1169, -0.1829, -0.1134, -0.1260, -0.1172, -0.1773,  0.1732,\n",
       "                       0.1746, -0.1182,  0.1825, -0.1092, -0.1895,  0.1275,  0.1911, -0.1244,\n",
       "                      -0.1692,  0.1226,  0.1345, -0.1504, -0.1697,  0.1929, -0.1792, -0.1901,\n",
       "                      -0.1469, -0.1890, -0.1306,  0.1686,  0.1751,  0.1368, -0.0786,  0.1277,\n",
       "                       0.1225, -0.1374,  0.1763, -0.1752,  0.1230, -0.2055,  0.1212,  0.1613,\n",
       "                       0.2061,  0.1118,  0.1281, -0.0894,  0.1159, -0.1721, -0.1882, -0.1594,\n",
       "                       0.1261,  0.1768, -0.1716, -0.1832, -0.1630, -0.1754,  0.1548, -0.1686,\n",
       "                      -0.1327,  0.1956,  0.1775,  0.1192,  0.1846, -0.1229, -0.1356, -0.1826,\n",
       "                      -0.1724, -0.1143, -0.2101,  0.1881,  0.1847,  0.1190,  0.1407, -0.1804,\n",
       "                       0.1489, -0.1629, -0.1160, -0.1829,  0.1187,  0.1881, -0.1893, -0.1791,\n",
       "                       0.1874, -0.1131, -0.1145, -0.1638,  0.1444, -0.1328,  0.1929, -0.1156,\n",
       "                       0.1539, -0.1370, -0.0782,  0.1593, -0.1241,  0.1795,  0.1778,  0.1433,\n",
       "                       0.1191,  0.1600,  0.1244, -0.1826, -0.1359, -0.1125, -0.1025,  0.1812,\n",
       "                       0.0786,  0.1469,  0.1718,  0.1251,  0.1848,  0.1148, -0.1530,  0.1926,\n",
       "                       0.1484,  0.1195, -0.1322,  0.1721,  0.1203,  0.1189,  0.1064, -0.1778,\n",
       "                      -0.1904,  0.1540,  0.1268,  0.1355, -0.1192,  0.1773, -0.0813,  0.1892,\n",
       "                       0.1825, -0.1138,  0.1787,  0.1911,  0.1635, -0.1749,  0.0871,  0.1876,\n",
       "                       0.1736,  0.1896,  0.1948, -0.1246, -0.1846, -0.1713,  0.1388, -0.1831,\n",
       "                      -0.1514, -0.1729, -0.1798,  0.1172,  0.1182, -0.1713,  0.1815, -0.1205,\n",
       "                       0.1804,  0.1887, -0.1655,  0.1334,  0.1747,  0.1867,  0.1806, -0.1822,\n",
       "                       0.1879, -0.1901, -0.1829, -0.1897,  0.1314,  0.1177, -0.1312,  0.1198,\n",
       "                      -0.1871,  0.0851,  0.1406, -0.1871,  0.1588, -0.1204, -0.0638,  0.1154,\n",
       "                       0.1386, -0.0644, -0.1836,  0.1166,  0.1357, -0.1808, -0.1241, -0.1853,\n",
       "                       0.2009, -0.1755, -0.1739,  0.1877, -0.1204, -0.1177,  0.1705, -0.1163,\n",
       "                      -0.1974, -0.1674,  0.1890,  0.0935,  0.1200,  0.1767, -0.1046,  0.1376,\n",
       "                       0.1713, -0.1275,  0.1831, -0.1920,  0.1863, -0.1734,  0.1588,  0.1811,\n",
       "                       0.1055,  0.1898, -0.1891, -0.1169, -0.1846,  0.1979,  0.1730, -0.1811,\n",
       "                       0.1852, -0.1478,  0.1177, -0.1934,  0.1755, -0.1140, -0.1484, -0.1549,\n",
       "                       0.1860,  0.1691,  0.1496,  0.1872,  0.1147,  0.1018,  0.1182,  0.1181,\n",
       "                       0.1173,  0.1152,  0.1197, -0.1870,  0.1260,  0.1723, -0.1148,  0.1611,\n",
       "                       0.1331, -0.1339, -0.1262, -0.1389,  0.1603, -0.1680, -0.1135,  0.1199,\n",
       "                      -0.1266,  0.1299,  0.0271, -0.1174,  0.1188, -0.2007, -0.1834, -0.1191,\n",
       "                       0.1190,  0.1522,  0.1300, -0.1864, -0.1352, -0.1170,  0.1873,  0.1822,\n",
       "                       0.1199, -0.1182, -0.1748, -0.1885, -0.2202, -0.1666, -0.1301, -0.1376,\n",
       "                       0.1728, -0.1529,  0.1232, -0.1287,  0.1081,  0.1619, -0.1662, -0.1116,\n",
       "                       0.1212,  0.1237,  0.1692,  0.1363, -0.1844,  0.1061, -0.1878, -0.1785,\n",
       "                       0.1915,  0.1317,  0.1204, -0.1329, -0.0061,  0.1203,  0.0542,  0.1554,\n",
       "                      -0.1879, -0.1160,  0.1680, -0.1368, -0.1622,  0.0691, -0.1151, -0.1468,\n",
       "                       0.1709,  0.0425,  0.1868, -0.1297, -0.1704, -0.1798, -0.1290, -0.1807,\n",
       "                       0.1359,  0.0073, -0.0204,  0.1298, -0.1439,  0.1157, -0.1123, -0.1220,\n",
       "                       0.1841,  0.1615, -0.1214, -0.1638, -0.1861, -0.1900,  0.1716, -0.1901,\n",
       "                       0.1922, -0.1212, -0.1202,  0.1637,  0.1444, -0.1348, -0.1321,  0.1216,\n",
       "                       0.1673, -0.1815, -0.1320, -0.1244, -0.1176, -0.1125,  0.1811,  0.1118,\n",
       "                      -0.1831, -0.1318, -0.1441,  0.0569, -0.1211,  0.1122,  0.1746, -0.1488,\n",
       "                      -0.1256,  0.1978, -0.1805,  0.1294,  0.1750, -0.1445,  0.1815, -0.1860,\n",
       "                      -0.1416,  0.1660, -0.1669,  0.1154,  0.1145, -0.1380, -0.1143,  0.1237,\n",
       "                       0.1741,  0.1581,  0.1151, -0.1781,  0.1502,  0.1169,  0.2033,  0.1263,\n",
       "                      -0.1959, -0.1252,  0.1738,  0.0558, -0.1898, -0.1753,  0.1773,  0.1080,\n",
       "                       0.1514, -0.1152,  0.1540, -0.1825,  0.1165,  0.1707,  0.1818, -0.1165,\n",
       "                      -0.1123,  0.1572, -0.1778,  0.1322,  0.1154,  0.1713,  0.1841, -0.1151,\n",
       "                      -0.1302,  0.1307,  0.1687, -0.1455, -0.1157, -0.1263, -0.1885,  0.1531,\n",
       "                      -0.1830, -0.1299, -0.1163,  0.1834,  0.1326,  0.1259,  0.1315, -0.1580,\n",
       "                      -0.1227,  0.1395, -0.1739,  0.1175,  0.1592, -0.1114, -0.1170,  0.1764],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.attention.W_Q.weight',\n",
       "              tensor([[-0.1009,  0.0856,  0.1968,  ..., -0.1791, -0.1692, -0.0973],\n",
       "                      [ 0.0996, -0.1787, -0.1047,  ...,  0.2291,  0.2293,  0.1049],\n",
       "                      [-0.0747,  0.1110,  0.2221,  ..., -0.1966, -0.0906, -0.1705],\n",
       "                      ...,\n",
       "                      [-0.0249,  0.1256,  0.0518,  ..., -0.0481, -0.1313, -0.0032],\n",
       "                      [-0.0103,  0.1604,  0.2252,  ..., -0.1489, -0.0761,  0.0318],\n",
       "                      [-0.1888,  0.0525,  0.2056,  ..., -0.2514, -0.1263, -0.1119]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.attention.W_K.weight',\n",
       "              tensor([[ 0.0778,  0.1430,  0.1936,  ..., -0.1199,  0.1799,  0.1778],\n",
       "                      [ 0.2560, -0.1034,  0.1714,  ...,  0.1217,  0.1125,  0.0273],\n",
       "                      [-0.1166,  0.1624,  0.2197,  ..., -0.2913,  0.1448, -0.1090],\n",
       "                      ...,\n",
       "                      [ 0.1687,  0.1498,  0.0722,  ..., -0.1498, -0.1305,  0.1373],\n",
       "                      [-0.1367, -0.0862, -0.2420,  ..., -0.1101, -0.1621, -0.0645],\n",
       "                      [ 0.2485,  0.2616,  0.0213,  ...,  0.1833,  0.0977,  0.0840]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.attention.W_V.weight',\n",
       "              tensor([[ 0.1340, -0.2059, -0.1241,  ...,  0.0621,  0.0821,  0.1237],\n",
       "                      [ 0.0563, -0.1616, -0.0897,  ...,  0.1775,  0.1957,  0.2409],\n",
       "                      [-0.1365,  0.1761,  0.2235,  ..., -0.0957, -0.1423, -0.1533],\n",
       "                      ...,\n",
       "                      [-0.1920,  0.0909,  0.0992,  ..., -0.1169, -0.0906, -0.2016],\n",
       "                      [ 0.1338, -0.1616, -0.1714,  ...,  0.1608,  0.1474,  0.2121],\n",
       "                      [-0.1624,  0.2093,  0.1302,  ..., -0.1807, -0.1632, -0.1729]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.attention.W_T.weight',\n",
       "              tensor([[-0.0794, -0.2207, -0.1179,  ..., -0.1622,  0.0918,  0.2433],\n",
       "                      [-0.2260, -0.1315, -0.0966,  ..., -0.1386,  0.1681,  0.2032],\n",
       "                      [-0.2168, -0.2343, -0.2191,  ..., -0.1577,  0.0863,  0.1074],\n",
       "                      ...,\n",
       "                      [-0.1402, -0.1683, -0.1225,  ..., -0.1920,  0.2179,  0.1246],\n",
       "                      [ 0.0914,  0.1208,  0.0698,  ...,  0.1336, -0.1862, -0.1795],\n",
       "                      [ 0.1291,  0.0824,  0.1561,  ...,  0.1759, -0.1571, -0.0739]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.layer_norm1.gamma',\n",
       "              tensor([1.1119, 1.1881, 1.2080, 0.8437, 0.8449, 1.2630, 0.9985, 1.2234, 0.9008,\n",
       "                      0.8712, 0.8148, 1.1992, 0.9091, 1.1729, 0.8388, 1.1831, 1.0035, 0.9315,\n",
       "                      0.7340, 0.9539, 1.1850, 1.1672, 1.1140, 0.8637, 1.1992, 0.8576, 0.8903,\n",
       "                      0.8156, 0.8488, 1.1761, 0.7092, 1.2101, 1.1955, 0.9163, 0.8696, 0.8699,\n",
       "                      1.1405, 1.0940, 1.2320, 1.1196, 0.8145, 0.9355, 1.1975, 1.1439, 1.2178,\n",
       "                      1.2147, 0.9404, 1.1686, 1.1584, 1.1588, 0.7554, 1.1513, 1.2294, 0.8226,\n",
       "                      1.2124, 1.2211, 1.0427, 0.8443, 0.9142, 0.9697, 0.8503, 0.9082, 1.0023,\n",
       "                      1.1337, 1.2675, 0.7670, 0.9140, 0.8266, 0.7331, 1.1988, 0.8739, 0.8974,\n",
       "                      1.1966, 1.0834, 0.8470, 1.2184, 1.1516, 0.8242, 0.7380, 0.8042, 0.9524,\n",
       "                      1.1786, 0.7283, 0.8534, 0.9581, 1.1450, 0.9158, 0.7014, 1.0574, 1.0280,\n",
       "                      1.1707, 0.7188, 1.1307, 0.8152, 1.1767, 1.1892, 0.8134, 0.8918, 0.8544,\n",
       "                      0.8660, 1.1466, 1.1359, 1.1995, 0.9246, 1.1578, 1.1115, 1.1303, 1.1333,\n",
       "                      1.1723, 1.2129, 1.0931, 1.1705, 1.1360, 1.1394, 0.8061, 0.8216, 0.9047,\n",
       "                      1.1277, 1.1696, 0.8466, 0.7887, 0.8201, 1.1514, 1.1444, 0.8507, 1.0433,\n",
       "                      0.8763, 0.7800, 0.7484, 1.0348, 0.8937, 1.0423, 0.8618, 0.8618, 1.1474,\n",
       "                      1.1933, 1.0926, 0.8276, 1.0492, 0.8641, 0.8819, 1.2521, 1.1216, 1.0660,\n",
       "                      0.8748, 0.9066, 1.1575, 1.1046, 1.2120, 1.1574, 0.9995, 1.2186, 0.8854,\n",
       "                      0.8752, 1.1730, 0.9787, 1.1935, 1.1788, 1.0601, 1.1888, 0.7865, 0.7310,\n",
       "                      0.8961, 1.1538, 0.8230, 1.1779, 1.1758, 1.0870, 1.1897, 0.8429, 1.1650,\n",
       "                      0.8464, 1.1527, 1.0078, 1.2126, 1.0045, 1.1688, 1.0497, 1.2201, 0.9789,\n",
       "                      0.8662, 0.8247, 0.8614, 0.8895, 1.1741, 1.1522, 0.9300, 0.9057, 1.1405,\n",
       "                      0.8673, 1.1577, 1.1905, 1.2174, 0.9294, 1.1817, 0.8440, 0.8720, 1.2240,\n",
       "                      1.1557, 1.1445, 0.8701, 1.1307, 0.9501, 1.1118, 1.0627, 0.7374, 1.1255,\n",
       "                      1.1180, 1.1886, 1.2118, 0.7483, 0.9809, 0.8302, 0.9909, 0.7857, 1.1964,\n",
       "                      1.2024, 0.8422, 1.2283, 1.0431, 1.1967, 0.9366, 1.1799, 1.1984, 1.1985,\n",
       "                      1.2002, 1.1827, 0.8943, 1.1583, 1.1678, 0.8832, 1.0960, 1.0391, 0.8292,\n",
       "                      0.7226, 0.8102, 0.8362, 0.8694, 0.9847, 1.0934, 0.9793, 1.1506, 1.1211,\n",
       "                      0.7212, 0.6964, 1.2097, 1.1742, 1.0107, 1.1349, 0.8529, 0.9392, 1.1949,\n",
       "                      1.1749, 0.9505, 1.2099, 0.8915, 1.2162, 0.8015, 1.1222, 1.1692, 1.1082,\n",
       "                      1.0689, 0.8762, 0.8743, 1.1226, 1.2057, 0.8736, 0.8753, 1.0173, 1.2105,\n",
       "                      0.9228, 1.2189, 1.1547, 1.2115, 0.9439, 1.1046, 0.9046, 1.1164, 1.1023,\n",
       "                      0.8537, 0.9698, 0.8208, 1.1133, 0.7123, 0.8843, 1.1667, 0.8784, 0.8679,\n",
       "                      0.8302, 0.8110, 1.0845, 1.2021, 1.0562, 1.1676, 1.2015, 0.9097, 0.7900,\n",
       "                      1.1962, 0.8502, 1.1480, 1.1031, 1.1018, 0.8197, 1.1498, 1.1402, 0.9052,\n",
       "                      1.1908, 0.8740, 1.1487, 0.8717, 1.2713, 1.1369, 0.7841, 0.8944, 1.1352,\n",
       "                      1.1616, 0.9073, 1.0954, 0.8980, 1.0787, 0.8867, 0.9823, 1.0308, 1.1534,\n",
       "                      0.8586, 1.1585, 0.8399, 0.8625, 0.9726, 1.1842, 1.0904, 0.8877, 0.8095,\n",
       "                      0.8460, 0.8387, 0.9102, 1.1219, 0.8527, 0.8539, 0.8380, 1.0385, 0.9792,\n",
       "                      0.7359, 0.8478, 1.1163, 1.1125, 1.1064, 1.1429, 1.1490, 0.8794, 1.1673,\n",
       "                      0.8703, 1.1681, 0.8999, 1.0281, 1.1167, 1.1684, 1.2127, 0.7985, 1.1916,\n",
       "                      1.1440, 0.7740, 0.8245, 0.9269, 0.7078, 1.2085, 1.0928, 1.1413, 0.8625,\n",
       "                      0.9609, 0.8389, 0.8921, 1.1900, 0.8169, 1.0726, 0.8858, 1.2056, 0.8926,\n",
       "                      0.8772, 0.9456, 1.0732, 0.8074, 0.8834, 0.7148, 0.8474, 0.8883, 1.1032,\n",
       "                      0.9197, 0.8628, 1.0732, 0.8948, 0.8217, 0.7967, 0.7477, 1.2197, 0.8621,\n",
       "                      1.1610, 1.1916, 1.1585, 1.1220, 0.9154, 0.9027, 1.1121, 0.8938, 1.1782,\n",
       "                      0.8356, 0.8126, 1.0897, 1.0940, 0.7029, 1.0982, 1.1428, 0.9607, 1.1153,\n",
       "                      1.1528, 1.1867, 0.8181, 0.9297, 0.6926, 1.1516, 0.7502, 0.8448, 1.1279,\n",
       "                      0.9039, 1.1326, 0.9191, 0.8329, 0.8817, 0.8632, 0.8092, 0.8521, 0.8704,\n",
       "                      1.1376, 1.0786, 0.8190, 1.1676, 0.8736, 0.6698, 0.7980, 1.2171, 1.2231,\n",
       "                      1.1218, 1.1546, 1.0818, 1.1531, 1.0572, 1.1312, 0.8954, 0.8675, 1.0967,\n",
       "                      0.8248, 0.8827, 0.9863, 1.1465, 0.8083, 1.1581, 1.2090, 1.1032, 0.8531,\n",
       "                      1.0999, 1.1119, 0.8794, 0.8590, 0.8382, 0.9700, 0.9439, 1.1648, 0.9334,\n",
       "                      1.1721, 0.9326, 1.1428, 0.8738, 1.1240, 0.8532, 0.8409, 1.1108, 0.8298,\n",
       "                      1.1219, 1.1683, 1.1040, 0.8640, 1.1937, 1.1319, 0.8734, 0.8202, 1.1762,\n",
       "                      1.0744, 1.2272, 1.1750, 0.9175, 0.8956, 0.9552, 0.8385, 0.8235, 0.6864,\n",
       "                      0.9668, 1.1472, 1.2007, 0.8844, 0.8823, 0.8818, 1.0276, 0.8876, 1.1158,\n",
       "                      0.7638, 1.1202, 1.2170, 0.8933, 0.7829, 0.8413, 1.1426, 1.1111],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.layer_norm1.beta',\n",
       "              tensor([-0.1409, -0.2089, -0.2446, -0.1183,  0.1963, -0.2042,  0.1727, -0.2107,\n",
       "                      -0.0991,  0.1599, -0.1437, -0.2035,  0.1450,  0.1611, -0.1111,  0.1300,\n",
       "                      -0.1803,  0.0186,  0.0103,  0.2159,  0.1467,  0.1834,  0.1556,  0.1146,\n",
       "                      -0.1392,  0.1396,  0.1907,  0.1604, -0.0786, -0.1244,  0.0879,  0.2326,\n",
       "                       0.2246, -0.1752,  0.1736,  0.1368,  0.1133, -0.1185, -0.2194, -0.1150,\n",
       "                       0.1577, -0.2102,  0.2148, -0.1348, -0.1811,  0.1859, -0.0816,  0.1429,\n",
       "                      -0.1210,  0.1349, -0.0108, -0.1312, -0.1870, -0.1751,  0.2188,  0.1796,\n",
       "                      -0.1045,  0.1813,  0.1799,  0.2060, -0.1629, -0.1731,  0.1966,  0.1368,\n",
       "                      -0.2662, -0.1018,  0.2009, -0.1388,  0.0240, -0.2117,  0.1925, -0.2096,\n",
       "                       0.2399,  0.2135, -0.1187,  0.2542,  0.0965, -0.1465,  0.0591, -0.1455,\n",
       "                       0.1624, -0.1342,  0.0394,  0.1841,  0.2106,  0.1329, -0.0169,  0.0188,\n",
       "                      -0.0523,  0.1002, -0.1709, -0.0546, -0.1263, -0.1780, -0.1388,  0.2400,\n",
       "                       0.1406,  0.1326,  0.1863, -0.1597, -0.1805,  0.1443,  0.2267, -0.1506,\n",
       "                      -0.1493,  0.1017,  0.1855, -0.1321, -0.1632,  0.2022, -0.1367, -0.1296,\n",
       "                       0.0339, -0.1029, -0.1451,  0.0151,  0.1973,  0.1185, -0.1403,  0.1343,\n",
       "                       0.0539, -0.1449,  0.1324, -0.1582,  0.1885, -0.1173,  0.1798,  0.0284,\n",
       "                       0.0063, -0.2077,  0.1890, -0.0943,  0.1794, -0.1590,  0.1205, -0.2172,\n",
       "                       0.1209,  0.1057,  0.1859,  0.1821, -0.1709, -0.2502,  0.1638, -0.1089,\n",
       "                      -0.1725, -0.1712,  0.1280, -0.0854,  0.2424, -0.1342, -0.1595, -0.1995,\n",
       "                      -0.0219, -0.1806, -0.1964,  0.2322,  0.1708,  0.1372,  0.0972, -0.1380,\n",
       "                      -0.1136,  0.0095, -0.1940, -0.1389,  0.1398,  0.1500, -0.1364, -0.1159,\n",
       "                       0.1937, -0.1640, -0.1231,  0.1559, -0.1812, -0.1304,  0.2248, -0.0905,\n",
       "                       0.1253, -0.1753,  0.1593,  0.0421, -0.1584,  0.1326,  0.1644,  0.1693,\n",
       "                      -0.2235, -0.1580,  0.0316,  0.1400,  0.1344, -0.0912,  0.1802,  0.2152,\n",
       "                      -0.2129, -0.1866,  0.1681,  0.1546,  0.1932, -0.2226, -0.1545,  0.1156,\n",
       "                       0.1898,  0.1489,  0.1827,  0.1221,  0.1032,  0.0251, -0.1078, -0.1188,\n",
       "                      -0.2348,  0.2126, -0.0082, -0.0637, -0.1369, -0.1719,  0.1065,  0.2130,\n",
       "                       0.2199, -0.1842,  0.2216,  0.2483,  0.1877, -0.1934, -0.1841,  0.1787,\n",
       "                       0.2225,  0.2308,  0.1583, -0.2037, -0.2334,  0.2176,  0.1526, -0.0170,\n",
       "                       0.0894, -0.0028,  0.0691,  0.1377,  0.1867, -0.2004,  0.2138, -0.1307,\n",
       "                      -0.2155,  0.1904, -0.1357,  0.0192,  0.0340,  0.1808,  0.1999, -0.1996,\n",
       "                       0.1503,  0.1365,  0.1350, -0.1311,  0.1482,  0.1513,  0.2174,  0.1982,\n",
       "                      -0.2051, -0.1572,  0.1227, -0.2692, -0.0818, -0.1181,  0.0910,  0.1750,\n",
       "                       0.1407, -0.2045, -0.2001,  0.1728,  0.0787, -0.1577,  0.1702, -0.1886,\n",
       "                       0.2012, -0.2279, -0.1620,  0.1332, -0.2143, -0.1091, -0.0289, -0.1705,\n",
       "                      -0.2141, -0.0312,  0.1209, -0.0230,  0.2005,  0.1091, -0.1686,  0.1529,\n",
       "                       0.1605, -0.1370,  0.1767, -0.2214,  0.1161, -0.1501, -0.1962,  0.1666,\n",
       "                       0.0777,  0.1480,  0.1498, -0.1273, -0.0680,  0.1936,  0.1278, -0.1387,\n",
       "                       0.0355, -0.0908, -0.1771, -0.1850,  0.1852, -0.1802, -0.2320,  0.1460,\n",
       "                       0.0876,  0.1636,  0.2132,  0.1478,  0.2025, -0.1332,  0.1786,  0.0861,\n",
       "                       0.1967,  0.0882, -0.1332, -0.1157,  0.1521, -0.1489, -0.1845, -0.1332,\n",
       "                       0.1945, -0.1869, -0.1088,  0.1017, -0.0811, -0.1809, -0.1886,  0.1966,\n",
       "                      -0.1579,  0.1816,  0.1803, -0.1566, -0.0924, -0.2017,  0.0185, -0.1767,\n",
       "                       0.0890, -0.1071,  0.1212, -0.1896, -0.1244, -0.1665,  0.1570,  0.1914,\n",
       "                      -0.1815, -0.0194, -0.1501, -0.1209, -0.1473, -0.1841, -0.1350, -0.2219,\n",
       "                       0.1406, -0.1156,  0.1765, -0.1420,  0.0120,  0.1910, -0.1535,  0.2072,\n",
       "                       0.1962, -0.0083,  0.1434, -0.1737, -0.1712,  0.1937, -0.1225,  0.1355,\n",
       "                       0.1550,  0.2005,  0.1912,  0.1555,  0.0421,  0.1407,  0.1881,  0.0423,\n",
       "                       0.1533, -0.1770,  0.1026,  0.1836, -0.1666,  0.1079, -0.2116, -0.1557,\n",
       "                       0.0373, -0.0651,  0.1777, -0.1688, -0.1656, -0.1814, -0.1338, -0.1248,\n",
       "                       0.0412,  0.2065, -0.0841, -0.1557, -0.1668,  0.1779, -0.1804,  0.1127,\n",
       "                       0.1509,  0.0082, -0.2219,  0.0736,  0.0296,  0.0389,  0.1149, -0.1800,\n",
       "                       0.1955, -0.1993,  0.0547,  0.0212, -0.0131, -0.1443,  0.1192,  0.1862,\n",
       "                       0.1299, -0.1785, -0.1595,  0.1816, -0.1972, -0.1485,  0.1468,  0.1861,\n",
       "                      -0.1290,  0.2448, -0.0676,  0.1421, -0.2037, -0.0018,  0.1417, -0.1431,\n",
       "                       0.2118,  0.1839, -0.1413,  0.1208,  0.1325,  0.1209,  0.1350,  0.1795,\n",
       "                       0.1315,  0.1772, -0.1608,  0.2120,  0.2006,  0.1404, -0.1571,  0.1573,\n",
       "                       0.2063, -0.1519,  0.1425, -0.1363,  0.1151,  0.1728,  0.1386,  0.1896,\n",
       "                      -0.0993, -0.1479,  0.1352,  0.0022, -0.1385, -0.1795,  0.1312,  0.1583,\n",
       "                      -0.1600, -0.1622, -0.1071, -0.1077,  0.1533,  0.1504,  0.2263, -0.0933,\n",
       "                      -0.1769, -0.1720, -0.1341,  0.1945,  0.1764, -0.1975,  0.2076,  0.2385,\n",
       "                       0.1473,  0.2192, -0.1567, -0.1958, -0.1857, -0.0846, -0.0246, -0.1707,\n",
       "                      -0.1502, -0.2068, -0.1260, -0.1475,  0.0476,  0.1147,  0.1426, -0.1445,\n",
       "                      -0.0704, -0.1304, -0.1705,  0.1863,  0.1656, -0.1533,  0.1861,  0.1427],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.ffn.layer1.weight',\n",
       "              tensor([[ 0.1758,  0.1120,  0.1710,  ..., -0.1062, -0.1071, -0.1452],\n",
       "                      [ 0.1934,  0.1839,  0.1285,  ..., -0.1448, -0.1898, -0.1971],\n",
       "                      [ 0.1376,  0.1935,  0.1182,  ...,  0.1790, -0.1997, -0.1644],\n",
       "                      ...,\n",
       "                      [-0.2252, -0.2113, -0.2207,  ..., -0.1669,  0.1901,  0.2067],\n",
       "                      [-0.1168, -0.1748, -0.1881,  ..., -0.1685,  0.0887,  0.1712],\n",
       "                      [-0.0050,  0.0131,  0.0452,  ...,  0.0400, -0.0403, -0.0053]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.ffn.layer1.bias',\n",
       "              tensor([-0.1860, -0.1832, -0.1361,  ...,  0.1525,  0.1425,  0.0269],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.ffn.layer2.weight',\n",
       "              tensor([[ 0.1719,  0.1629,  0.1223,  ..., -0.1076, -0.1345,  0.0379],\n",
       "                      [-0.1230, -0.1050, -0.1311,  ..., -0.2012, -0.2514,  0.0253],\n",
       "                      [-0.1922, -0.1369, -0.1725,  ..., -0.1592, -0.1161, -0.0429],\n",
       "                      ...,\n",
       "                      [-0.1520, -0.1669, -0.1120,  ..., -0.1780, -0.0633, -0.0059],\n",
       "                      [ 0.1586, -0.1624, -0.1484,  ...,  0.1840,  0.2240,  0.0352],\n",
       "                      [ 0.1682,  0.1612,  0.1365,  ...,  0.1389,  0.0760, -0.0464]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.ffn.layer2.bias',\n",
       "              tensor([ 0.0770, -0.1779, -0.1826, -0.1249,  0.1665, -0.1687,  0.1537, -0.1669,\n",
       "                      -0.1756,  0.1257, -0.1249,  0.1443,  0.1550,  0.1508,  0.1336,  0.1642,\n",
       "                      -0.1647, -0.1861, -0.0237,  0.1519,  0.1826,  0.1679,  0.1923,  0.1515,\n",
       "                      -0.1585, -0.1842,  0.1803,  0.1592, -0.1088, -0.1637,  0.1623, -0.1470,\n",
       "                       0.2220, -0.1294,  0.1811,  0.1365,  0.1739, -0.1691, -0.1676, -0.1642,\n",
       "                       0.1132, -0.1648,  0.1650, -0.1423, -0.1531,  0.1448,  0.0723,  0.1319,\n",
       "                      -0.1405,  0.1938, -0.1240,  0.0931, -0.1450, -0.1253,  0.1749,  0.1669,\n",
       "                      -0.1382,  0.1555,  0.1488,  0.1342, -0.1627, -0.1690,  0.1657,  0.1808,\n",
       "                      -0.1863, -0.1295,  0.1664, -0.1355, -0.1062, -0.1704,  0.1630, -0.1476,\n",
       "                       0.1960, -0.1811, -0.1358,  0.1333,  0.1696, -0.1681, -0.0734, -0.1258,\n",
       "                       0.1653, -0.1430,  0.1779,  0.1656,  0.1326,  0.1622, -0.2005,  0.1474,\n",
       "                      -0.1996,  0.1588,  0.1447, -0.1603, -0.1542, -0.1388, -0.1554,  0.1481,\n",
       "                       0.1195,  0.1545,  0.1336, -0.1457, -0.2168,  0.1988,  0.1884, -0.1246,\n",
       "                      -0.1711,  0.1512,  0.1521, -0.1643, -0.1564,  0.1570, -0.1469, -0.1531,\n",
       "                       0.1245, -0.1575, -0.1245,  0.1786,  0.1328,  0.1263, -0.1721,  0.1522,\n",
       "                       0.1386, -0.1321,  0.1412, -0.1976,  0.1361, -0.1739,  0.1683, -0.1009,\n",
       "                       0.1738, -0.1920,  0.1412, -0.1640,  0.1230, -0.1316,  0.1486, -0.1508,\n",
       "                       0.1468, -0.1570,  0.1639,  0.1219, -0.1654, -0.1496,  0.1457,  0.1654,\n",
       "                      -0.1442, -0.1276,  0.1484, -0.1466,  0.1585, -0.1727, -0.1261, -0.1583,\n",
       "                      -0.1867, -0.1398, -0.1658,  0.1880,  0.1668,  0.1631,  0.1648, -0.1493,\n",
       "                      -0.1298,  0.0802, -0.1608, -0.1821,  0.1460,  0.1723, -0.1420, -0.1615,\n",
       "                       0.1388, -0.1703, -0.1761, -0.1808, -0.1748, -0.1523,  0.1533,  0.1985,\n",
       "                       0.1510, -0.1711,  0.1996,  0.1582, -0.1451,  0.1786,  0.1403,  0.1258,\n",
       "                      -0.1805, -0.1513,  0.1420,  0.1940,  0.1624, -0.1334,  0.1572, -0.1170,\n",
       "                       0.1222, -0.1559,  0.1791,  0.1627,  0.1699, -0.1416, -0.1679,  0.1426,\n",
       "                       0.1586,  0.2025,  0.1613,  0.1334, -0.1505,  0.1460, -0.1277,  0.1658,\n",
       "                      -0.1554,  0.1526,  0.1480, -0.0682, -0.1426, -0.1464, -0.1477,  0.1899,\n",
       "                       0.1874, -0.1346,  0.1778, -0.1503,  0.1542, -0.1726, -0.1574,  0.1479,\n",
       "                       0.1423,  0.1693,  0.1505, -0.1445, -0.1554,  0.2032,  0.1266, -0.2012,\n",
       "                      -0.1446, -0.1321,  0.1339, -0.1849,  0.1558, -0.1468,  0.1625, -0.1276,\n",
       "                       0.1491, -0.1533, -0.1461, -0.0829, -0.1420,  0.1440,  0.1713, -0.1424,\n",
       "                       0.1617, -0.1751,  0.1419, -0.1435,  0.1378,  0.1308,  0.1743,  0.1559,\n",
       "                      -0.1500, -0.1556,  0.1367, -0.0530, -0.1466, -0.1379, -0.1256,  0.1645,\n",
       "                       0.1491, -0.1663, -0.1490,  0.1425,  0.1415, -0.1838,  0.1257, -0.1374,\n",
       "                      -0.1262, -0.2017,  0.2084,  0.1285, -0.1695, -0.1835,  0.2118, -0.1529,\n",
       "                      -0.1290, -0.1705,  0.1663,  0.1099,  0.1443, -0.1411, -0.1490,  0.1478,\n",
       "                      -0.0803, -0.1160,  0.1785, -0.1466,  0.1266, -0.1446, -0.0798,  0.1141,\n",
       "                       0.1526,  0.1446,  0.1333, -0.1504, -0.1683, -0.0106,  0.1481,  0.0978,\n",
       "                       0.1811, -0.1735, -0.1551, -0.0861, -0.1384, -0.1654,  0.0864,  0.1589,\n",
       "                       0.1453,  0.1638,  0.1730,  0.1764,  0.1532,  0.1559,  0.1327, -0.0932,\n",
       "                       0.1335,  0.1969, -0.1486, -0.1711, -0.1834, -0.1513, -0.1510, -0.1517,\n",
       "                       0.1306, -0.1370, -0.1554, -0.0456,  0.1817, -0.1665, -0.1337,  0.1341,\n",
       "                      -0.1195,  0.1302,  0.1243, -0.1195, -0.1321, -0.1657,  0.1178, -0.1593,\n",
       "                       0.1314, -0.1763,  0.1462, -0.1588, -0.1644, -0.1463,  0.1829,  0.1577,\n",
       "                      -0.1690, -0.0275,  0.0351, -0.1760, -0.1741, -0.1555, -0.1553, -0.1680,\n",
       "                       0.1398, -0.1452,  0.1125, -0.1366,  0.1219,  0.1700,  0.1891,  0.1733,\n",
       "                       0.1424,  0.1335,  0.1414, -0.1408, -0.1504,  0.1640, -0.1320,  0.1356,\n",
       "                       0.2073,  0.1252,  0.1103,  0.1553, -0.0500,  0.0945,  0.1255,  0.1369,\n",
       "                       0.1408, -0.1526,  0.1567,  0.1514, -0.1200,  0.1307, -0.1520, -0.1578,\n",
       "                       0.0892,  0.0223,  0.1756, -0.1331,  0.1316, -0.1381, -0.1501, -0.1469,\n",
       "                      -0.1906,  0.1608,  0.1432, -0.1405, -0.1343,  0.1561, -0.1639, -0.1437,\n",
       "                      -0.1375, -0.1445, -0.1649,  0.1701,  0.1576, -0.1467,  0.1774, -0.1656,\n",
       "                       0.1387, -0.1525, -0.1911,  0.1334,  0.1987, -0.1488,  0.1547,  0.1314,\n",
       "                       0.1683, -0.1254, -0.1323,  0.1293, -0.1466, -0.1555,  0.1612,  0.1470,\n",
       "                      -0.1732,  0.2020, -0.1524,  0.1437, -0.1354,  0.1245, -0.2147, -0.1540,\n",
       "                       0.1466,  0.1927, -0.1578,  0.1424,  0.1559,  0.1743,  0.1360,  0.1685,\n",
       "                      -0.1468,  0.1468, -0.1396,  0.1428,  0.1535,  0.1343, -0.1536,  0.1555,\n",
       "                       0.1792, -0.0795, -0.1622, -0.1703,  0.1770,  0.1604, -0.1763,  0.1552,\n",
       "                      -0.1843, -0.1527,  0.1439,  0.1766, -0.1439, -0.1306,  0.1583,  0.1465,\n",
       "                      -0.1484, -0.1267, -0.0828, -0.1653,  0.1310,  0.1541,  0.1726, -0.1664,\n",
       "                      -0.1433, -0.1602, -0.1509,  0.1335,  0.1270, -0.1705,  0.2308,  0.1856,\n",
       "                       0.1524,  0.1480, -0.1653, -0.1768, -0.1403, -0.1672,  0.1152, -0.1453,\n",
       "                      -0.1636,  0.0292,  0.0879, -0.1353, -0.1370,  0.1495,  0.1456, -0.1577,\n",
       "                      -0.1542, -0.1497, -0.1644,  0.1481,  0.1674, -0.1553,  0.0749,  0.1335],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.layer_norm2.gamma',\n",
       "              tensor([0.8175, 1.3178, 1.1995, 0.7379, 0.7934, 1.2794, 1.1440, 1.1200, 1.1469,\n",
       "                      0.6720, 0.6370, 0.6523, 1.0460, 1.3047, 0.6865, 1.0980, 0.7620, 1.1979,\n",
       "                      0.7918, 0.7815, 1.0999, 1.2785, 1.0573, 0.7114, 1.2199, 1.0853, 1.1792,\n",
       "                      0.7057, 0.9028, 1.2679, 0.7425, 0.6347, 1.2481, 0.7021, 1.2122, 0.7524,\n",
       "                      1.0964, 1.3122, 1.2930, 1.1348, 0.8915, 0.7248, 1.3369, 1.1991, 1.2263,\n",
       "                      1.2012, 1.0146, 0.7241, 0.7213, 1.1980, 0.6395, 0.7410, 1.2738, 0.7009,\n",
       "                      1.1948, 1.1869, 1.2588, 0.7639, 1.2430, 0.6554, 0.7532, 1.3130, 0.8003,\n",
       "                      1.2745, 1.2349, 0.7283, 0.8640, 0.7252, 0.7542, 1.1837, 0.7657, 0.7836,\n",
       "                      1.2630, 1.1603, 0.7305, 1.1792, 1.3372, 0.7023, 0.9020, 0.6101, 0.7596,\n",
       "                      1.1947, 1.3136, 1.1781, 0.6986, 1.2231, 1.2939, 0.6839, 1.2527, 0.8018,\n",
       "                      0.6022, 0.7795, 1.1263, 0.6717, 1.1605, 1.1792, 0.7409, 0.6856, 0.7244,\n",
       "                      0.5677, 1.2960, 1.1469, 1.3292, 0.7264, 1.2214, 1.0995, 1.3296, 1.2052,\n",
       "                      1.2062, 1.1857, 0.8483, 1.1400, 0.8187, 0.9724, 0.7993, 0.9369, 0.9533,\n",
       "                      1.0978, 1.2708, 0.7699, 0.7374, 0.6870, 1.3168, 1.2266, 0.8392, 1.2479,\n",
       "                      0.7581, 0.8450, 1.1504, 1.1626, 1.1584, 1.0388, 0.7394, 0.6960, 1.3423,\n",
       "                      0.6906, 1.2578, 1.1882, 1.2125, 0.7450, 0.7968, 1.0091, 0.9080, 0.8870,\n",
       "                      0.6947, 0.7447, 1.2304, 1.2527, 1.3537, 1.2576, 0.7396, 1.1487, 1.1138,\n",
       "                      0.7410, 1.1764, 1.1118, 1.2465, 1.2356, 1.1823, 1.2415, 0.7700, 0.7686,\n",
       "                      0.7530, 1.2755, 0.5343, 1.3400, 1.1560, 1.1918, 1.2517, 0.8952, 1.2342,\n",
       "                      1.2821, 1.2773, 0.7434, 1.2617, 1.3301, 0.7362, 1.2603, 1.2897, 1.1093,\n",
       "                      0.7401, 1.1327, 0.7296, 0.6662, 1.2463, 1.3339, 0.6576, 1.3047, 1.1098,\n",
       "                      0.9105, 0.7547, 0.7926, 0.6168, 1.1612, 1.2109, 0.7564, 0.7907, 1.2941,\n",
       "                      1.3793, 1.1905, 0.7108, 1.1126, 1.0401, 0.7550, 0.7316, 0.6623, 1.2141,\n",
       "                      0.8306, 1.2317, 1.1099, 1.1744, 0.8103, 0.7790, 0.8829, 1.0464, 1.3245,\n",
       "                      1.1708, 0.7644, 1.1972, 0.7539, 1.1266, 1.1727, 1.1331, 1.1721, 1.1708,\n",
       "                      1.1906, 1.1320, 0.6947, 1.2463, 1.2339, 0.7238, 1.3825, 1.2305, 0.7412,\n",
       "                      0.8976, 1.2665, 0.7412, 0.7489, 1.3137, 0.6886, 0.6294, 0.6530, 1.2837,\n",
       "                      0.8683, 0.7130, 1.1601, 1.2215, 0.6637, 1.3268, 1.1924, 1.2192, 1.2551,\n",
       "                      1.2499, 0.9405, 1.2250, 1.0914, 1.1598, 0.7350, 0.7602, 1.0516, 1.3325,\n",
       "                      0.6792, 0.7328, 0.7402, 1.1344, 1.2972, 0.9099, 0.5753, 0.6995, 1.1862,\n",
       "                      0.7963, 1.2629, 0.6592, 1.3103, 1.0720, 0.7430, 1.3607, 1.3548, 1.1900,\n",
       "                      0.7569, 0.6579, 1.0696, 1.2658, 0.7570, 1.1313, 0.9530, 0.6623, 0.7647,\n",
       "                      0.9609, 0.6808, 1.2332, 1.1596, 0.7448, 0.9962, 1.0642, 0.7091, 0.7655,\n",
       "                      1.0399, 0.7019, 0.9488, 1.3256, 0.7845, 0.7619, 0.7562, 1.2477, 1.0819,\n",
       "                      1.0790, 0.8477, 0.8088, 0.7451, 0.8325, 1.1585, 0.6560, 0.7189, 0.8520,\n",
       "                      1.1999, 0.7640, 1.3398, 0.7845, 0.8653, 0.7257, 1.2073, 1.1047, 1.3055,\n",
       "                      1.0234, 1.1431, 1.2262, 0.7866, 0.7703, 0.8356, 0.6681, 1.0659, 1.2716,\n",
       "                      1.2672, 0.8300, 0.7031, 0.6837, 0.7938, 0.7139, 0.6625, 0.7547, 1.1201,\n",
       "                      0.7152, 0.7376, 0.9024, 1.3702, 1.1652, 1.3303, 1.2728, 0.7638, 1.2420,\n",
       "                      0.8521, 1.2208, 0.9769, 0.9241, 1.2527, 1.1982, 1.1915, 0.7403, 1.1815,\n",
       "                      1.1917, 0.6892, 0.8417, 0.7958, 0.7014, 1.3793, 1.1986, 1.1318, 0.7822,\n",
       "                      0.6470, 0.6865, 0.7184, 1.3275, 0.7317, 0.7083, 0.6933, 1.2581, 0.7406,\n",
       "                      0.7713, 1.1708, 1.1027, 0.9009, 0.7615, 0.8173, 0.6777, 0.7676, 0.8384,\n",
       "                      1.0442, 0.7424, 0.9856, 0.7175, 0.7086, 0.8204, 0.9622, 1.2622, 0.6713,\n",
       "                      0.7376, 1.2726, 1.1637, 1.2736, 1.2684, 0.7516, 0.7476, 0.7396, 1.1731,\n",
       "                      0.6835, 0.8131, 1.1934, 0.6653, 0.5667, 1.0357, 1.3321, 0.9854, 0.8815,\n",
       "                      1.3050, 0.7701, 0.6968, 0.7522, 1.3872, 1.2641, 1.0439, 0.7462, 1.2451,\n",
       "                      0.7837, 1.3694, 0.7229, 0.7712, 0.7022, 0.8158, 0.6046, 0.7696, 0.8356,\n",
       "                      1.2142, 1.2413, 1.1150, 1.2359, 0.7356, 0.7569, 1.1994, 1.4174, 1.1936,\n",
       "                      1.1275, 1.1984, 0.7441, 1.1593, 1.1990, 1.2294, 1.1975, 1.2931, 0.7438,\n",
       "                      0.7330, 0.6772, 0.6817, 0.7115, 0.5768, 0.9820, 1.2534, 0.8866, 1.1987,\n",
       "                      0.9597, 1.3163, 0.7855, 1.1607, 0.9435, 1.1655, 0.7412, 1.2064, 1.2336,\n",
       "                      1.0992, 0.8663, 1.2330, 0.7589, 0.8836, 0.7829, 0.7954, 0.7198, 0.7247,\n",
       "                      0.7837, 1.1792, 0.8326, 0.7524, 0.8851, 0.8004, 0.7265, 0.7664, 1.0480,\n",
       "                      1.2507, 1.2355, 1.2348, 0.8308, 0.6346, 1.2085, 0.6764, 1.1818, 0.5535,\n",
       "                      0.6489, 1.2301, 0.9339, 1.0029, 0.7874, 0.8830, 0.7667, 0.7099, 0.8061,\n",
       "                      0.6958, 0.9064, 1.2992, 0.7225, 0.7542, 0.7667, 1.0612, 0.9360],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.layers.5.layer_norm2.beta',\n",
       "              tensor([-0.0781, -0.3300, -0.3178,  0.0657, -0.0325, -0.2733,  0.2297, -0.3315,\n",
       "                      -0.1416, -0.0690,  0.0486, -0.1173,  0.0833,  0.2727, -0.0711,  0.3784,\n",
       "                       0.0546, -0.1805,  0.0704,  0.0049,  0.1970,  0.2758,  0.0924, -0.1000,\n",
       "                      -0.2897, -0.2288,  0.1876, -0.0933,  0.0680, -0.3289, -0.1309,  0.0713,\n",
       "                       0.2169,  0.0973,  0.1297, -0.0828,  0.0893, -0.1606, -0.3663, -0.2279,\n",
       "                      -0.0645,  0.0062,  0.3229, -0.3064, -0.2859,  0.3223,  0.0657, -0.0914,\n",
       "                      -0.0363,  0.1386,  0.1102, -0.0857, -0.2291,  0.1319,  0.2244,  0.2242,\n",
       "                      -0.2570, -0.0348,  0.2302, -0.0898,  0.1003, -0.2798,  0.0264,  0.2607,\n",
       "                      -0.1777,  0.0389,  0.0955,  0.0397,  0.1021, -0.1235, -0.0251, -0.0447,\n",
       "                       0.2968, -0.2575,  0.0728,  0.3616,  0.2239,  0.0152, -0.1487,  0.0975,\n",
       "                      -0.0841, -0.1909,  0.2488,  0.1749, -0.0754,  0.2534, -0.3047, -0.1261,\n",
       "                      -0.2949, -0.1532, -0.0981,  0.0910, -0.2234,  0.0815, -0.2735,  0.2622,\n",
       "                      -0.0537, -0.1100, -0.0433,  0.1143, -0.2147,  0.1881,  0.3908,  0.0579,\n",
       "                      -0.1716,  0.0387,  0.3079, -0.1945, -0.3004,  0.3082, -0.0446, -0.2816,\n",
       "                      -0.0713,  0.1007,  0.1646,  0.1410,  0.0482,  0.1379, -0.3304, -0.0529,\n",
       "                      -0.1511,  0.0978,  0.2558, -0.2963, -0.0615, -0.3223, -0.0341,  0.0876,\n",
       "                       0.1678, -0.2698,  0.3244, -0.1594, -0.1028,  0.0674,  0.3190,  0.0653,\n",
       "                       0.1697, -0.2921,  0.2154, -0.1551,  0.0087, -0.0586, -0.0882,  0.0193,\n",
       "                       0.0654,  0.1083,  0.2475, -0.2192,  0.3197, -0.2717,  0.1021, -0.2917,\n",
       "                      -0.1666,  0.1425, -0.3596,  0.2886,  0.3046,  0.3248,  0.2048, -0.2050,\n",
       "                       0.0414, -0.0770, -0.0270, -0.2091, -0.1073,  0.3219, -0.2093, -0.2309,\n",
       "                       0.3178, -0.0455, -0.2204, -0.2259, -0.1529,  0.0646,  0.2401,  0.2162,\n",
       "                      -0.1004, -0.2752,  0.2309,  0.2074,  0.0426,  0.1911, -0.1154, -0.0842,\n",
       "                      -0.2413, -0.3066, -0.1024,  0.2356,  0.1776, -0.0819, -0.0848,  0.0833,\n",
       "                      -0.1092, -0.1096,  0.2808, -0.0642, -0.0326, -0.3231, -0.3103,  0.3428,\n",
       "                      -0.1036,  0.0643,  0.0949, -0.0750,  0.0691, -0.1086, -0.2925, -0.0095,\n",
       "                      -0.3283,  0.2909,  0.2161, -0.0403,  0.0396, -0.0660, -0.1593,  0.2744,\n",
       "                       0.3070,  0.0416,  0.1235,  0.0708,  0.3159, -0.2521, -0.2462,  0.3102,\n",
       "                       0.2478,  0.2550,  0.2294,  0.0581, -0.2623,  0.2193, -0.0106, -0.3287,\n",
       "                      -0.3279,  0.1014,  0.0520, -0.3047, -0.0604,  0.0155,  0.3454,  0.1092,\n",
       "                      -0.0764,  0.0716, -0.2579,  0.0181,  0.1456,  0.3063,  0.3016,  0.0368,\n",
       "                       0.3063, -0.1407,  0.3433, -0.2563,  0.2115, -0.1291,  0.2732,  0.1171,\n",
       "                      -0.2612,  0.0652, -0.0449, -0.1957, -0.2418,  0.1011,  0.0693, -0.0659,\n",
       "                       0.2420, -0.3155, -0.0923, -0.0922, -0.1334, -0.2230, -0.0677, -0.2631,\n",
       "                       0.1325, -0.2851,  0.2271, -0.0867, -0.3262, -0.1593,  0.2118, -0.0024,\n",
       "                       0.0680, -0.2808,  0.2477, -0.0715,  0.1627,  0.0273,  0.1027, -0.0802,\n",
       "                      -0.1827,  0.0885,  0.2418, -0.2955, -0.1138, -0.1325, -0.0638, -0.0724,\n",
       "                      -0.0977,  0.1621, -0.0775, -0.1105, -0.3212,  0.0644, -0.0940, -0.0933,\n",
       "                       0.1992, -0.1884, -0.3181,  0.0711,  0.0497, -0.0511, -0.0904,  0.2491,\n",
       "                      -0.0974, -0.1003, -0.0218,  0.3155, -0.0196,  0.2193, -0.0724,  0.1075,\n",
       "                       0.0016,  0.2484, -0.1853, -0.2786, -0.2159, -0.2622, -0.2641,  0.0200,\n",
       "                      -0.0683,  0.1250,  0.0634, -0.1987,  0.2260, -0.1976,  0.0159, -0.0952,\n",
       "                       0.0848, -0.0153, -0.0228,  0.0747,  0.0430, -0.2617, -0.1152,  0.1038,\n",
       "                      -0.0086, -0.2898,  0.2598, -0.2881, -0.2711,  0.0137,  0.3510,  0.0499,\n",
       "                      -0.1872,  0.0713, -0.1495, -0.3146, -0.1955, -0.1297,  0.1532, -0.3290,\n",
       "                       0.2656,  0.0210, -0.0690,  0.0779, -0.0932,  0.2327,  0.1838,  0.2340,\n",
       "                      -0.1098, -0.0815, -0.1118,  0.1104, -0.3286, -0.0971,  0.0559, -0.0931,\n",
       "                       0.2641, -0.0405, -0.0245,  0.2034, -0.0700, -0.0663, -0.0193, -0.0937,\n",
       "                      -0.1221,  0.0400, -0.0384,  0.0948,  0.0729, -0.1174,  0.1225,  0.1123,\n",
       "                      -0.0590, -0.0535,  0.2779,  0.0900, -0.1120, -0.2088, -0.1641, -0.3214,\n",
       "                      -0.2139,  0.0045, -0.0878,  0.1753, -0.2511, -0.1048,  0.0165, -0.2305,\n",
       "                       0.0748,  0.0674, -0.1509,  0.3136,  0.0222,  0.0767,  0.3169,  0.0230,\n",
       "                      -0.0886, -0.0628, -0.2846,  0.3376,  0.2656,  0.0637,  0.2695, -0.0721,\n",
       "                       0.2256,  0.1235, -0.0587, -0.1154, -0.0391,  0.1077, -0.0852, -0.0851,\n",
       "                      -0.3679,  0.2486, -0.2649,  0.2289, -0.0347, -0.0858, -0.1977, -0.1754,\n",
       "                       0.2931,  0.2398, -0.2862, -0.0261,  0.2652,  0.2364,  0.3239,  0.2025,\n",
       "                      -0.3275, -0.0847,  0.0984, -0.0578, -0.1138, -0.0477,  0.1080,  0.2022,\n",
       "                       0.3004,  0.0810, -0.3029, -0.0725,  0.1803, -0.0094, -0.2107,  0.0954,\n",
       "                      -0.2622, -0.0235,  0.2976,  0.2253, -0.2033, -0.0786,  0.3061, -0.1663,\n",
       "                      -0.0849,  0.0576,  0.0663, -0.0477, -0.0897, -0.0273,  0.2901,  0.0853,\n",
       "                       0.1133, -0.0896,  0.0365, -0.0894, -0.0510, -0.0815,  0.2648,  0.2376,\n",
       "                       0.2882, -0.0528,  0.0655, -0.2297,  0.1024, -0.2431, -0.0997,  0.0891,\n",
       "                      -0.2293, -0.0661,  0.1661,  0.0751, -0.0493, -0.0119, -0.0697, -0.0048,\n",
       "                       0.0295,  0.0323, -0.2993, -0.1014, -0.0547, -0.0235,  0.1589,  0.0913],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.emb.token_emb.weight',\n",
       "              tensor([[-0.1545, -0.1284, -0.1396,  ...,  0.1445, -0.1433,  0.1618],\n",
       "                      [ 0.0291,  0.0020, -0.0065,  ...,  0.0020,  0.0124,  0.0236],\n",
       "                      [-0.1420, -0.1693, -0.1597,  ...,  0.1765, -0.1460,  0.1785],\n",
       "                      ...,\n",
       "                      [-0.0195,  0.0137,  0.0185,  ...,  0.0190, -0.0105, -0.0169],\n",
       "                      [ 0.0273, -0.0224, -0.0141,  ...,  0.0239, -0.0029,  0.0171],\n",
       "                      [ 0.0254, -0.0169, -0.0252,  ..., -0.0275, -0.0132, -0.0166]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.self_attention.W_Q.weight',\n",
       "              tensor([[-0.2046, -0.1939, -0.2162,  ..., -0.2172,  0.0191, -0.0968],\n",
       "                      [-0.1598, -0.1277,  0.0727,  ...,  0.1003,  0.1616,  0.1416],\n",
       "                      [-0.2124, -0.1239, -0.1698,  ...,  0.1750,  0.0274,  0.1630],\n",
       "                      ...,\n",
       "                      [ 0.1544,  0.1144,  0.1126,  ...,  0.0896,  0.0308,  0.1594],\n",
       "                      [ 0.1497,  0.1528,  0.0946,  ..., -0.0806, -0.1818, -0.1554],\n",
       "                      [-0.1454, -0.1153, -0.1643,  ..., -0.1270,  0.0113, -0.1347]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.self_attention.W_K.weight',\n",
       "              tensor([[ 0.0877,  0.1562,  0.2147,  ...,  0.1333, -0.1813, -0.1954],\n",
       "                      [ 0.1809,  0.0757,  0.1253,  ..., -0.2084, -0.0533, -0.1129],\n",
       "                      [-0.1408,  0.1569, -0.0791,  ...,  0.1906, -0.1510, -0.1945],\n",
       "                      ...,\n",
       "                      [ 0.1218, -0.0938,  0.1141,  ..., -0.1722, -0.1957,  0.1053],\n",
       "                      [ 0.1980, -0.1542,  0.2073,  ..., -0.1066,  0.0845,  0.1517],\n",
       "                      [-0.0877, -0.1207, -0.2070,  ...,  0.1545, -0.0395, -0.1916]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.self_attention.W_V.weight',\n",
       "              tensor([[-0.2104, -0.1484, -0.2193,  ..., -0.2017, -0.1491, -0.1624],\n",
       "                      [ 0.1377,  0.1236,  0.1131,  ...,  0.1559,  0.1857,  0.1419],\n",
       "                      [ 0.1239,  0.1448,  0.1590,  ...,  0.0817,  0.2045,  0.1985],\n",
       "                      ...,\n",
       "                      [ 0.1517,  0.1705,  0.2246,  ...,  0.1623,  0.1784,  0.0828],\n",
       "                      [ 0.1384,  0.1518,  0.1029,  ...,  0.1733,  0.1252,  0.2046],\n",
       "                      [ 0.2208,  0.1659,  0.1430,  ...,  0.1421,  0.0902,  0.1803]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.self_attention.W_T.weight',\n",
       "              tensor([[-0.0942,  0.1730, -0.1273,  ..., -0.2001,  0.2264, -0.1526],\n",
       "                      [ 0.1434, -0.1559,  0.2246,  ...,  0.0964, -0.1824,  0.1778],\n",
       "                      [-0.1083,  0.0864, -0.1069,  ..., -0.1219,  0.1129, -0.1643],\n",
       "                      ...,\n",
       "                      [-0.1167, -0.1457,  0.1439,  ...,  0.1099, -0.1729,  0.1689],\n",
       "                      [-0.1355,  0.1044, -0.1488,  ..., -0.1379,  0.1591, -0.1812],\n",
       "                      [ 0.1981,  0.1834, -0.1171,  ..., -0.2118,  0.1065, -0.1513]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm1.gamma',\n",
       "              tensor([1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8479, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 1.1522, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8478, 0.8477, 1.1523, 1.1523, 0.8480,\n",
       "                      0.8477, 1.1523, 1.1522, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1522, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1522, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8478, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1468,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1522, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8478, 0.8477, 1.1523, 1.1522, 1.1523, 0.8477, 0.8477, 0.8477, 1.1520,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8478, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8480, 0.8477, 1.1523, 1.1523, 0.8479, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1521, 1.1523, 0.8478, 1.1523, 1.1522, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1515, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8479, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8478, 1.1523, 1.1523, 1.1522, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8479, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1518, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1522, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1502, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8503, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8478, 1.1523, 0.8477, 1.1523, 0.8477, 1.1522, 1.1523,\n",
       "                      1.1523, 0.8478, 0.8478, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1522, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8479, 0.8477, 0.8478, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1522, 1.1523, 1.1522, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1522, 0.8477, 1.1523, 1.1523, 1.1523, 0.8478, 1.1523,\n",
       "                      0.8481, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1521, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1522, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8479, 0.8484, 1.1520, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8478, 1.1523, 0.8479, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8484, 0.8477, 1.1520, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8478, 0.8891, 1.1523, 0.8477, 0.8477, 0.8477, 1.1499, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8483, 0.8477, 1.1523, 0.8478,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1522, 1.1523, 0.8478, 0.8477, 1.1523, 1.1523, 1.1522,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8479, 1.1522,\n",
       "                      0.8481, 0.8477, 0.8477, 0.8478, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8480, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1522, 0.8477,\n",
       "                      0.8480, 1.1523, 1.1522, 1.1522, 1.1523, 0.8477, 0.8478, 1.1521, 0.8488,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1510, 1.1523, 0.8477, 0.8477, 0.8478, 0.8501, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8478, 1.1523, 1.1523, 0.8477, 0.8477, 0.8489, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm1.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1522,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1511,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1522,  0.1523,\n",
       "                       0.1523,  0.1522, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1521, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1520,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1521,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1522, -0.1523,  0.1522,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1521, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1515,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1522,  0.1523,  0.1513, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1522, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1522,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1522, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1522, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1522, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1522, -0.1523,  0.1523,  0.1523, -0.1522,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1522,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1522, -0.1522, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1519,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1522,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1522,  0.1523,  0.1523,  0.1523,  0.1492, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1522,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1522, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1522,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[-0.0798, -0.1144, -0.1068,  ...,  0.1865, -0.1993,  0.1068],\n",
       "                      [-0.1206,  0.1135, -0.1893,  ..., -0.1096,  0.1626,  0.0849],\n",
       "                      [-0.0776, -0.0978,  0.1670,  ...,  0.2216,  0.2182,  0.1906],\n",
       "                      ...,\n",
       "                      [-0.1212,  0.0855,  0.0878,  ...,  0.1017,  0.2217,  0.0934],\n",
       "                      [ 0.1593, -0.0830, -0.2084,  ...,  0.0822, -0.0754, -0.1455],\n",
       "                      [ 0.2203, -0.1549, -0.2082,  ...,  0.2096, -0.0849, -0.2233]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.enc_dec_attention.W_K.weight',\n",
       "              tensor([[ 0.1873,  0.1654, -0.1479,  ..., -0.2080, -0.1561, -0.1770],\n",
       "                      [-0.1298, -0.1009,  0.1167,  ...,  0.1231,  0.2211, -0.1477],\n",
       "                      [ 0.0991,  0.1311, -0.1660,  ..., -0.0898, -0.1859, -0.1551],\n",
       "                      ...,\n",
       "                      [-0.1287,  0.1022,  0.1114,  ..., -0.1982,  0.1713,  0.1067],\n",
       "                      [ 0.1390,  0.1487,  0.1146,  ..., -0.0807, -0.2161,  0.1231],\n",
       "                      [ 0.2197,  0.2108,  0.1183,  ..., -0.1212, -0.2170,  0.0934]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.enc_dec_attention.W_V.weight',\n",
       "              tensor([[ 0.0833,  0.1062,  0.0941,  ...,  0.1402, -0.1751, -0.1243],\n",
       "                      [ 0.1232,  0.1578,  0.1111,  ..., -0.0808, -0.0957,  0.0959],\n",
       "                      [ 0.2222,  0.1699,  0.1375,  ...,  0.1732, -0.0816, -0.1298],\n",
       "                      ...,\n",
       "                      [-0.1904, -0.1538, -0.2105,  ...,  0.1006,  0.2159,  0.1353],\n",
       "                      [-0.1202, -0.1498, -0.0853,  ...,  0.1437,  0.1114, -0.2217],\n",
       "                      [ 0.1389,  0.0816,  0.2160,  ..., -0.0940, -0.1943,  0.1842]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.enc_dec_attention.W_T.weight',\n",
       "              tensor([[-0.1216, -0.1804, -0.1454,  ..., -0.1472, -0.1931, -0.1950],\n",
       "                      [ 0.1256,  0.1465,  0.1594,  ...,  0.1045,  0.1979,  0.1675],\n",
       "                      [-0.1182, -0.1772, -0.1030,  ..., -0.2258, -0.1860, -0.1887],\n",
       "                      ...,\n",
       "                      [ 0.1344,  0.1591,  0.1029,  ...,  0.1251,  0.1947,  0.1580],\n",
       "                      [-0.1804, -0.1774, -0.0992,  ..., -0.1549, -0.2163, -0.2245],\n",
       "                      [-0.1168, -0.1419, -0.1826,  ..., -0.0806, -0.1203, -0.0905]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm2.gamma',\n",
       "              tensor([1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8479, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1522, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1521,\n",
       "                      1.1519, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8480, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8485, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1520, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8478, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8479, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1522, 1.1523, 0.8478, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1521, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8479, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1516, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8478, 0.8479, 1.1523, 1.1523, 1.1522, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.9619, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8480, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8480, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1521, 1.1523, 0.8477, 0.8478, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8478, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8478, 1.1523, 1.1522,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8479, 0.8477, 0.8477, 1.1522, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8479, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8479,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1522, 0.8477, 0.8477, 0.8477, 0.8477, 1.1521, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8484, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1521, 1.1522,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1522, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1522, 1.1523,\n",
       "                      1.1504, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8479, 1.1523, 1.1519,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8483, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8485, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8482, 1.1523, 1.1523, 0.8478, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8478, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm2.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1522,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1521,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1521, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1521,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1522, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1519, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1522,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1522, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1522, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1481, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1522, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1522,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1521, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1521, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1522,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1522, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1519,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1522, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1521,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1521, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1522,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.ffn.layer1.weight',\n",
       "              tensor([[ 0.1351, -0.1361, -0.1918,  ...,  0.1779, -0.1675, -0.1621],\n",
       "                      [ 0.1522,  0.1277,  0.1978,  ..., -0.1418,  0.1332,  0.1968],\n",
       "                      [ 0.1452, -0.1557,  0.1852,  ...,  0.1903,  0.1627, -0.1895],\n",
       "                      ...,\n",
       "                      [-0.1484,  0.1869, -0.1387,  ..., -0.1239, -0.1147, -0.1341],\n",
       "                      [ 0.1193,  0.1206,  0.1382,  ...,  0.1826,  0.1754,  0.1214],\n",
       "                      [ 0.1286, -0.1136,  0.1484,  ...,  0.1261,  0.1610,  0.1966]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.ffn.layer1.bias',\n",
       "              tensor([-0.1190,  0.1784,  0.1581,  ..., -0.1882,  0.1580,  0.1207],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.ffn.layer2.weight',\n",
       "              tensor([[-0.1316, -0.1445, -0.1043,  ..., -0.1961, -0.1840, -0.1946],\n",
       "                      [ 0.1887,  0.1890,  0.1826,  ..., -0.1605,  0.1649,  0.1558],\n",
       "                      [-0.1468, -0.1703, -0.1398,  ..., -0.1906, -0.1249, -0.1515],\n",
       "                      ...,\n",
       "                      [ 0.1682,  0.1570,  0.1171,  ...,  0.1103,  0.1984,  0.1197],\n",
       "                      [-0.2001, -0.1545, -0.1508,  ..., -0.1975, -0.1878, -0.1448],\n",
       "                      [ 0.1912, -0.1265, -0.1696,  ..., -0.1715, -0.1810,  0.1314]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.ffn.layer2.bias',\n",
       "              tensor([-0.1423,  0.1317, -0.1707,  0.1450,  0.1576,  0.1492,  0.1679,  0.1421,\n",
       "                       0.1556, -0.1401,  0.1552,  0.1473,  0.1486,  0.1409,  0.1450,  0.1377,\n",
       "                      -0.1557, -0.1692, -0.1575,  0.1329, -0.1525, -0.1506, -0.1564,  0.1489,\n",
       "                       0.1524,  0.1387,  0.1583, -0.1364, -0.1434, -0.1370, -0.1671, -0.1409,\n",
       "                       0.1325,  0.1690, -0.1515, -0.1604, -0.1435,  0.1596,  0.1429,  0.1620,\n",
       "                       0.1591,  0.1428,  0.1653,  0.1585,  0.1341, -0.1574, -0.1608,  0.1502,\n",
       "                       0.1651, -0.1600,  0.1739,  0.1474, -0.1695, -0.1358, -0.1502,  0.1530,\n",
       "                       0.1536, -0.1722,  0.1527,  0.1412,  0.1518, -0.1340, -0.1447,  0.1311,\n",
       "                       0.1615,  0.1206,  0.1491, -0.1317, -0.1674, -0.1672,  0.1682, -0.1500,\n",
       "                      -0.1328, -0.1643,  0.1475, -0.1454, -0.1395, -0.1704, -0.1566, -0.1607,\n",
       "                       0.1412, -0.1365,  0.1612,  0.1535, -0.1524,  0.1672,  0.1368, -0.1630,\n",
       "                      -0.1738,  0.1339, -0.1307, -0.1730, -0.1680, -0.1679, -0.1522,  0.1658,\n",
       "                       0.1606,  0.1629, -0.1660, -0.1429, -0.1348, -0.1313, -0.1614,  0.1525,\n",
       "                      -0.1490,  0.1544, -0.1522,  0.1507, -0.1515,  0.1686, -0.1354,  0.1389,\n",
       "                      -0.1646, -0.1399,  0.1456, -0.1529, -0.1598, -0.1665,  0.1398,  0.1430,\n",
       "                      -0.1701,  0.1526,  0.1671,  0.1337,  0.1600, -0.1550,  0.1336, -0.1463,\n",
       "                       0.1742, -0.1324, -0.1435, -0.1417, -0.1392,  0.1656,  0.1721, -0.1473,\n",
       "                       0.1401, -0.1440, -0.1535,  0.1560,  0.1316,  0.1507,  0.1716,  0.1569,\n",
       "                       0.1712,  0.1690, -0.1559,  0.1342, -0.1583, -0.1461, -0.1413,  0.1473,\n",
       "                      -0.1481, -0.1393, -0.1632, -0.1626, -0.1544,  0.1539,  0.1711, -0.1393,\n",
       "                       0.1349, -0.1309, -0.1637,  0.1330,  0.1724,  0.1703,  0.1544, -0.1347,\n",
       "                      -0.1620,  0.1354,  0.1446,  0.1680, -0.1434,  0.1510,  0.1726,  0.1683,\n",
       "                       0.1567,  0.1452,  0.1307,  0.1699, -0.1641,  0.1671, -0.1554, -0.1353,\n",
       "                       0.1304,  0.1348,  0.1651, -0.1506, -0.1646,  0.1721, -0.1622,  0.1434,\n",
       "                       0.1408, -0.1406,  0.1564,  0.1678, -0.1518,  0.1607, -0.1577, -0.1589,\n",
       "                       0.1624,  0.1433,  0.1471,  0.1484, -0.1371,  0.1325,  0.1672,  0.1586,\n",
       "                      -0.1725, -0.1303, -0.1558, -0.1560,  0.1601, -0.1338, -0.1561,  0.1621,\n",
       "                       0.1415,  0.1605, -0.1466, -0.1722,  0.1465,  0.1362,  0.1574,  0.1544,\n",
       "                       0.1527, -0.1387,  0.1576, -0.1465,  0.1371,  0.1536, -0.1488,  0.1322,\n",
       "                       0.1451,  0.1438, -0.1432,  0.1378, -0.1412,  0.1491,  0.1665, -0.1619,\n",
       "                       0.1527,  0.1737, -0.1588, -0.1435,  0.1453, -0.1500, -0.1690, -0.1372,\n",
       "                      -0.1540,  0.1482, -0.1477, -0.1651,  0.1411, -0.1418,  0.1664,  0.1384,\n",
       "                      -0.1313, -0.1378, -0.1566, -0.1683, -0.1564,  0.1714, -0.1591,  0.1384,\n",
       "                      -0.1608,  0.1722,  0.1385,  0.1322,  0.1646,  0.1742,  0.1713, -0.1341,\n",
       "                       0.1498,  0.1421, -0.1717, -0.1618,  0.1424,  0.1593, -0.1435, -0.1740,\n",
       "                       0.1571, -0.1565,  0.1678,  0.1683,  0.1390, -0.1498, -0.1593, -0.1335,\n",
       "                       0.1371, -0.1404, -0.1714,  0.1419, -0.1501, -0.1627, -0.1515,  0.1427,\n",
       "                      -0.1669, -0.1345, -0.1413,  0.1477,  0.1607,  0.1458, -0.1663,  0.1409,\n",
       "                       0.1730, -0.1556, -0.1654,  0.1652,  0.1376, -0.1734, -0.1364,  0.1560,\n",
       "                       0.1455, -0.1583, -0.1527,  0.1369, -0.1383, -0.1711,  0.1572,  0.1512,\n",
       "                      -0.1543,  0.1453,  0.1368, -0.1590,  0.1561,  0.1369,  0.1561, -0.1385,\n",
       "                       0.1344,  0.1341,  0.1304, -0.1638,  0.1533,  0.1566, -0.1333,  0.1624,\n",
       "                      -0.1394,  0.1684, -0.1424, -0.1342, -0.1706,  0.1727,  0.1735, -0.1612,\n",
       "                      -0.1567,  0.1440, -0.1667, -0.1430,  0.1649,  0.1571,  0.1689,  0.1558,\n",
       "                      -0.1456, -0.1693,  0.1434, -0.1478,  0.1735, -0.1350, -0.1465, -0.1321,\n",
       "                       0.1576, -0.1408,  0.1644,  0.1716,  0.1675, -0.1304,  0.1618, -0.1554,\n",
       "                      -0.1369,  0.1524,  0.1631, -0.1308,  0.1686, -0.1570, -0.1350, -0.1740,\n",
       "                      -0.1318, -0.1682,  0.1626, -0.1483, -0.1702, -0.1581,  0.1487, -0.1592,\n",
       "                      -0.1626, -0.1411, -0.1318, -0.1503, -0.1659,  0.1692,  0.1544, -0.1402,\n",
       "                       0.1705,  0.1313, -0.1731, -0.1679,  0.1509, -0.1335,  0.1687,  0.1594,\n",
       "                      -0.1564,  0.1400,  0.1583, -0.1533,  0.1713,  0.1322, -0.1455, -0.1521,\n",
       "                       0.1610,  0.1611, -0.1516, -0.1532, -0.1522, -0.1357,  0.1440,  0.1310,\n",
       "                       0.1516, -0.1694,  0.1349,  0.1607,  0.1588, -0.1628, -0.1631,  0.1358,\n",
       "                       0.1399, -0.1688, -0.1566, -0.1414,  0.1534,  0.1487, -0.1728, -0.1721,\n",
       "                       0.1478,  0.1738,  0.1610,  0.1403, -0.1398, -0.1689, -0.1635,  0.1322,\n",
       "                      -0.1417, -0.1549,  0.1449, -0.1499, -0.1535, -0.1562,  0.1660, -0.1373,\n",
       "                      -0.1642, -0.1707, -0.1485, -0.1714, -0.1717, -0.1352,  0.1361,  0.1397,\n",
       "                      -0.1545,  0.1366,  0.1389,  0.1585, -0.1660, -0.1384, -0.1613,  0.1713,\n",
       "                      -0.1349, -0.1649, -0.1578,  0.1692, -0.1319, -0.1729, -0.1616,  0.1713,\n",
       "                       0.1453,  0.1504, -0.1301,  0.1448, -0.1540,  0.1491,  0.1584,  0.1667,\n",
       "                      -0.1483, -0.1620, -0.1552,  0.1673, -0.1714, -0.1645, -0.1477,  0.1736,\n",
       "                       0.1596,  0.1326, -0.1702,  0.1692, -0.1711,  0.1743, -0.1330, -0.1527,\n",
       "                       0.1481,  0.1365, -0.1735, -0.1309, -0.1526,  0.1699, -0.1445,  0.1710,\n",
       "                      -0.1414, -0.1414,  0.1340, -0.1361,  0.1635,  0.1600, -0.1326, -0.1519],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm3.gamma',\n",
       "              tensor([1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8548, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1522, 0.8477, 0.8477, 1.1522, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8479, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1519,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8481, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8481, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1521, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8478, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1522, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1522, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1501, 0.8477, 1.1523, 1.1523, 1.1523, 0.8500, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8479, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8489, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1521, 0.8477, 1.1523, 1.1523, 1.1522, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8478, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8491, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1522,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1522,\n",
       "                      0.8477, 0.8484, 0.8477, 1.1523, 1.1523, 1.1493, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1513, 1.1523, 1.1523, 1.1522, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1522, 0.8483, 1.1523, 0.8477, 0.8478, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1520, 1.1523, 1.1523, 0.8478, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 0.8478, 1.1521,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8482, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8479, 0.8477, 0.8484,\n",
       "                      1.1523, 1.1517, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8479, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1522, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1521, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1522,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1520, 0.8477, 1.1523, 1.1518, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.0.layer_norm3.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1522,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1522, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1522, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1521,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1521, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1522,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1521, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1521, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1522, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1517, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1521,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1518, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1522, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1522,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1522,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1522,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1522,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1514,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.self_attention.W_Q.weight',\n",
       "              tensor([[-0.2100, -0.1033,  0.0892,  ...,  0.0746, -0.2012,  0.0850],\n",
       "                      [ 0.1268,  0.1190, -0.1658,  ...,  0.1554,  0.2157,  0.2011],\n",
       "                      [ 0.1591, -0.1941, -0.0981,  ..., -0.1775, -0.1019, -0.1914],\n",
       "                      ...,\n",
       "                      [-0.1183, -0.1031, -0.1248,  ...,  0.1479,  0.0903,  0.1933],\n",
       "                      [-0.2176,  0.2014, -0.1966,  ..., -0.0923,  0.0876,  0.1108],\n",
       "                      [ 0.1238, -0.2108,  0.2022,  ...,  0.1637, -0.2012, -0.1006]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.self_attention.W_K.weight',\n",
       "              tensor([[-0.1513,  0.1503, -0.1196,  ..., -0.0926,  0.1140,  0.1228],\n",
       "                      [-0.2253, -0.1944,  0.2038,  ...,  0.0836, -0.0751, -0.1541],\n",
       "                      [-0.1591,  0.0818, -0.1704,  ...,  0.2268,  0.1515, -0.1330],\n",
       "                      ...,\n",
       "                      [-0.1022,  0.0848, -0.1154,  ...,  0.2267,  0.1111, -0.1073],\n",
       "                      [-0.1891, -0.1204, -0.1679,  ..., -0.1815, -0.1994,  0.1765],\n",
       "                      [ 0.1885,  0.1144,  0.2009,  ...,  0.1548,  0.0667, -0.2074]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.self_attention.W_V.weight',\n",
       "              tensor([[-0.1518,  0.1915, -0.1767,  ...,  0.1190,  0.1575,  0.0932],\n",
       "                      [-0.0828,  0.1459, -0.1608,  ...,  0.1375,  0.2084,  0.1748],\n",
       "                      [-0.1526,  0.2151,  0.0971,  ..., -0.1894,  0.0944,  0.1530],\n",
       "                      ...,\n",
       "                      [-0.1417,  0.1949, -0.1984,  ...,  0.1983,  0.1318,  0.1775],\n",
       "                      [ 0.1563, -0.1170,  0.2006,  ...,  0.2094, -0.1605, -0.0778],\n",
       "                      [ 0.1871, -0.1500, -0.1882,  ...,  0.1203, -0.1540, -0.0839]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.self_attention.W_T.weight',\n",
       "              tensor([[ 0.1552,  0.1832, -0.0801,  ..., -0.1579,  0.1700,  0.1959],\n",
       "                      [ 0.0820, -0.1662,  0.1327,  ..., -0.1014, -0.2205, -0.2281],\n",
       "                      [-0.2189,  0.1295, -0.1915,  ...,  0.0975,  0.1857,  0.1159],\n",
       "                      ...,\n",
       "                      [ 0.1386, -0.2078,  0.1966,  ...,  0.1912,  0.1410, -0.1762],\n",
       "                      [ 0.2006,  0.1740,  0.1166,  ...,  0.1169,  0.1194,  0.1482],\n",
       "                      [ 0.2061,  0.1647, -0.0902,  ...,  0.1220,  0.1883,  0.1812]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm1.gamma',\n",
       "              tensor([1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1521, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8482, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8479, 0.8480,\n",
       "                      0.8477, 0.8478, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1487, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 0.8480, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8478, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8478, 1.1523, 1.1522, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1522, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8480, 0.8480, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8894, 0.8478, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1522, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1522, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8478, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1522, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1522, 0.8477, 1.1523,\n",
       "                      1.1521, 1.1523, 0.8477, 1.1521, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8478, 0.8478, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1522, 1.1519, 0.8477, 1.1522, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1522, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1521, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1521, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8478, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1522, 0.8477, 0.8477, 0.8477, 0.8477, 1.1522, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 0.8477],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm1.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1498, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1522, -0.1523,  0.1523, -0.1523, -0.1523, -0.1516, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1522,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1506,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1521,  0.1523, -0.1523, -0.1523, -0.1523,  0.1521, -0.1522,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1522, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1522, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1518, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1522, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1522,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1522, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1522, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1522,  0.1523,  0.1523, -0.1523,  0.1520, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[-0.2157, -0.2178,  0.1850,  ..., -0.0786, -0.1662, -0.1822],\n",
       "                      [-0.1457,  0.1313,  0.1563,  ...,  0.1466, -0.1174, -0.1547],\n",
       "                      [ 0.1062,  0.1981,  0.1340,  ...,  0.1637,  0.1225,  0.2037],\n",
       "                      ...,\n",
       "                      [-0.1538, -0.1573,  0.1552,  ...,  0.1653, -0.0760, -0.1123],\n",
       "                      [-0.1446,  0.0888, -0.1064,  ...,  0.1397,  0.2034,  0.1581],\n",
       "                      [-0.0756,  0.1810, -0.1062,  ..., -0.2195,  0.0757,  0.2157]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.enc_dec_attention.W_K.weight',\n",
       "              tensor([[ 0.1854, -0.2053,  0.1433,  ..., -0.2272, -0.1562,  0.0900],\n",
       "                      [ 0.1065,  0.2006, -0.1719,  ..., -0.1183, -0.1630,  0.1380],\n",
       "                      [-0.1067, -0.1005,  0.1020,  ..., -0.1441,  0.2004, -0.1540],\n",
       "                      ...,\n",
       "                      [ 0.2232,  0.1350,  0.1612,  ...,  0.1126, -0.0817,  0.1412],\n",
       "                      [ 0.0822, -0.1231, -0.1370,  ..., -0.1317, -0.1625, -0.1931],\n",
       "                      [ 0.1270, -0.2121,  0.1484,  ...,  0.1894, -0.2108, -0.1350]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.enc_dec_attention.W_V.weight',\n",
       "              tensor([[ 0.0723, -0.1233,  0.1353,  ..., -0.1406, -0.0947,  0.1338],\n",
       "                      [ 0.2235,  0.1966,  0.2239,  ..., -0.1998, -0.0845,  0.1297],\n",
       "                      [ 0.1143,  0.1952,  0.2048,  ..., -0.1072, -0.1046,  0.1835],\n",
       "                      ...,\n",
       "                      [-0.1868, -0.0832, -0.1580,  ..., -0.2010,  0.1329,  0.1785],\n",
       "                      [ 0.0940,  0.1402,  0.1426,  ..., -0.1019, -0.1834, -0.0813],\n",
       "                      [-0.2166, -0.2161, -0.1228,  ..., -0.0951, -0.1077, -0.1760]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.enc_dec_attention.W_T.weight',\n",
       "              tensor([[-0.1522, -0.2067, -0.1795,  ..., -0.1377, -0.1235, -0.1536],\n",
       "                      [-0.1471,  0.0795,  0.2089,  ...,  0.1093,  0.2066,  0.2251],\n",
       "                      [ 0.1976, -0.1424, -0.0759,  ..., -0.1976, -0.0778, -0.1279],\n",
       "                      ...,\n",
       "                      [ 0.1059,  0.1307, -0.1582,  ...,  0.2241,  0.1960, -0.2249],\n",
       "                      [ 0.1104, -0.1783, -0.1681,  ..., -0.1793, -0.1580, -0.1778],\n",
       "                      [ 0.1897, -0.2219, -0.1091,  ..., -0.1273, -0.2031, -0.1708]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm2.gamma',\n",
       "              tensor([1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8478, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8478, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1521,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8482, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1522, 0.8477, 1.1522, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1521, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8478, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8478, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1522, 0.8477, 0.8477, 0.8478,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8478, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1520, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1513, 1.1523, 0.8477, 0.8477, 0.8477, 0.8478, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1522, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1522, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1522, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8587, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1522, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1522, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8480, 1.1523, 0.8477, 1.1514, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1517, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1513, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1521, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1522, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm2.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1522, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1522,  0.1523, -0.1522, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1521, -0.1523,  0.1523, -0.1523, -0.1523,  0.1522, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1522,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1521,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1522,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1521,  0.1523, -0.1523, -0.1523, -0.1523,  0.1519, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1521, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1522,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1522, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1520,  0.1523,  0.1523, -0.1523,  0.1519, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.ffn.layer1.weight',\n",
       "              tensor([[-0.1947,  0.1814,  0.1102,  ..., -0.1726, -0.1924,  0.1389],\n",
       "                      [ 0.1801,  0.1167, -0.1698,  ...,  0.1631,  0.1984,  0.1217],\n",
       "                      [ 0.1446, -0.1303,  0.1424,  ...,  0.1314, -0.1902,  0.1166],\n",
       "                      ...,\n",
       "                      [-0.1956, -0.1056, -0.1357,  ..., -0.1472, -0.1686, -0.1044],\n",
       "                      [-0.1499,  0.1347, -0.1250,  ...,  0.1714, -0.1809,  0.1447],\n",
       "                      [ 0.1422,  0.1367, -0.1490,  ...,  0.1424,  0.1512,  0.1124]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.ffn.layer1.bias',\n",
       "              tensor([-0.1695, -0.1665, -0.1764,  ...,  0.1497, -0.1274, -0.1093],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.ffn.layer2.weight',\n",
       "              tensor([[-0.1265, -0.1800, -0.1622,  ...,  0.1538,  0.1432, -0.1384],\n",
       "                      [ 0.1974,  0.1472,  0.1238,  ...,  0.1541,  0.1765,  0.1824],\n",
       "                      [-0.1689, -0.1162, -0.1563,  ...,  0.1318, -0.1388, -0.1770],\n",
       "                      ...,\n",
       "                      [ 0.1079,  0.1846, -0.1676,  ..., -0.1704,  0.1775,  0.1264],\n",
       "                      [ 0.1902, -0.1269, -0.1107,  ...,  0.1488,  0.1213,  0.1711],\n",
       "                      [-0.1101, -0.1662, -0.1341,  ..., -0.1430, -0.1154, -0.1518]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.ffn.layer2.bias',\n",
       "              tensor([-0.1733,  0.1372, -0.1336, -0.1563,  0.1725,  0.1450, -0.1347,  0.1692,\n",
       "                       0.1700, -0.1663,  0.1719,  0.1496, -0.1489,  0.1603,  0.1333,  0.1386,\n",
       "                      -0.1463,  0.1328, -0.1473,  0.1407,  0.1559, -0.1629, -0.1397,  0.1580,\n",
       "                       0.1584,  0.1571,  0.1534, -0.1661, -0.1525, -0.1351, -0.1614,  0.1419,\n",
       "                       0.1451,  0.1407, -0.1308, -0.1740, -0.1644, -0.1596,  0.1709,  0.1373,\n",
       "                       0.1671,  0.1535,  0.1640,  0.1507,  0.1308, -0.1374, -0.1699,  0.1447,\n",
       "                       0.1330,  0.1522, -0.1324, -0.1357,  0.1614, -0.1674, -0.1511,  0.1621,\n",
       "                       0.1359, -0.1729,  0.1345,  0.1724, -0.1562, -0.1563, -0.1736,  0.1666,\n",
       "                       0.1576, -0.1624,  0.1556, -0.1369, -0.1736, -0.1563, -0.1555, -0.1506,\n",
       "                      -0.1601, -0.1392,  0.1475, -0.1369, -0.1500, -0.1547, -0.1674, -0.1346,\n",
       "                       0.1466,  0.1689,  0.1413,  0.1697, -0.1658,  0.1647,  0.1660,  0.1314,\n",
       "                       0.1352, -0.1700, -0.1505, -0.1736, -0.1379, -0.1310, -0.1495,  0.1405,\n",
       "                       0.1743,  0.1589, -0.1695, -0.1585,  0.1393,  0.1461,  0.1725, -0.1742,\n",
       "                       0.1570, -0.1602, -0.1620,  0.1459,  0.1661,  0.1323,  0.1425,  0.1547,\n",
       "                      -0.1519, -0.1515, -0.1322, -0.1571, -0.1680, -0.1688,  0.1472,  0.1732,\n",
       "                      -0.1641,  0.1674,  0.1478, -0.1552, -0.1665,  0.1465,  0.1358, -0.1633,\n",
       "                       0.1366,  0.1583, -0.1488, -0.1667,  0.1423,  0.1692, -0.1588,  0.1544,\n",
       "                       0.1740,  0.1317, -0.1642,  0.1342, -0.1557, -0.1375,  0.1560,  0.1659,\n",
       "                       0.1579,  0.1702, -0.1358,  0.1308,  0.1503, -0.1675,  0.1344,  0.1619,\n",
       "                       0.1680, -0.1508, -0.1484, -0.1607, -0.1358, -0.1618,  0.1689, -0.1510,\n",
       "                       0.1454, -0.1547, -0.1394,  0.1471,  0.1429,  0.1362,  0.1434, -0.1668,\n",
       "                      -0.1444,  0.1590, -0.1381,  0.1538, -0.1306,  0.1625,  0.1686,  0.1727,\n",
       "                       0.1499,  0.1434,  0.1325, -0.1595, -0.1730, -0.1471, -0.1409, -0.1497,\n",
       "                       0.1314,  0.1612,  0.1357, -0.1443,  0.1539, -0.1668, -0.1715,  0.1398,\n",
       "                       0.1714, -0.1473,  0.1715, -0.1467, -0.1605, -0.1483,  0.1366, -0.1319,\n",
       "                       0.1358,  0.1402, -0.1729,  0.1466, -0.1596,  0.1727,  0.1739,  0.1666,\n",
       "                       0.1457, -0.1621, -0.1398, -0.1467,  0.1716, -0.1350, -0.1547,  0.1723,\n",
       "                      -0.1702, -0.1472, -0.1508, -0.1604, -0.1492,  0.1664,  0.1571,  0.1346,\n",
       "                       0.1626, -0.1573,  0.1703, -0.1604,  0.1656,  0.1482,  0.1350,  0.1483,\n",
       "                       0.1730,  0.1655, -0.1692, -0.1322, -0.1658,  0.1581,  0.1508, -0.1689,\n",
       "                       0.1371,  0.1729, -0.1598, -0.1522, -0.1632, -0.1675, -0.1533,  0.1725,\n",
       "                       0.1557, -0.1698,  0.1417,  0.1658,  0.1733, -0.1727, -0.1341, -0.1504,\n",
       "                      -0.1537, -0.1388, -0.1651, -0.1487, -0.1678,  0.1498, -0.1463, -0.1620,\n",
       "                       0.1741,  0.1689, -0.1470, -0.1509, -0.1663,  0.1738, -0.1635, -0.1459,\n",
       "                       0.1543,  0.1449,  0.1408, -0.1315, -0.1665,  0.1376, -0.1574, -0.1427,\n",
       "                       0.1560, -0.1445, -0.1622,  0.1673,  0.1323,  0.1338, -0.1674,  0.1366,\n",
       "                      -0.1590, -0.1455,  0.1489, -0.1412, -0.1405,  0.1669, -0.1689,  0.1707,\n",
       "                      -0.1454, -0.1564,  0.1317, -0.1334, -0.1380, -0.1413, -0.1520, -0.1526,\n",
       "                       0.1372,  0.1674,  0.1508,  0.1491,  0.1346, -0.1661, -0.1471,  0.1726,\n",
       "                       0.1314,  0.1411, -0.1719,  0.1304, -0.1629, -0.1375,  0.1306,  0.1606,\n",
       "                      -0.1401, -0.1587,  0.1633, -0.1710, -0.1385, -0.1319,  0.1664, -0.1425,\n",
       "                       0.1555,  0.1697,  0.1550, -0.1381,  0.1391,  0.1320, -0.1473, -0.1540,\n",
       "                      -0.1702,  0.1394,  0.1414, -0.1571, -0.1403,  0.1668,  0.1392, -0.1690,\n",
       "                      -0.1534,  0.1592,  0.1715, -0.1499,  0.1518,  0.1305, -0.1514,  0.1323,\n",
       "                      -0.1613, -0.1607,  0.1727,  0.1615,  0.1588, -0.1662,  0.1397, -0.1330,\n",
       "                       0.1637,  0.1467,  0.1435,  0.1716, -0.1304, -0.1676, -0.1468, -0.1313,\n",
       "                      -0.1333,  0.1602, -0.1412, -0.1322,  0.1452,  0.1489, -0.1493,  0.1524,\n",
       "                       0.1648,  0.1312,  0.1695, -0.1698,  0.1659, -0.1542,  0.1314, -0.1685,\n",
       "                      -0.1345, -0.1552, -0.1527, -0.1334,  0.1553,  0.1455,  0.1621, -0.1640,\n",
       "                       0.1603,  0.1669,  0.1511, -0.1351,  0.1532, -0.1667,  0.1382,  0.1424,\n",
       "                      -0.1675,  0.1397,  0.1517, -0.1358, -0.1511,  0.1720, -0.1404,  0.1577,\n",
       "                       0.1724,  0.1700, -0.1485, -0.1446,  0.1334, -0.1520, -0.1505,  0.1741,\n",
       "                       0.1639, -0.1375,  0.1359,  0.1639,  0.1713, -0.1436, -0.1740, -0.1394,\n",
       "                      -0.1528, -0.1480, -0.1351,  0.1354, -0.1512,  0.1318, -0.1362, -0.1581,\n",
       "                      -0.1386, -0.1705,  0.1715,  0.1636, -0.1429,  0.1562, -0.1592,  0.1675,\n",
       "                       0.1595, -0.1554,  0.1344, -0.1548, -0.1454, -0.1481,  0.1335, -0.1609,\n",
       "                       0.1408, -0.1675, -0.1371, -0.1304,  0.1342, -0.1538, -0.1491,  0.1491,\n",
       "                      -0.1356,  0.1512, -0.1531, -0.1422,  0.1445, -0.1509,  0.1322,  0.1386,\n",
       "                      -0.1506,  0.1691, -0.1538,  0.1738, -0.1739, -0.1431,  0.1369,  0.1317,\n",
       "                      -0.1521,  0.1370,  0.1742, -0.1425,  0.1392, -0.1663,  0.1311,  0.1571,\n",
       "                       0.1593, -0.1654,  0.1316,  0.1586, -0.1643, -0.1696,  0.1697,  0.1569,\n",
       "                       0.1590,  0.1620, -0.1533, -0.1618, -0.1396,  0.1737, -0.1363,  0.1693,\n",
       "                       0.1620,  0.1661, -0.1461, -0.1714,  0.1486, -0.1680, -0.1536,  0.1650,\n",
       "                       0.1565, -0.1700,  0.1544, -0.1731, -0.1674, -0.1340, -0.1598, -0.1451],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm3.gamma',\n",
       "              tensor([0.8477, 0.8475, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8478, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8489, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8476, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1532, 0.8477, 1.1524,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8476, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8479, 0.8477, 1.1523, 0.8477, 0.8477, 0.8478,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8481, 1.1523, 1.1523, 1.1523, 1.1520, 0.8477, 1.1523,\n",
       "                      1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 1.1450,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8476, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8478,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8478, 0.8477, 0.8481,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8476, 1.1521,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8480, 0.8477, 1.1523, 1.1522, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8476, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8478, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1524, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1512, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8476, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8479, 0.8477, 1.1522, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8478,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8516, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8478, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8476, 1.1524, 1.1523, 1.1521, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1522, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1524, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1521, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8512, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8478, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.1.layer_norm3.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1522,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1522, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1522,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1522,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1525, -0.1524, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1505,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1522,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1524, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1522,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1524, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1522, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1522,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1520, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1524, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1520,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1519,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1522, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523,  0.1517, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1522,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1521, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1525,  0.1523,  0.1523, -0.1523,  0.1501, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1522,  0.1523, -0.1523, -0.1523,  0.1523, -0.1524, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1520, -0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.self_attention.W_Q.weight',\n",
       "              tensor([[ 0.1620, -0.1920,  0.0774,  ..., -0.0996,  0.1568, -0.0765],\n",
       "                      [-0.1538, -0.1180, -0.2203,  ...,  0.1384,  0.1742,  0.0888],\n",
       "                      [-0.1428, -0.0921, -0.2221,  ...,  0.1886,  0.2188,  0.1296],\n",
       "                      ...,\n",
       "                      [-0.2207, -0.1549, -0.0912,  ...,  0.2028,  0.2145,  0.2086],\n",
       "                      [ 0.1496,  0.0831, -0.0744,  ..., -0.1341,  0.1960, -0.0860],\n",
       "                      [-0.1161, -0.1686, -0.0994,  ...,  0.1585,  0.1884,  0.1006]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.self_attention.W_K.weight',\n",
       "              tensor([[-0.1659,  0.1358, -0.1306,  ...,  0.1239, -0.1634, -0.1051],\n",
       "                      [ 0.1840, -0.1931, -0.0914,  ..., -0.2244, -0.1243, -0.2121],\n",
       "                      [ 0.1447,  0.1090, -0.1621,  ..., -0.1206,  0.0936, -0.1878],\n",
       "                      ...,\n",
       "                      [-0.1983,  0.1465, -0.2273,  ..., -0.1513,  0.1475, -0.1538],\n",
       "                      [-0.1062,  0.2223,  0.1097,  ...,  0.0766,  0.0750,  0.1389],\n",
       "                      [ 0.1343, -0.1193,  0.1402,  ..., -0.0790, -0.1976,  0.0793]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.self_attention.W_V.weight',\n",
       "              tensor([[-0.2240, -0.2223, -0.1863,  ...,  0.1039,  0.1513,  0.1403],\n",
       "                      [ 0.1371, -0.2278,  0.2241,  ..., -0.1795,  0.1506, -0.2022],\n",
       "                      [-0.2196, -0.1696, -0.1891,  ...,  0.1447, -0.1882,  0.0835],\n",
       "                      ...,\n",
       "                      [-0.1626,  0.1119, -0.0796,  ...,  0.1185, -0.1138,  0.1771],\n",
       "                      [-0.1378,  0.1310, -0.1019,  ...,  0.1205, -0.2141,  0.1628],\n",
       "                      [-0.1330,  0.0783, -0.1906,  ...,  0.0790, -0.1009,  0.2249]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.self_attention.W_T.weight',\n",
       "              tensor([[-0.2023, -0.1149, -0.0776,  ..., -0.0917, -0.1871,  0.1300],\n",
       "                      [ 0.1143,  0.1165,  0.2132,  ...,  0.1771,  0.1791,  0.2082],\n",
       "                      [-0.1433, -0.1065,  0.2212,  ..., -0.0836, -0.1160,  0.1582],\n",
       "                      ...,\n",
       "                      [-0.1483, -0.1964, -0.0949,  ..., -0.0836,  0.1576,  0.1154],\n",
       "                      [ 0.1881,  0.2174, -0.1234,  ...,  0.0785,  0.1191, -0.1643],\n",
       "                      [-0.1703, -0.1019,  0.0953,  ..., -0.0825, -0.2027,  0.1343]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm1.gamma',\n",
       "              tensor([0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8478, 1.1523, 1.1522, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8479, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1529, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1524, 1.1523, 1.1523, 0.8477, 1.1523, 1.1524,\n",
       "                      0.8479, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1524, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1525, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1520, 1.1522,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8478, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1523, 1.1523, 1.1523, 0.8477, 0.8476, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1519, 0.8480, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 1.1524,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1522, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 1.1522, 0.8477, 0.8478, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1522, 1.1523, 1.1522, 1.1523, 1.1523, 0.8482, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1521, 1.1521, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1510,\n",
       "                      1.1523, 1.1523, 0.8476, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8480, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1517, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8474, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8480, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8482, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1525, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 0.8478, 0.8476, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1524, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1522, 1.1523,\n",
       "                      1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1512, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 1.1522, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 1.1524, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1522, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      1.1525, 0.8477, 0.8477, 1.1524, 0.8477, 0.8477, 0.8478, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1521, 1.1523, 0.8476, 0.8477, 0.8477, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1504, 1.1523, 0.8477, 0.8479, 0.8477, 1.1523, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 0.8477, 0.8476, 0.8477, 0.8477, 1.1524, 1.1522, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1530, 1.1523, 1.1523, 0.8477, 0.8477, 0.8483, 1.1523,\n",
       "                      0.8477, 0.8477, 1.1523, 0.8479, 1.1523, 1.1523, 0.8477, 1.1524, 0.8477,\n",
       "                      1.1522, 0.8477, 1.1523, 0.8477, 0.8476, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8476, 1.1522, 1.1523, 1.1523, 1.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm1.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1521,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1522,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1522, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1522,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1524,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1524, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1522,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1525,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1522, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1521,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1522, -0.1523,  0.1519, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1530,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1522,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1522, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1519,  0.1523, -0.1523,  0.1523, -0.1524,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1524, -0.1522, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1522, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1524, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1525,  0.1523,  0.1523, -0.1523, -0.1523,  0.1524,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1525, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1524, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1521, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[-0.2180, -0.0782, -0.1313,  ...,  0.1132, -0.2053,  0.1132],\n",
       "                      [-0.1960, -0.2114, -0.1223,  ..., -0.0813,  0.0922,  0.2215],\n",
       "                      [ 0.2126, -0.0706,  0.0987,  ...,  0.0938,  0.1885, -0.2257],\n",
       "                      ...,\n",
       "                      [-0.1724,  0.1435, -0.1372,  ..., -0.0835,  0.1736,  0.1557],\n",
       "                      [-0.1838,  0.2050, -0.0819,  ..., -0.2259, -0.2087,  0.2144],\n",
       "                      [-0.1863, -0.1375, -0.0808,  ..., -0.1640,  0.1023,  0.0814]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.enc_dec_attention.W_K.weight',\n",
       "              tensor([[ 0.0937,  0.1219,  0.1636,  ..., -0.1906,  0.1009, -0.1724],\n",
       "                      [ 0.1198,  0.0858,  0.1693,  ..., -0.1864,  0.2265, -0.0783],\n",
       "                      [-0.1874, -0.2208, -0.1131,  ...,  0.1347, -0.1878,  0.1771],\n",
       "                      ...,\n",
       "                      [-0.0889,  0.1026, -0.1948,  ..., -0.0848,  0.2087,  0.1682],\n",
       "                      [ 0.1698,  0.2216, -0.1962,  ...,  0.0882, -0.1292, -0.2229],\n",
       "                      [ 0.1305,  0.0866, -0.1884,  ...,  0.1754, -0.1813, -0.1103]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.enc_dec_attention.W_V.weight',\n",
       "              tensor([[ 0.1492,  0.2256,  0.1813,  ..., -0.0760, -0.1146,  0.0823],\n",
       "                      [ 0.0826,  0.2276,  0.0834,  ...,  0.1215, -0.1895,  0.1613],\n",
       "                      [ 0.0817,  0.1970,  0.1739,  ..., -0.1467, -0.1765, -0.1002],\n",
       "                      ...,\n",
       "                      [-0.1579, -0.1474, -0.1574,  ...,  0.1373,  0.1196,  0.1192],\n",
       "                      [-0.1425, -0.1521, -0.2269,  ..., -0.2037,  0.1837,  0.1525],\n",
       "                      [ 0.1444,  0.1415,  0.1149,  ..., -0.0966, -0.1468,  0.1850]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.enc_dec_attention.W_T.weight',\n",
       "              tensor([[ 0.2204,  0.0931,  0.2239,  ..., -0.1102,  0.1951, -0.1049],\n",
       "                      [-0.1926, -0.1841, -0.0838,  ...,  0.1571, -0.1602,  0.0861],\n",
       "                      [ 0.2056,  0.2230,  0.2103,  ..., -0.2013,  0.1449, -0.2250],\n",
       "                      ...,\n",
       "                      [ 0.1144,  0.0931,  0.1912,  ...,  0.0981,  0.1584, -0.1383],\n",
       "                      [-0.1531, -0.1743, -0.1192,  ..., -0.1710, -0.2076,  0.1500],\n",
       "                      [ 0.1455,  0.1297,  0.1041,  ..., -0.1222,  0.2138, -0.1068]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm2.gamma',\n",
       "              tensor([1.1523, 1.1523, 0.8476, 1.1521, 0.8477, 1.1523, 1.1523, 1.1523, 1.1520,\n",
       "                      0.8477, 1.1524, 1.1521, 0.8468, 1.1523, 0.8477, 0.8477, 0.8481, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1537, 1.1523, 1.1520,\n",
       "                      1.1524, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8476, 1.1523, 0.8477,\n",
       "                      0.8476, 1.1523, 0.8477, 1.1523, 0.8476, 0.8479, 0.8476, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8476, 0.8476, 1.1523, 0.8476, 1.1522, 1.1523, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1524, 1.1523, 1.1523, 0.8477, 0.8477, 0.8477, 1.1521,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8478, 1.1523, 1.1523, 0.8483, 1.1523, 1.1524,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8478, 0.8477, 0.8477, 0.8477, 0.8476, 0.8477,\n",
       "                      0.8476, 0.8477, 1.1523, 1.1524, 0.8477, 1.1523, 0.8477, 1.1523, 0.8478,\n",
       "                      0.8541, 1.1523, 0.8477, 0.8473, 0.8477, 0.8476, 1.1523, 1.1523, 0.8477,\n",
       "                      0.8476, 1.1523, 1.1525, 0.8477, 0.8477, 0.8481, 0.8477, 1.1523, 1.1524,\n",
       "                      1.1523, 1.1523, 0.8477, 0.8476, 0.8477, 0.8477, 1.1523, 1.1520, 1.1523,\n",
       "                      0.8477, 0.8476, 0.8477, 1.1523, 0.8477, 0.8486, 0.8477, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 1.1524,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1524, 1.1523, 0.8477, 0.8466, 0.8477, 1.1523,\n",
       "                      0.8475, 1.1523, 1.1510, 0.8477, 1.1523, 0.8476, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1523, 0.8477, 1.1523, 0.8477, 1.1523, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1522, 1.1523, 0.8476, 0.8478, 1.1524, 0.8479, 1.1523, 1.1523,\n",
       "                      0.8477, 1.1523, 1.1524, 0.8468, 1.1523, 1.1523, 0.8476, 0.8477, 0.8476,\n",
       "                      1.1523, 1.1523, 1.1523, 0.8476, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1527, 0.8477, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8476, 0.8477,\n",
       "                      0.8476, 1.1523, 1.1523, 0.8480, 1.1523, 1.1523, 0.8478, 0.8477, 1.1523,\n",
       "                      1.1524, 1.1524, 0.8476, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8476, 1.1523, 0.8476, 1.1523, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8477, 1.1523, 1.1523, 0.8475, 0.8477, 0.8477, 0.8478,\n",
       "                      0.8477, 0.8477, 1.1523, 1.1523, 1.1523, 0.8477, 0.8476, 0.8477, 0.8479,\n",
       "                      0.8477, 1.1523, 1.1523, 1.1523, 1.1524, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      1.1523, 1.1523, 1.1542, 0.8477, 1.1523, 0.8477, 0.8479, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8477, 0.8477, 0.8477, 1.1525, 0.8477, 0.8475, 1.1523, 1.1523,\n",
       "                      1.1523, 0.8479, 1.1524, 0.8477, 0.8477, 1.1524, 0.8477, 0.8476, 1.1523,\n",
       "                      0.8477, 0.8476, 0.8477, 1.1524, 0.8477, 0.8474, 1.1523, 0.8477, 1.1523,\n",
       "                      0.8477, 1.1523, 0.8477, 0.8477, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1526, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1524, 1.1524, 0.8477, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1523, 0.8473, 0.8477, 0.8477, 0.8477, 1.1520, 1.1524, 1.1523,\n",
       "                      1.1523, 1.1523, 0.8484, 1.1524, 1.1524, 0.8477, 1.1523, 0.8477, 0.8477,\n",
       "                      0.8477, 0.8477, 1.1524, 1.1523, 1.1523, 0.8477, 1.1523, 1.1522, 1.1524,\n",
       "                      1.1523, 1.1523, 1.1524, 0.8479, 0.8477, 0.8477, 1.1525, 1.1523, 1.1523,\n",
       "                      1.1523, 1.1524, 0.8477, 0.8477, 0.8477, 0.8477, 1.1522, 1.1523, 0.8477,\n",
       "                      1.1523, 0.8477, 0.8477, 1.1523, 0.8483, 0.8466, 0.8477, 0.8472, 0.8476,\n",
       "                      1.1523, 0.8476, 0.8477, 0.8477, 1.1523, 1.1524, 1.1523, 0.8476, 0.8477,\n",
       "                      1.1523, 1.1524, 0.8477, 0.8476, 1.1523, 0.8477, 0.8477, 0.8477, 1.1520,\n",
       "                      0.8481, 1.1523, 1.1523, 0.8477, 1.1523, 1.1524, 0.8477, 0.8477, 0.8476,\n",
       "                      1.1523, 1.1521, 0.8478, 1.1523, 0.8477, 1.1523, 0.8477, 0.8477, 0.8477,\n",
       "                      0.8477, 1.1524, 1.1523, 1.1523, 1.1523, 0.8477, 1.1523, 0.8476, 1.1526,\n",
       "                      1.1521, 1.1522, 0.8477, 1.1524, 0.8477, 1.1526, 0.8476, 1.1524, 1.1509,\n",
       "                      1.1523, 0.8476, 1.1523, 0.8476, 0.8478, 0.8477, 0.8477, 1.1523, 0.8477,\n",
       "                      0.8476, 1.1523, 0.8474, 1.1524, 0.8477, 1.1523, 1.1523, 0.8477, 1.1520,\n",
       "                      0.8477, 1.1527, 0.8477, 0.8476, 1.1524, 0.8477, 1.1523, 1.1522, 0.8477,\n",
       "                      1.1523, 0.8476, 0.8477, 1.1523, 1.1524, 0.8477, 0.8477, 1.1523, 1.1523,\n",
       "                      1.1524, 1.1523, 0.8478, 1.1523, 0.8477, 0.8477, 0.8477, 0.8477, 1.1525,\n",
       "                      1.1524, 0.8477, 0.8477, 1.1524, 1.1524, 1.1524, 0.8477, 1.1524, 0.8477,\n",
       "                      1.1524, 0.8477, 1.1523, 0.8477, 0.8477, 0.8476, 1.1527, 1.1523, 0.8477,\n",
       "                      1.1524, 1.1523, 0.8477, 0.8476, 1.1523, 1.1524, 0.8477, 0.8477, 1.1523,\n",
       "                      1.1524, 1.1523, 0.8477, 1.1524, 0.8481, 1.1523, 1.1523, 1.1522, 0.8477,\n",
       "                      0.8477, 1.1523, 1.1522, 1.1524, 1.1523, 1.1523, 1.1523, 1.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm2.beta',\n",
       "              tensor([-0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1515,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1524, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1524,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1517,  0.1523,\n",
       "                       0.1523, -0.1522, -0.1522,  0.1521, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1522,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1524, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1524,  0.1524, -0.1523,  0.1523, -0.1524,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1524, -0.1523,\n",
       "                      -0.1523, -0.1524,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1522,  0.1523,  0.1523, -0.1524,  0.1523, -0.1523,  0.1525,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1524, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                      -0.1524, -0.1523, -0.1523, -0.1524, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1519,  0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1524,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,  0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1524,  0.1524,  0.1524, -0.1522,  0.1523,  0.1521,\n",
       "                       0.1522,  0.1523, -0.1523, -0.1522,  0.1526,  0.1522,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1522, -0.1523, -0.1523,  0.1524,  0.1524,\n",
       "                       0.1523, -0.1523, -0.1529,  0.1523,  0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1524,  0.1521, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1524,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523, -0.1523, -0.1522,  0.1523,  0.1523,  0.1523,  0.1524,\n",
       "                       0.1524, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1522, -0.1523,  0.1520, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1524, -0.1523,  0.1523, -0.1523,  0.1523,  0.1524,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1524, -0.1523, -0.1523, -0.1523,  0.1524,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1520, -0.1523,  0.1523,\n",
       "                       0.1523,  0.1523,  0.1522,  0.1523,  0.1523, -0.1523,  0.1510,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1521,\n",
       "                       0.1523,  0.1524, -0.1522,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1524,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1523, -0.1524, -0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1524, -0.1523,\n",
       "                       0.1523, -0.1523,  0.1522, -0.1523,  0.1523,  0.1523, -0.1424, -0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523, -0.1523, -0.1523,\n",
       "                      -0.1524, -0.1523,  0.1524, -0.1523,  0.1523, -0.1525,  0.1523,  0.1523,\n",
       "                      -0.1524,  0.1523, -0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1525,  0.1523, -0.1523, -0.1523,  0.1523, -0.1524,\n",
       "                      -0.1523,  0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1524,  0.1524,\n",
       "                       0.1523,  0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523, -0.1523,  0.1523, -0.1523,  0.1523,\n",
       "                       0.1524,  0.1523, -0.1522, -0.1522,  0.1523,  0.1524, -0.1524, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523, -0.1523,  0.1523, -0.1524,  0.1524,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1523,  0.1523,  0.1524, -0.1522, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523, -0.1524,  0.1522,  0.1524, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1523, -0.1523,\n",
       "                      -0.1523, -0.1523,  0.1523,  0.1523,  0.1523, -0.1524, -0.1523, -0.1522,\n",
       "                       0.1520, -0.1523,  0.1523, -0.1523,  0.1523, -0.1524, -0.1523, -0.1523,\n",
       "                       0.1523,  0.1523, -0.1523,  0.1523,  0.1523, -0.1523, -0.1523,  0.1523,\n",
       "                      -0.1523,  0.1523,  0.1523,  0.1524,  0.1523, -0.1524,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1523, -0.1524, -0.1523, -0.1523,  0.1523, -0.1517,\n",
       "                       0.1526,  0.1523,  0.1523, -0.1524, -0.1523,  0.1524,  0.1523,  0.1523,\n",
       "                       0.1523, -0.1523,  0.1523, -0.1526, -0.1523,  0.1523,  0.1523,  0.1523,\n",
       "                       0.1523,  0.1523, -0.1524, -0.1524, -0.1523,  0.1524, -0.1523,  0.1523,\n",
       "                      -0.1523, -0.1523, -0.1523, -0.1524, -0.1523, -0.1523, -0.1522,  0.1523,\n",
       "                       0.1523, -0.1523, -0.1520, -0.1524, -0.1523, -0.1523,  0.1523, -0.1523],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.ffn.layer1.weight',\n",
       "              tensor([[ 0.1571,  0.1157,  0.1076,  ..., -0.1345,  0.1690, -0.1151],\n",
       "                      [-0.1564,  0.1754,  0.1138,  ..., -0.1574,  0.1966, -0.1054],\n",
       "                      [ 0.1674, -0.1932, -0.1496,  ...,  0.1917, -0.1884,  0.1292],\n",
       "                      ...,\n",
       "                      [-0.1866, -0.1248, -0.1902,  ..., -0.1666, -0.1234, -0.1556],\n",
       "                      [-0.1781,  0.1281,  0.1151,  ..., -0.1697,  0.1994, -0.1063],\n",
       "                      [-0.1383,  0.1938,  0.1618,  ..., -0.1693,  0.1194, -0.1520]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.ffn.layer1.bias',\n",
       "              tensor([ 0.1726,  0.1522, -0.1370,  ..., -0.1543,  0.1366,  0.1404],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.ffn.layer2.weight',\n",
       "              tensor([[-0.1167, -0.1611, -0.1106,  ..., -0.1291, -0.1523, -0.1735],\n",
       "                      [ 0.1194,  0.1279,  0.1187,  ...,  0.1229, -0.1438,  0.1212],\n",
       "                      [-0.1425, -0.1269, -0.1174,  ..., -0.1279, -0.1855, -0.1082],\n",
       "                      ...,\n",
       "                      [-0.1980, -0.1777, -0.1309,  ..., -0.1767,  0.1932, -0.1140],\n",
       "                      [ 0.1479,  0.1778,  0.1162,  ...,  0.1507,  0.1510,  0.1824],\n",
       "                      [-0.1310, -0.1883, -0.1883,  ..., -0.1896, -0.1167, -0.1683]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.ffn.layer2.bias',\n",
       "              tensor([-0.1545,  0.1541, -0.1734, -0.1311,  0.1357,  0.1352,  0.1568,  0.1695,\n",
       "                       0.1335, -0.1614, -0.1681, -0.1385, -0.1476,  0.1579,  0.1456, -0.1510,\n",
       "                      -0.1633, -0.1337, -0.1574, -0.1413, -0.1675, -0.1500, -0.1655,  0.1468,\n",
       "                      -0.1654,  0.1742, -0.1517, -0.1399, -0.1637,  0.1555, -0.1530,  0.1448,\n",
       "                       0.1586, -0.1641, -0.1614,  0.1376, -0.1476,  0.1440, -0.1640,  0.1570,\n",
       "                       0.1588,  0.1536, -0.1538,  0.1684,  0.1494, -0.1373, -0.1554,  0.1374,\n",
       "                       0.1559, -0.1626, -0.1434, -0.1559,  0.1732,  0.1646,  0.1318,  0.1383,\n",
       "                       0.1738, -0.1724,  0.1386,  0.1676, -0.1702, -0.1365, -0.1584, -0.1360,\n",
       "                       0.1652,  0.1715,  0.1517, -0.1414,  0.1688,  0.1663, -0.1404, -0.1358,\n",
       "                      -0.1570, -0.1588,  0.1477, -0.1650, -0.1644, -0.1560, -0.1573, -0.1588,\n",
       "                       0.1369,  0.1340,  0.1710,  0.1568, -0.1315,  0.1666, -0.1494,  0.1462,\n",
       "                       0.1349,  0.1675, -0.1659,  0.1662, -0.1417,  0.1482, -0.1741,  0.1322,\n",
       "                       0.1346, -0.1371,  0.1382, -0.1678,  0.1402,  0.1328,  0.1568, -0.1350,\n",
       "                      -0.1334, -0.1375, -0.1396, -0.1340, -0.1701,  0.1657, -0.1657,  0.1687,\n",
       "                      -0.1549,  0.1382,  0.1344, -0.1409,  0.1551, -0.1562,  0.1541,  0.1370,\n",
       "                       0.1431,  0.1363, -0.1588,  0.1655, -0.1306,  0.1350, -0.1308,  0.1736,\n",
       "                       0.1550,  0.1371, -0.1492,  0.1578,  0.1430,  0.1529,  0.1502, -0.1713,\n",
       "                       0.1360,  0.1331, -0.1685,  0.1337,  0.1372, -0.1702,  0.1499,  0.1358,\n",
       "                       0.1429, -0.1500, -0.1612,  0.1314, -0.1387,  0.1512,  0.1538, -0.1355,\n",
       "                      -0.1314, -0.1696, -0.1640, -0.1706, -0.1647, -0.1546,  0.1329, -0.1665,\n",
       "                       0.1434, -0.1349, -0.1642, -0.1685, -0.1518,  0.1317, -0.1324, -0.1646,\n",
       "                       0.1452,  0.1549, -0.1478,  0.1364,  0.1570,  0.1453,  0.1317,  0.1595,\n",
       "                       0.1593,  0.1488, -0.1354, -0.1484, -0.1379,  0.1743, -0.1597, -0.1499,\n",
       "                       0.1562,  0.1643,  0.1403,  0.1584, -0.1709, -0.1605, -0.1587, -0.1579,\n",
       "                      -0.1668, -0.1668,  0.1392,  0.1538, -0.1683, -0.1549,  0.1727,  0.1452,\n",
       "                       0.1547,  0.1429, -0.1320, -0.1535,  0.1437,  0.1656,  0.1675,  0.1408,\n",
       "                      -0.1619, -0.1719, -0.1543, -0.1690,  0.1680, -0.1319, -0.1605,  0.1693,\n",
       "                      -0.1614, -0.1571,  0.1577, -0.1618, -0.1597, -0.1522, -0.1650,  0.1320,\n",
       "                       0.1403,  0.1488,  0.1580,  0.1344,  0.1697,  0.1359,  0.1530,  0.1551,\n",
       "                       0.1362,  0.1363,  0.1436, -0.1621, -0.1449,  0.1629,  0.1493,  0.1337,\n",
       "                       0.1568, -0.1571, -0.1341, -0.1573, -0.1414,  0.1532, -0.1567,  0.1435,\n",
       "                       0.1613,  0.1481, -0.1591,  0.1383,  0.1414, -0.1313,  0.1500,  0.1447,\n",
       "                       0.1515, -0.1607, -0.1704,  0.1411,  0.1530, -0.1477, -0.1606,  0.1490,\n",
       "                       0.1485,  0.1626,  0.1662,  0.1537,  0.1688, -0.1371, -0.1394, -0.1417,\n",
       "                      -0.1518,  0.1683,  0.1317, -0.1592, -0.1355,  0.1702, -0.1622, -0.1338,\n",
       "                       0.1524, -0.1325, -0.1322,  0.1502,  0.1467,  0.1318, -0.1648,  0.1498,\n",
       "                      -0.1319, -0.1590,  0.1503, -0.1435,  0.1350, -0.1479, -0.1495, -0.1497,\n",
       "                      -0.1356, -0.1411,  0.1320, -0.1482, -0.1548, -0.1720,  0.1638, -0.1530,\n",
       "                      -0.1732,  0.1440,  0.1469,  0.1650,  0.1680, -0.1364,  0.1532,  0.1404,\n",
       "                       0.1737,  0.1515, -0.1372,  0.1414, -0.1596,  0.1589,  0.1348,  0.1698,\n",
       "                      -0.1447, -0.1528,  0.1587, -0.1545,  0.1733, -0.1588,  0.1442, -0.1577,\n",
       "                       0.1373, -0.1635, -0.1538, -0.1393,  0.1567,  0.1634, -0.1526, -0.1634,\n",
       "                       0.1654, -0.1331,  0.1549, -0.1665, -0.1603, -0.1723, -0.1542, -0.1572,\n",
       "                       0.1344, -0.1368,  0.1531, -0.1534,  0.1382, -0.1688,  0.1624, -0.1410,\n",
       "                      -0.1607,  0.1663,  0.1681,  0.1618, -0.1422, -0.1462, -0.1645,  0.1303,\n",
       "                       0.1336, -0.1507,  0.1354,  0.1629, -0.1519, -0.1656, -0.1310, -0.1654,\n",
       "                      -0.1680, -0.1453,  0.1333,  0.1738,  0.1679,  0.1724, -0.1710,  0.1428,\n",
       "                       0.1398, -0.1666,  0.1338, -0.1696, -0.1658, -0.1643,  0.1417, -0.1364,\n",
       "                      -0.1382, -0.1423,  0.1582, -0.1591, -0.1325,  0.1590, -0.1379, -0.1631,\n",
       "                       0.1602,  0.1559, -0.1692, -0.1469,  0.1615, -0.1532, -0.1418, -0.1730,\n",
       "                       0.1666,  0.1446, -0.1489, -0.1567, -0.1632,  0.1443, -0.1593, -0.1655,\n",
       "                       0.1595,  0.1317, -0.1683, -0.1582,  0.1545, -0.1689, -0.1516, -0.1490,\n",
       "                       0.1436, -0.1532,  0.1521, -0.1697,  0.1377,  0.1624, -0.1396, -0.1717,\n",
       "                      -0.1436, -0.1692,  0.1509,  0.1350,  0.1621,  0.1683, -0.1712, -0.1710,\n",
       "                      -0.1544, -0.1668, -0.1499,  0.1449,  0.1304, -0.1612, -0.1348,  0.1609,\n",
       "                      -0.1595, -0.1685,  0.1326, -0.1454,  0.1413, -0.1406, -0.1655, -0.1326,\n",
       "                       0.1635,  0.1488, -0.1555,  0.1428,  0.1516, -0.1446, -0.1384,  0.1361,\n",
       "                      -0.1596,  0.1304,  0.1716, -0.1531,  0.1623, -0.1399,  0.1705,  0.1566,\n",
       "                       0.1407,  0.1664, -0.1511,  0.1576, -0.1355, -0.1584,  0.1598, -0.1388,\n",
       "                       0.1688,  0.1565,  0.1344, -0.1729, -0.1430,  0.1458,  0.1659, -0.1600,\n",
       "                       0.1326, -0.1664,  0.1483, -0.1636, -0.1520,  0.1662,  0.1331,  0.1479,\n",
       "                       0.1344, -0.1308,  0.1429, -0.1485, -0.1624,  0.1685, -0.1346,  0.1650,\n",
       "                      -0.1577,  0.1341, -0.1617, -0.1622, -0.1643,  0.1527, -0.1574,  0.1719,\n",
       "                       0.1627, -0.1512, -0.1391, -0.1428,  0.1320, -0.1738,  0.1448, -0.1695],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm3.gamma',\n",
       "              tensor([0.8464, 0.8472, 0.8475, 0.8475, 0.8491, 1.1523, 0.8460, 1.1525, 1.1510,\n",
       "                      0.8477, 0.8476, 0.8473, 0.8418, 1.1519, 0.8478, 1.1560, 0.8481, 1.1471,\n",
       "                      1.1527, 1.1524, 0.8477, 0.8477, 1.1523, 0.8474, 1.1521, 1.1523, 0.8477,\n",
       "                      1.1520, 0.8475, 0.8477, 1.1534, 0.8473, 1.1521, 0.8453, 1.1523, 0.8472,\n",
       "                      0.8469, 1.1526, 0.8476, 1.1516, 0.8472, 1.1543, 0.8472, 0.8477, 1.1528,\n",
       "                      0.8474, 1.1525, 0.8469, 0.8483, 1.1538, 0.8489, 0.8469, 1.1524, 1.1712,\n",
       "                      0.8473, 0.8478, 0.8472, 1.1524, 1.1539, 0.8477, 1.1523, 0.8489, 1.1526,\n",
       "                      1.1522, 1.1528, 0.8476, 0.8472, 1.1525, 1.1535, 0.8502, 0.8472, 1.1526,\n",
       "                      1.1518, 1.1526, 1.1526, 0.8479, 0.8473, 1.1516, 0.8469, 0.8474, 0.8470,\n",
       "                      0.8474, 0.8471, 1.1520, 0.8484, 0.8475, 1.1522, 1.1531, 1.1524, 0.8470,\n",
       "                      0.8475, 0.8460, 0.8477, 1.1532, 0.8472, 1.1528, 1.1523, 1.1525, 0.8477,\n",
       "                      0.8473, 1.1528, 1.1526, 1.1523, 0.8472, 1.1537, 1.1526, 0.8471, 1.1532,\n",
       "                      1.1524, 1.1524, 1.1508, 0.8474, 0.8439, 1.1527, 0.8476, 1.1542, 0.8264,\n",
       "                      0.8476, 0.8466, 0.8470, 1.1492, 0.8474, 1.1533, 0.8478, 0.8472, 1.1525,\n",
       "                      0.8455, 0.8476, 1.1523, 0.8462, 1.1525, 0.8474, 1.1525, 0.8477, 1.1523,\n",
       "                      0.8476, 1.1524, 1.1524, 1.1530, 1.1529, 0.8477, 0.8472, 0.8475, 1.1516,\n",
       "                      0.8464, 0.8474, 1.1522, 0.8464, 0.8475, 0.8472, 0.8471, 0.8475, 0.8475,\n",
       "                      0.8193, 1.1521, 1.1524, 1.1526, 0.8478, 0.8402, 1.1521, 1.1530, 1.1522,\n",
       "                      1.1522, 0.8552, 1.1533, 0.8476, 1.1518, 1.1523, 1.1532, 1.1519, 1.1538,\n",
       "                      1.1544, 1.1523, 1.1525, 0.8480, 1.1526, 1.1516, 1.1526, 1.1524, 1.1525,\n",
       "                      1.1524, 1.1526, 1.1526, 0.8475, 1.1524, 0.8476, 0.8476, 1.1525, 1.1530,\n",
       "                      0.8476, 0.8525, 0.8474, 1.1522, 1.1524, 0.8471, 1.1522, 0.8472, 0.8473,\n",
       "                      0.8461, 1.1525, 0.8475, 1.1529, 1.1535, 0.8449, 0.8471, 1.1524, 1.1526,\n",
       "                      1.1529, 0.8474, 0.8472, 1.1526, 0.8476, 1.0469, 0.8473, 0.8465, 1.1525,\n",
       "                      1.1524, 0.8473, 0.8476, 1.1524, 1.1524, 1.1526, 0.8478, 1.1524, 1.1517,\n",
       "                      0.8472, 1.1525, 0.8477, 1.1526, 1.1530, 0.8473, 0.8477, 1.1524, 0.8479,\n",
       "                      1.1528, 0.8479, 0.8472, 1.1527, 1.1524, 0.8472, 0.8470, 0.8469, 1.1526,\n",
       "                      1.1529, 1.1524, 1.1523, 1.1521, 0.8472, 0.8476, 0.8468, 1.1530, 1.1512,\n",
       "                      1.1519, 1.1518, 0.8469, 0.8476, 1.1526, 0.8476, 1.1530, 1.1532, 1.1552,\n",
       "                      1.1522, 1.1524, 0.8468, 0.8452, 1.1551, 0.8538, 1.1504, 1.1531, 1.1529,\n",
       "                      1.1530, 1.1534, 1.1541, 1.1526, 0.8481, 1.1530, 0.8472, 0.8471, 1.1524,\n",
       "                      1.1530, 0.8475, 0.8510, 1.1536, 0.8475, 0.8474, 1.1525, 0.8476, 1.1526,\n",
       "                      1.1520, 1.1515, 0.8479, 0.8474, 1.1520, 0.8491, 1.1526, 0.8476, 0.8476,\n",
       "                      1.1555, 1.1523, 0.8475, 1.1514, 0.8474, 1.1525, 0.8477, 1.1523, 0.8474,\n",
       "                      0.8475, 0.8478, 0.8472, 1.1534, 0.8474, 0.8488, 1.1532, 0.8478, 1.1536,\n",
       "                      0.8474, 0.8475, 0.8470, 1.1500, 0.8476, 0.8476, 1.1541, 1.1532, 1.1526,\n",
       "                      1.1519, 1.1525, 1.1533, 0.8477, 0.8471, 0.8475, 0.8481, 0.8471, 0.8470,\n",
       "                      0.8476, 0.8474, 1.1527, 1.1519, 0.8475, 0.8476, 1.1521, 1.1525, 0.7984,\n",
       "                      1.1525, 1.1504, 0.8472, 0.8469, 0.8473, 0.8474, 1.1517, 1.1535, 1.1522,\n",
       "                      1.1525, 1.1527, 0.8475, 1.1522, 0.8476, 0.8471, 0.8478, 1.1527, 0.8475,\n",
       "                      1.1479, 0.8475, 1.1509, 1.1525, 1.1529, 0.8451, 1.1527, 0.8505, 0.8476,\n",
       "                      1.1525, 1.1471, 0.8470, 1.1525, 0.8488, 1.1714, 1.1525, 0.8472, 1.1523,\n",
       "                      1.1517, 0.8476, 1.1526, 1.1525, 1.1544, 0.8475, 0.8505, 0.8476, 1.1536,\n",
       "                      1.1520, 1.1526, 1.1529, 0.8474, 1.1512, 1.1527, 1.1559, 0.8472, 0.8477,\n",
       "                      1.1523, 0.8399, 0.8475, 1.1529, 0.8476, 1.1524, 1.1530, 0.8477, 0.8477,\n",
       "                      0.8473, 1.1529, 0.8469, 1.1525, 1.1524, 0.8478, 1.1541, 0.8474, 0.8449,\n",
       "                      0.8472, 1.1519, 0.8478, 1.1572, 0.8475, 1.1538, 0.8465, 1.1524, 1.1531,\n",
       "                      1.1512, 0.8475, 1.1519, 1.1528, 0.8479, 0.8475, 0.8479, 1.1522, 1.1527,\n",
       "                      0.8485, 0.8410, 1.1521, 1.1527, 1.1516, 1.1526, 1.1525, 1.1529, 0.8477,\n",
       "                      1.1525, 0.8481, 0.8475, 0.8470, 1.1525, 0.8476, 1.1521, 1.1522, 0.8476,\n",
       "                      1.1526, 0.8477, 0.8479, 1.1528, 0.8487, 0.8476, 0.8481, 1.1527, 1.1527,\n",
       "                      1.1523, 1.1525, 0.8480, 1.1525, 0.8475, 0.8475, 1.1543, 0.8474, 0.8473,\n",
       "                      1.1525, 0.8476, 0.8476, 0.8468, 1.1525, 1.1523, 0.8476, 1.1528, 0.8474,\n",
       "                      1.1533, 1.1526, 0.8476, 0.8474, 0.8472, 0.8447, 0.8492, 1.1525, 0.8470,\n",
       "                      1.1521, 0.8475, 0.8479, 1.1532, 0.8468, 1.1531, 0.8469, 0.8480, 1.1523,\n",
       "                      1.1544, 1.1524, 1.1521, 1.1526, 1.1521, 1.1514, 1.1536, 0.8475, 0.8475,\n",
       "                      0.8477, 1.1521, 0.8474, 1.1526, 0.8482, 1.1527, 1.1526, 1.1526],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.2.layer_norm3.beta',\n",
       "              tensor([-0.1519,  0.1520, -0.1522, -0.1523,  0.1527,  0.1523,  0.1519,  0.1526,\n",
       "                       0.1521, -0.1494, -0.1482, -0.1516, -0.1520,  0.1521,  0.1524, -0.1533,\n",
       "                      -0.1524, -0.1521, -0.1527, -0.1525, -0.1523, -0.1524, -0.1523,  0.1476,\n",
       "                      -0.1522,  0.1523, -0.1523, -0.1523, -0.1520,  0.1522, -0.1527,  0.1522,\n",
       "                       0.1523, -0.1514, -0.1524,  0.1503, -0.1517,  0.1526, -0.1522,  0.1521,\n",
       "                       0.1521,  0.1532, -0.1514,  0.1523,  0.1527, -0.1521, -0.1526,  0.1522,\n",
       "                       0.1524, -0.1526, -0.1533, -0.1521,  0.1524,  0.1534,  0.1514,  0.1524,\n",
       "                       0.1516, -0.1524,  0.1530, -0.1622, -0.1527, -0.1528, -0.1537, -0.1522,\n",
       "                       0.1531,  0.1523,  0.1520, -0.1526,  0.1536,  0.1527, -0.1509, -0.1526,\n",
       "                      -0.1521, -0.1530,  0.1524, -0.1524, -0.1519, -0.1523, -0.1520, -0.1521,\n",
       "                       0.1522,  0.1517,  0.1509,  0.1521, -0.1524,  0.1522, -0.1522,  0.1542,\n",
       "                       0.1524,  0.1506, -0.1522,  0.1522, -0.1523,  0.1525, -0.1518,  0.1524,\n",
       "                       0.1522, -0.1530,  0.1523, -0.1521,  0.1525,  0.1525,  0.1523, -0.1521,\n",
       "                      -0.1831, -0.1533, -0.1522, -0.1528, -0.1524,  0.1525, -0.1518,  0.1519,\n",
       "                      -0.1517,  0.1528,  0.1501, -0.1566,  0.1522, -0.1522,  0.1513,  0.1521,\n",
       "                       0.1522,  0.1520, -0.1527,  0.1524, -0.1523,  0.1526, -0.1518,  0.1514,\n",
       "                       0.1523,  0.1522, -0.1525,  0.1521,  0.1524,  0.1524,  0.1523, -0.1523,\n",
       "                       0.1525,  0.1524, -0.1527,  0.1545,  0.1524, -0.1520,  0.1520,  0.1520,\n",
       "                       0.1490, -0.1517, -0.1523,  0.1511, -0.1522,  0.1502,  0.1506, -0.1517,\n",
       "                      -0.1522, -0.1520, -0.1520, -0.1526, -0.1525, -0.1524,  0.1519, -0.1522,\n",
       "                       0.1530, -0.1523, -0.1523, -0.1556, -0.1525, -0.1536, -0.1522, -0.1524,\n",
       "                       0.1527,  0.1519, -0.1547,  0.1529,  0.1518,  0.1525,  0.1532,  0.1531,\n",
       "                       0.1510,  0.1525, -0.1525, -0.1590, -0.1524,  0.1526, -0.1525, -0.1522,\n",
       "                       0.1524,  0.1523,  0.1522,  0.1524, -0.1530, -0.1519, -0.1554, -0.1518,\n",
       "                      -0.1536, -0.1524,  0.1519,  0.1523, -0.1521, -0.1521,  0.1519,  0.1527,\n",
       "                       0.1521,  0.1524, -0.1527, -0.1518,  0.1522,  0.1523,  0.1527,  0.1531,\n",
       "                      -0.1521, -0.1520, -0.1529, -0.1522,  0.1520, -0.1520, -0.1521,  0.1524,\n",
       "                      -0.1524, -0.1519,  0.1521, -0.1523, -0.1523, -0.1525, -0.1525,  0.1524,\n",
       "                       0.1504,  0.1521,  0.1525, -0.1379,  0.1526,  0.1529,  0.1520,  0.1523,\n",
       "                       0.1525, -0.1506,  0.1527, -0.1525, -0.1511,  0.1526,  0.1524,  0.1520,\n",
       "                       0.1521, -0.1500, -0.1524, -0.1523, -0.1524,  0.1525, -0.1522,  0.1522,\n",
       "                       0.1523,  0.1523, -0.1526,  0.1522,  0.1523, -0.1523,  0.1515,  0.1522,\n",
       "                       0.1525, -0.1522, -0.1525,  0.1524,  0.1532, -0.1519, -0.1524,  0.1505,\n",
       "                       0.1519,  0.1529,  0.1541,  0.1469,  0.1526, -0.1529, -0.1530, -0.1524,\n",
       "                      -0.1527,  0.1524,  0.1525, -0.1528, -0.1522,  0.1522, -0.1526, -0.1527,\n",
       "                       0.1520, -0.1526, -0.1529,  0.1522,  0.1520,  0.1524, -0.1523,  0.1526,\n",
       "                      -0.1519, -0.1521,  0.1524, -0.1521,  0.1521, -0.1526, -0.1525, -0.1523,\n",
       "                      -0.1522, -0.1525,  0.1523, -0.1521, -0.1514, -0.1522,  0.1532, -0.1457,\n",
       "                      -0.1523,  0.1515,  0.1523,  0.1523,  0.1518, -0.1527,  0.1521,  0.1526,\n",
       "                       0.1524,  0.1524, -0.1524,  0.1521, -0.1516,  0.1519,  0.1491,  0.1523,\n",
       "                      -0.1521, -0.1543,  0.1524, -0.1526,  0.1521, -0.1526,  0.1526, -0.1524,\n",
       "                       0.1522, -0.1521, -0.1525, -0.1519,  0.1517,  0.1523,  0.1632, -0.1527,\n",
       "                      -0.1528, -0.1519,  0.1522, -0.1523, -0.1525, -0.1523, -0.1526, -0.1518,\n",
       "                       0.1519, -0.1515,  0.1520, -0.1470,  0.1520, -0.1530,  0.1522, -0.1524,\n",
       "                      -0.1528,  0.1521, -0.1545,  0.1523, -0.1519, -0.1524, -0.1528,  0.1522,\n",
       "                       0.1519, -0.1522,  0.1517,  0.1524, -0.1524, -0.1520, -0.1524, -0.1528,\n",
       "                      -0.1523, -0.1514,  0.1521, -0.1584,  0.1524,  0.1525, -0.1535,  0.1529,\n",
       "                       0.1520,  0.1526,  0.1518, -0.1523, -0.1524, -0.1526,  0.1526, -0.1522,\n",
       "                      -0.1526, -0.1523,  0.1524, -0.1522, -0.1526,  0.1525, -0.1522, -0.1512,\n",
       "                       0.1526,  0.1529, -0.1501, -0.1523,  0.1523, -0.1513, -0.1523, -0.1525,\n",
       "                       0.1523,  0.1524, -0.1524, -0.1523, -0.1523,  0.1522, -0.1535, -0.1513,\n",
       "                       0.1524,  0.1524, -0.1524, -0.1526,  0.1522, -0.1501, -0.1519, -0.1523,\n",
       "                       0.1539, -0.1536,  0.1523, -0.1528,  0.1499,  0.1525, -0.1524, -0.1523,\n",
       "                      -0.1517, -0.1521,  0.1527,  0.1524,  0.1507,  0.1524, -0.1523, -0.1524,\n",
       "                      -0.1524, -0.1522, -0.1519,  0.1524,  0.1521, -0.1527, -0.1526,  0.1527,\n",
       "                      -0.1522, -0.1524,  0.1524, -0.1522,  0.1517, -0.1525, -0.1520, -0.1523,\n",
       "                       0.1521,  0.1523, -0.1524, -0.0929,  0.1524, -0.1536, -0.1524,  0.1522,\n",
       "                      -0.1524,  0.1527,  0.1527, -0.1502,  0.1526, -0.1528,  0.1524,  0.1522,\n",
       "                       0.1521,  0.1525, -0.1520,  0.1521, -0.1525, -0.1523,  0.1522, -0.1512,\n",
       "                       0.1678,  0.1523,  0.1523, -0.1526, -0.1521,  0.1533,  0.1524, -0.1522,\n",
       "                       0.1522, -0.1522,  0.1513, -0.1527, -0.1532,  0.1520,  0.1522,  0.1522,\n",
       "                       0.1524, -0.1655,  0.1521, -0.1531, -0.1520,  0.1527, -0.1523,  0.1531,\n",
       "                      -0.1524,  0.1327, -0.1526, -0.1523, -0.1521,  0.1536, -0.1522,  0.1522,\n",
       "                       0.1523, -0.1520, -0.1517, -0.1525, -0.1648, -0.1527,  0.1530, -0.1526],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.self_attention.W_Q.weight',\n",
       "              tensor([[-0.1622,  0.1348, -0.1789,  ..., -0.2016,  0.1192, -0.1339],\n",
       "                      [-0.1004, -0.0922, -0.1808,  ...,  0.1277, -0.1857,  0.2140],\n",
       "                      [-0.1536,  0.1473,  0.1481,  ...,  0.1448, -0.1139,  0.2155],\n",
       "                      ...,\n",
       "                      [ 0.1954,  0.1026,  0.0865,  ...,  0.1160,  0.1992, -0.1027],\n",
       "                      [-0.1408, -0.1625,  0.1463,  ..., -0.1930,  0.1633, -0.1190],\n",
       "                      [-0.2059, -0.0782, -0.1203,  ...,  0.1729, -0.1558,  0.1134]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.self_attention.W_K.weight',\n",
       "              tensor([[-0.1715, -0.1331,  0.1307,  ...,  0.1521,  0.2080,  0.1949],\n",
       "                      [-0.1279,  0.0771, -0.1531,  ...,  0.2168, -0.1750,  0.0799],\n",
       "                      [-0.1667,  0.2193, -0.1212,  ..., -0.1700, -0.1805, -0.1584],\n",
       "                      ...,\n",
       "                      [-0.2130, -0.1836, -0.1768,  ..., -0.2008,  0.1850,  0.1900],\n",
       "                      [ 0.0972,  0.1539,  0.1845,  ...,  0.1905,  0.0791, -0.1880],\n",
       "                      [-0.1146,  0.1189,  0.1196,  ...,  0.1102, -0.1870,  0.1026]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.self_attention.W_V.weight',\n",
       "              tensor([[ 0.1824,  0.2112,  0.1078,  ..., -0.2188,  0.0781, -0.1847],\n",
       "                      [ 0.0958,  0.1948,  0.1069,  ..., -0.1496,  0.0825, -0.1555],\n",
       "                      [ 0.0880,  0.2069,  0.2046,  ..., -0.0884,  0.1014, -0.1493],\n",
       "                      ...,\n",
       "                      [-0.2076, -0.1234, -0.0930,  ...,  0.1521, -0.0917, -0.0989],\n",
       "                      [ 0.1215,  0.1948,  0.1505,  ..., -0.1729,  0.1151, -0.1543],\n",
       "                      [ 0.1243,  0.1131, -0.1304,  ..., -0.1642,  0.2161, -0.1047]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.self_attention.W_T.weight',\n",
       "              tensor([[-0.1928,  0.2246,  0.1740,  ...,  0.1551, -0.2101,  0.1758],\n",
       "                      [ 0.1753, -0.0831,  0.0798,  ..., -0.0965,  0.2039, -0.2217],\n",
       "                      [-0.1548, -0.1318, -0.1763,  ..., -0.1702,  0.1114, -0.0818],\n",
       "                      ...,\n",
       "                      [-0.1664,  0.1154,  0.1843,  ...,  0.1684, -0.1870,  0.0866],\n",
       "                      [ 0.1058,  0.2022, -0.0771,  ..., -0.1618, -0.1184,  0.1758],\n",
       "                      [-0.1723,  0.1163,  0.0965,  ..., -0.0745,  0.1101,  0.1917]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm1.gamma',\n",
       "              tensor([1.1527, 1.1525, 0.8477, 0.8475, 1.1525, 1.1525, 0.8323, 0.8486, 0.8469,\n",
       "                      0.8474, 1.1529, 0.8478, 0.8469, 1.1532, 0.8484, 1.1515, 1.1479, 1.1523,\n",
       "                      1.1524, 1.1533, 0.8476, 0.8475, 1.1522, 0.8478, 1.1522, 1.1559, 1.1523,\n",
       "                      0.8465, 1.1523, 1.1898, 1.1523, 0.8477, 0.8489, 0.8473, 1.1515, 0.8476,\n",
       "                      0.8476, 1.1518, 0.8475, 0.8475, 1.1518, 0.8479, 0.8477, 1.1477, 1.1523,\n",
       "                      0.8461, 1.1525, 0.8475, 1.1503, 1.1525, 0.8465, 0.8476, 1.1524, 0.8496,\n",
       "                      0.8474, 0.8477, 0.8458, 1.1535, 1.1525, 0.8468, 1.1518, 0.8492, 1.1527,\n",
       "                      1.1527, 1.1525, 1.1514, 1.1525, 1.1523, 1.1524, 0.8476, 0.8473, 1.1526,\n",
       "                      0.8477, 1.1523, 1.1810, 0.8476, 1.1523, 1.1522, 1.1523, 0.8475, 0.8404,\n",
       "                      0.8462, 0.8431, 1.1524, 1.1521, 0.8478, 1.1523, 0.8472, 1.1524, 1.1537,\n",
       "                      0.8475, 1.1519, 0.8476, 1.1529, 0.8474, 0.8476, 0.8478, 1.1529, 1.1516,\n",
       "                      0.8468, 0.8284, 1.1528, 0.8480, 0.8493, 1.1525, 0.8477, 1.1529, 1.1504,\n",
       "                      1.1533, 1.1525, 0.8475, 0.8477, 0.8473, 1.1529, 0.8478, 1.1531, 1.1526,\n",
       "                      0.8475, 0.8471, 0.8474, 0.8478, 0.8477, 0.8480, 0.8476, 0.8477, 0.8469,\n",
       "                      0.8474, 0.8477, 0.8464, 0.8476, 1.1525, 0.8478, 1.1522, 0.8472, 1.1531,\n",
       "                      0.8475, 1.1528, 1.1523, 1.1530, 1.1523, 0.8476, 0.8477, 0.8476, 0.8461,\n",
       "                      1.1505, 0.8475, 1.1523, 1.1559, 0.8474, 0.8449, 0.8477, 0.8477, 1.1522,\n",
       "                      1.1521, 1.1523, 0.8671, 1.1526, 0.8478, 1.1524, 1.1522, 0.8481, 1.1516,\n",
       "                      0.8477, 0.8470, 0.8476, 0.8440, 0.8470, 1.1524, 1.1525, 1.1522, 0.8470,\n",
       "                      1.1526, 0.8456, 1.1519, 1.1524, 1.1523, 0.8478, 1.1527, 1.1525, 1.1526,\n",
       "                      1.1532, 1.1526, 1.1523, 1.1537, 1.1523, 1.1514, 0.8476, 1.1513, 1.1523,\n",
       "                      0.8463, 0.8478, 0.8473, 0.9174, 1.1523, 1.1521, 0.8477, 1.1524, 0.8478,\n",
       "                      0.8474, 1.1523, 0.8476, 1.1532, 1.1621, 1.1513, 1.1522, 0.8478, 1.1523,\n",
       "                      1.2043, 0.8473, 1.1522, 1.1524, 0.8477, 1.1528, 0.8475, 1.1524, 0.8455,\n",
       "                      1.1525, 0.8471, 1.1523, 0.8474, 1.1522, 0.8476, 0.8477, 1.1484, 0.8473,\n",
       "                      0.8470, 1.1521, 1.1523, 1.1523, 1.1523, 1.1523, 0.8478, 1.1523, 0.8481,\n",
       "                      1.1559, 1.1529, 1.1507, 1.1521, 1.1576, 1.1506, 0.8478, 0.8477, 0.8477,\n",
       "                      1.1526, 1.1521, 1.1522, 0.8477, 1.1525, 0.8476, 0.8471, 1.1522, 1.1522,\n",
       "                      1.1565, 0.8473, 0.8479, 1.1525, 1.1524, 0.8477, 0.8473, 0.8478, 1.1526,\n",
       "                      0.8556, 1.1522, 1.1527, 0.8397, 1.1518, 0.8446, 0.8622, 0.8475, 0.8470,\n",
       "                      1.1524, 1.1512, 0.8482, 1.1541, 0.8476, 1.1515, 0.8486, 1.1533, 0.8477,\n",
       "                      1.1530, 0.8477, 0.8446, 1.1536, 1.1518, 1.1552, 0.8477, 0.8477, 1.1522,\n",
       "                      1.1497, 0.8557, 0.8495, 0.8483, 1.1523, 1.1523, 0.8477, 0.8476, 1.1580,\n",
       "                      0.8469, 0.8478, 0.8484, 0.8481, 0.8476, 1.1521, 0.8477, 1.1524, 1.1518,\n",
       "                      1.1520, 1.1523, 0.8483, 1.1527, 1.1496, 0.8472, 0.8489, 0.8477, 1.1527,\n",
       "                      0.8477, 0.8460, 1.1558, 0.8481, 0.8477, 0.8475, 1.1523, 0.8476, 1.1522,\n",
       "                      0.8360, 0.8487, 0.8477, 1.1522, 0.8471, 0.8479, 1.1538, 1.1526, 1.1523,\n",
       "                      1.1523, 1.1523, 1.1530, 0.8478, 0.8474, 0.8476, 0.8477, 1.1525, 1.1521,\n",
       "                      0.8473, 0.8467, 1.1516, 1.1517, 1.1526, 0.8504, 1.1522, 1.1490, 1.1523,\n",
       "                      1.1524, 1.1524, 0.8477, 1.1526, 0.8477, 1.1523, 1.1541, 0.8477, 0.8472,\n",
       "                      1.1521, 0.8477, 0.8477, 1.1523, 0.8475, 1.1519, 0.8474, 1.1529, 0.8487,\n",
       "                      1.1522, 0.8477, 1.1526, 0.8476, 0.8477, 0.8497, 1.1523, 0.8477, 1.1513,\n",
       "                      1.1523, 0.8487, 0.8473, 0.8481, 1.1526, 0.8477, 0.8478, 0.8475, 1.1467,\n",
       "                      0.8478, 1.1511, 1.1524, 1.1669, 1.1537, 1.1526, 1.1526, 1.1524, 0.8465,\n",
       "                      1.1511, 0.8493, 0.8478, 1.1524, 1.1520, 1.1524, 0.8361, 0.8471, 1.1528,\n",
       "                      0.8475, 0.8471, 0.8474, 1.1523, 0.8463, 0.8469, 0.8556, 0.8475, 0.8540,\n",
       "                      0.8477, 0.8476, 1.1491, 0.8629, 0.8476, 1.1528, 1.1522, 1.1525, 0.8476,\n",
       "                      1.1545, 1.1523, 0.8476, 1.1531, 0.8478, 0.8476, 1.1530, 0.8474, 0.8471,\n",
       "                      1.1525, 0.8473, 0.8477, 1.1528, 0.8468, 0.8480, 0.8477, 1.1528, 1.1498,\n",
       "                      1.1525, 0.8477, 0.8465, 0.8473, 1.1525, 0.8475, 1.1522, 1.1521, 0.8476,\n",
       "                      1.1532, 0.8476, 0.8476, 0.8472, 1.1511, 0.8476, 1.1524, 1.1524, 0.8483,\n",
       "                      1.1518, 1.1528, 0.8493, 0.8478, 0.8473, 0.8476, 0.8483, 1.1516, 0.8477,\n",
       "                      1.1525, 0.8476, 0.8475, 1.1519, 1.1509, 1.1524, 0.8476, 1.1526, 0.8472,\n",
       "                      1.1527, 1.1522, 1.1523, 0.8481, 1.1513, 0.8478, 1.1525, 1.1528, 0.8468,\n",
       "                      0.8475, 0.8479, 1.1523, 1.1526, 1.1524, 1.1526, 0.8475, 0.8493, 1.1523,\n",
       "                      0.8492, 1.1521, 1.1528, 1.1525, 1.1520, 1.1527, 1.1528, 0.8476, 0.8476,\n",
       "                      0.8472, 0.8478, 0.8475, 1.1523, 1.1524, 0.8477, 1.1615, 1.1495],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm1.beta',\n",
       "              tensor([ 0.1517,  0.1522, -0.1523, -0.1516,  0.1525,  0.1525,  0.1489, -0.1511,\n",
       "                       0.1522,  0.1521,  0.1543, -0.1524,  0.1533, -0.1550,  0.1526, -0.1520,\n",
       "                       0.1146, -0.1523, -0.1524, -0.1530, -0.1523, -0.1521, -0.1526, -0.1519,\n",
       "                      -0.1523,  0.1519, -0.1525, -0.1522,  0.1515, -0.1491, -0.1524,  0.1515,\n",
       "                       0.1633, -0.1522,  0.1524,  0.1512, -0.1522,  0.1530, -0.1520,  0.1521,\n",
       "                       0.1520,  0.1526, -0.1523, -0.2038,  0.1525, -0.1517, -0.1524,  0.1521,\n",
       "                       0.1520,  0.1526, -0.1548, -0.1521,  0.1524,  0.1562,  0.1520,  0.1523,\n",
       "                      -0.1579, -0.1525,  0.1525,  0.1538,  0.1523, -0.1545,  0.1525, -0.1522,\n",
       "                       0.1544,  0.1516,  0.1526, -0.1525,  0.1528, -0.1524, -0.1488, -0.1533,\n",
       "                       0.1527, -0.1524,  0.1529,  0.1525,  0.1527, -0.1527, -0.1523, -0.1521,\n",
       "                       0.1520,  0.1508, -0.1507,  0.1524, -0.1522, -0.1537, -0.1523, -0.1516,\n",
       "                       0.1524, -0.1520, -0.1522,  0.1524, -0.1521,  0.1529, -0.1513,  0.1525,\n",
       "                      -0.1472, -0.1526,  0.1529,  0.1518,  0.1522,  0.1525,  0.1524, -0.1519,\n",
       "                      -0.1520, -0.1504, -0.1533, -0.1566, -0.1525,  0.1525,  0.1523,  0.1521,\n",
       "                      -0.1521, -0.1526, -0.1526, -0.1535,  0.1521, -0.1516,  0.1515,  0.1522,\n",
       "                       0.1532,  0.1522,  0.1523,  0.1525, -0.1524, -0.1470, -0.1522,  0.1509,\n",
       "                       0.1520,  0.1522, -0.1521,  0.1521,  0.1524,  0.1522,  0.1526, -0.1520,\n",
       "                       0.1532,  0.1522, -0.1525,  0.1527,  0.1525, -0.1520,  0.1522,  0.1513,\n",
       "                      -0.1532, -0.1522, -0.1523, -0.1525, -0.1518, -0.1545, -0.1522, -0.1522,\n",
       "                       0.1526, -0.1522, -0.1523,  0.1515,  0.1512, -0.1505, -0.1529, -0.1525,\n",
       "                      -0.1519, -0.1522, -0.1523, -0.1564, -0.1521,  0.1522, -0.1522, -0.1543,\n",
       "                       0.1524, -0.1528,  0.1520,  0.1529,  0.1519,  0.1528,  0.1523,  0.1523,\n",
       "                      -0.1531,  0.1525, -0.1524, -0.1535, -0.1524,  0.1524, -0.1524,  0.1525,\n",
       "                       0.1521,  0.1521,  0.1523,  0.1521, -0.1524,  0.1519,  0.1519,  0.1527,\n",
       "                      -0.1445, -0.1524, -0.1549,  0.1523, -0.1525, -0.1520,  0.1521,  0.1526,\n",
       "                       0.1522,  0.1529, -0.1546, -0.1517,  0.1521,  0.1524,  0.1522, -0.1517,\n",
       "                      -0.1521, -0.1523, -0.1525, -0.1523,  0.1524, -0.1520, -0.1526, -0.1534,\n",
       "                      -0.1517, -0.1519, -0.1523, -0.1522, -0.1522, -0.1568, -0.1526,  0.1521,\n",
       "                       0.1521,  0.1521,  0.1523,  0.1523,  0.1523,  0.1524, -0.1523,  0.1524,\n",
       "                       0.1524, -0.1517,  0.1521, -0.1530, -0.1534,  0.1521,  0.1524,  0.1410,\n",
       "                       0.1522, -0.1523, -0.1523, -0.1526, -0.1520, -0.1536,  0.1526,  0.1527,\n",
       "                       0.1522,  0.1521, -0.1529,  0.1542,  0.1519, -0.1520,  0.1490,  0.1488,\n",
       "                       0.1524, -0.1523, -0.1522,  0.1524,  0.1521,  0.1527, -0.1530, -0.1527,\n",
       "                       0.1527,  0.1530,  0.1546, -0.1534,  0.1521,  0.1519, -0.1526, -0.1528,\n",
       "                       0.1524,  0.1524,  0.1522, -0.1529, -0.1526,  0.1524,  0.1523, -0.1537,\n",
       "                       0.1524,  0.1511, -0.1527,  0.1516, -0.1498, -0.1523, -0.1524,  0.1526,\n",
       "                      -0.1524, -0.1524,  0.1525, -0.1526,  0.1523, -0.1525,  0.1532, -0.1521,\n",
       "                      -0.1526, -0.1520,  0.1522, -0.1518,  0.1525, -0.1523, -0.1524,  0.1523,\n",
       "                      -0.1524, -0.1527,  0.1528,  0.1523,  0.1514, -0.1525,  0.1522,  0.1522,\n",
       "                      -0.1521,  0.1527, -0.1528,  0.1521, -0.1519, -0.1543,  0.1525,  0.1522,\n",
       "                      -0.1522,  0.1523,  0.1526,  0.1549,  0.1535,  0.1477, -0.1512,  0.1524,\n",
       "                       0.1521, -0.1527, -0.1525, -0.1570, -0.1541,  0.1525,  0.1524, -0.1526,\n",
       "                       0.1522, -0.1512,  0.1523, -0.1523, -0.1524, -0.1528,  0.1516, -0.1516,\n",
       "                      -0.1526, -0.1519,  0.1525,  0.1529, -0.1544, -0.1552,  0.1523, -0.1520,\n",
       "                      -0.1525,  0.1523,  0.1512,  0.1523, -0.1524, -0.1663,  0.1521, -0.1523,\n",
       "                       0.1524, -0.1523, -0.1524,  0.1522, -0.1522, -0.1522, -0.1530, -0.1525,\n",
       "                      -0.1524,  0.1523,  0.1522,  0.1513, -0.1522, -0.1523, -0.1525,  0.1519,\n",
       "                       0.1521,  0.1555, -0.1523, -0.1524, -0.1522, -0.1524,  0.1525, -0.1522,\n",
       "                      -0.1525, -0.1522, -0.1530,  0.1526,  0.1715,  0.1525,  0.1522,  0.1519,\n",
       "                       0.1524,  0.1529,  0.1537,  0.1562,  0.1522, -0.1506,  0.1522, -0.1524,\n",
       "                       0.1522,  0.1523,  0.1527, -0.1518, -0.0412,  0.1522,  0.1515, -0.1520,\n",
       "                       0.1523,  0.1395, -0.1530,  0.1519,  0.1521, -0.1546, -0.1524, -0.1523,\n",
       "                       0.1615, -0.1556,  0.1523, -0.1526, -0.1526,  0.1524, -0.1523, -0.1530,\n",
       "                       0.1536, -0.1523,  0.1528, -0.1522,  0.1521,  0.1528, -0.1522, -0.1522,\n",
       "                      -0.1524, -0.1522,  0.1524,  0.1527,  0.1522,  0.1526,  0.1518,  0.1526,\n",
       "                       0.1490, -0.1524,  0.1524, -0.1521,  0.1520, -0.1524, -0.1522, -0.1516,\n",
       "                      -0.1539,  0.1523, -0.1527, -0.1522,  0.1521,  0.1517, -0.1523,  0.1523,\n",
       "                      -0.1525,  0.1525, -0.1511,  0.1528,  0.1525, -0.1536,  0.1523,  0.1525,\n",
       "                       0.1522,  0.1529,  0.1570,  0.1524, -0.1525, -0.1522,  0.1523,  0.1525,\n",
       "                      -0.0998,  0.1525,  0.1524, -0.1526, -0.1520,  0.1529,  0.1527,  0.1524,\n",
       "                       0.1535, -0.1522,  0.1521,  0.1522, -0.1519,  0.1505, -0.1524,  0.1525,\n",
       "                       0.1525,  0.1513, -0.1517, -0.1526, -0.1521, -0.1453, -0.1524, -0.1510,\n",
       "                      -0.1522, -0.1582, -0.1525, -0.1523, -0.1525,  0.1527, -0.1517,  0.1522,\n",
       "                       0.1521,  0.1522, -0.1520, -0.1524,  0.1526,  0.1523,  0.1548, -0.1604],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[-0.1323,  0.1416, -0.0975,  ...,  0.0773, -0.2163,  0.1909],\n",
       "                      [ 0.1616, -0.1101,  0.1044,  ..., -0.1007,  0.2022, -0.1633],\n",
       "                      [-0.0961,  0.0811, -0.1566,  ...,  0.1063, -0.1929,  0.1497],\n",
       "                      ...,\n",
       "                      [-0.1105, -0.1379,  0.1426,  ..., -0.1891, -0.1102,  0.1974],\n",
       "                      [-0.1939,  0.1168,  0.0840,  ...,  0.2121, -0.0810, -0.0961],\n",
       "                      [-0.0968,  0.1722, -0.0869,  ...,  0.1959, -0.1891,  0.1835]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.enc_dec_attention.W_K.weight',\n",
       "              tensor([[-0.1130,  0.1698, -0.1374,  ...,  0.1457, -0.1772, -0.1442],\n",
       "                      [ 0.1737, -0.1003,  0.1524,  ..., -0.1526,  0.1743,  0.0762],\n",
       "                      [-0.2149, -0.2019, -0.1869,  ...,  0.1057, -0.1631, -0.2257],\n",
       "                      ...,\n",
       "                      [-0.2030, -0.0866, -0.0805,  ..., -0.2025, -0.1276,  0.2239],\n",
       "                      [ 0.1667,  0.1105,  0.0856,  ...,  0.2055,  0.2257,  0.1266],\n",
       "                      [ 0.1936, -0.1196, -0.1188,  ...,  0.0795, -0.1789, -0.1845]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.enc_dec_attention.W_V.weight',\n",
       "              tensor([[ 0.0774,  0.2027,  0.1471,  ..., -0.2328, -0.1893, -0.1467],\n",
       "                      [-0.0848, -0.1820, -0.2201,  ..., -0.2083,  0.0892, -0.1803],\n",
       "                      [ 0.1395,  0.1674,  0.1607,  ..., -0.1443, -0.1339, -0.1401],\n",
       "                      ...,\n",
       "                      [-0.2101, -0.0926, -0.0832,  ..., -0.0713,  0.1265, -0.1058],\n",
       "                      [-0.1078, -0.2132, -0.0993,  ..., -0.1675,  0.0931,  0.0806],\n",
       "                      [ 0.0776,  0.1708,  0.1026,  ...,  0.1031, -0.2080,  0.2158]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.enc_dec_attention.W_T.weight',\n",
       "              tensor([[ 0.2236,  0.1754, -0.1123,  ..., -0.1064,  0.1483,  0.1615],\n",
       "                      [ 0.1887, -0.0938, -0.1866,  ..., -0.1926, -0.1538,  0.2063],\n",
       "                      [-0.0823, -0.2170,  0.1271,  ...,  0.1437,  0.1027, -0.1082],\n",
       "                      ...,\n",
       "                      [ 0.1897,  0.0768, -0.1808,  ..., -0.1977, -0.1202,  0.2198],\n",
       "                      [-0.2277,  0.1937,  0.1977,  ..., -0.1656, -0.2167,  0.1219],\n",
       "                      [ 0.1134,  0.1953, -0.0729,  ..., -0.0991,  0.1090,  0.1804]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm2.gamma',\n",
       "              tensor([1.1516, 0.8489, 0.8475, 0.8473, 1.1526, 1.1527, 0.8488, 0.8457, 0.8470,\n",
       "                      0.8470, 1.1559, 0.8477, 0.8541, 1.1541, 1.1527, 0.8463, 1.1351, 0.8476,\n",
       "                      1.1525, 1.1553, 0.8476, 0.8473, 1.1527, 0.8476, 1.1520, 0.8461, 1.1526,\n",
       "                      1.1521, 0.8451, 1.1886, 1.1524, 0.8483, 1.1432, 0.8473, 1.1524, 0.8474,\n",
       "                      0.8477, 1.1522, 0.8463, 0.8475, 0.8453, 0.8504, 0.8477, 1.1307, 1.1528,\n",
       "                      0.8425, 1.1525, 0.8473, 0.8477, 0.8485, 0.8487, 0.8465, 1.1528, 0.7976,\n",
       "                      0.8475, 0.8484, 1.1519, 1.1561, 1.1528, 1.1488, 1.1522, 0.8503, 1.1690,\n",
       "                      0.8479, 1.1566, 0.8425, 1.1528, 1.1525, 1.1522, 1.1528, 1.2009, 1.1527,\n",
       "                      0.8485, 1.1523, 1.1538, 0.8475, 1.1545, 1.1531, 0.8476, 0.8473, 0.8471,\n",
       "                      0.8473, 0.8293, 1.1525, 0.8432, 0.8483, 1.1521, 0.8462, 1.1525, 1.1523,\n",
       "                      0.8468, 0.8483, 0.8474, 1.1598, 0.8466, 1.1507, 0.8448, 1.1531, 1.1612,\n",
       "                      0.8446, 1.1514, 0.8484, 1.1533, 1.1474, 0.8460, 0.8477, 0.8524, 1.1540,\n",
       "                      1.1529, 1.1529, 0.8469, 0.8475, 0.8473, 1.1515, 1.1542, 1.1536, 0.8474,\n",
       "                      0.8473, 0.8470, 0.8474, 1.1984, 0.8473, 1.1523, 1.1524, 0.8479, 0.8460,\n",
       "                      0.8467, 0.8463, 0.8474, 0.8475, 0.8046, 0.8473, 0.8477, 0.8472, 1.1604,\n",
       "                      0.8478, 1.1522, 1.1526, 1.1536, 1.1524, 1.1556, 0.8473, 0.8477, 0.8474,\n",
       "                      1.1514, 0.8474, 1.1522, 1.1524, 1.1510, 0.8441, 0.8476, 0.8475, 0.8500,\n",
       "                      0.8468, 1.1522, 0.8494, 1.1518, 0.8487, 1.1545, 1.1525, 0.8472, 0.8458,\n",
       "                      1.1523, 0.8366, 0.8474, 1.1516, 0.8475, 1.1537, 1.1526, 1.1532, 0.8467,\n",
       "                      1.1546, 1.1493, 1.1518, 1.1524, 1.1523, 1.1475, 1.1529, 1.1527, 1.1594,\n",
       "                      1.1528, 1.1532, 1.1524, 1.1529, 1.1516, 0.8470, 0.8476, 1.1514, 1.1523,\n",
       "                      0.8473, 0.8476, 1.1531, 1.1502, 1.1524, 0.8465, 1.1522, 1.1568, 0.8482,\n",
       "                      0.8472, 1.1523, 0.8478, 1.1536, 1.0167, 0.8314, 0.8456, 1.1522, 1.1505,\n",
       "                      0.8515, 0.8468, 1.1522, 1.1545, 0.8477, 1.1524, 0.8475, 1.1523, 1.1513,\n",
       "                      1.1518, 0.8310, 1.1524, 0.8472, 1.1491, 0.8463, 0.8470, 0.8464, 0.8469,\n",
       "                      0.8476, 0.8478, 1.1522, 1.1523, 1.1524, 0.8475, 1.1528, 1.1525, 0.8474,\n",
       "                      0.8482, 1.1534, 0.8492, 0.8472, 0.8477, 1.1623, 0.8470, 0.8483, 1.1523,\n",
       "                      1.1546, 1.1507, 1.1523, 0.8495, 1.1528, 0.8476, 0.8474, 1.1505, 1.1506,\n",
       "                      0.8472, 0.8471, 1.1514, 1.1522, 1.1524, 0.8477, 0.8473, 1.1536, 0.8476,\n",
       "                      1.1535, 1.1530, 1.1531, 1.1532, 1.1533, 1.1499, 1.1504, 1.1468, 0.8469,\n",
       "                      1.1536, 1.1738, 1.1525, 1.1526, 0.8476, 1.1518, 1.1532, 1.1524, 0.8478,\n",
       "                      1.1589, 1.1525, 0.8473, 1.1533, 1.1454, 0.8495, 1.1524, 0.8533, 1.1530,\n",
       "                      1.1525, 0.8483, 1.1567, 1.1522, 1.1522, 1.1538, 1.1544, 0.8470, 1.1530,\n",
       "                      0.8469, 1.1523, 0.8489, 1.1544, 0.8473, 1.1526, 1.1523, 1.1523, 0.8502,\n",
       "                      1.1533, 1.1523, 1.1542, 1.1596, 0.8476, 1.1504, 0.8478, 1.1518, 1.1528,\n",
       "                      0.8475, 1.1397, 1.1523, 1.1535, 0.8477, 0.8475, 1.1523, 1.1523, 1.1504,\n",
       "                      1.1480, 0.8464, 0.8456, 1.1524, 0.8477, 1.1791, 1.1524, 1.1533, 1.1521,\n",
       "                      1.1585, 1.1523, 1.1531, 0.8477, 0.8463, 0.8475, 1.1523, 0.8484, 1.1535,\n",
       "                      0.8469, 0.8462, 1.1522, 0.8470, 1.1523, 1.1532, 1.1477, 1.1558, 0.8472,\n",
       "                      0.8462, 1.1533, 0.8478, 0.8484, 1.1510, 1.1526, 1.2039, 0.8475, 1.1520,\n",
       "                      1.1522, 1.1525, 1.1524, 0.8477, 0.8474, 0.8474, 1.1533, 0.8496, 0.8477,\n",
       "                      0.8474, 0.8478, 0.8465, 0.8475, 0.8476, 1.1528, 1.1531, 0.8471, 1.1497,\n",
       "                      1.1523, 1.1542, 1.1518, 0.8507, 1.1528, 0.8476, 0.8481, 0.8474, 1.1532,\n",
       "                      0.8474, 1.1519, 1.1524, 0.8478, 1.1525, 1.1530, 1.1667, 1.1518, 0.9340,\n",
       "                      0.8452, 1.1734, 0.8476, 1.1523, 0.8455, 1.1524, 0.8476, 0.8068, 1.1496,\n",
       "                      0.8471, 0.8462, 0.8468, 0.8478, 0.8432, 0.8500, 0.8486, 0.8473, 0.8665,\n",
       "                      0.8477, 0.8477, 1.1413, 0.8522, 0.8476, 1.1540, 1.1519, 1.1529, 0.8471,\n",
       "                      1.1542, 1.1518, 0.8476, 1.1531, 0.8477, 0.8469, 1.1566, 0.8472, 1.1544,\n",
       "                      0.8476, 1.1529, 0.8477, 1.1535, 0.8469, 1.1550, 0.8477, 0.8470, 1.1514,\n",
       "                      1.1525, 1.1524, 1.1509, 1.1508, 1.1530, 0.8472, 0.8366, 1.1522, 0.8419,\n",
       "                      1.1978, 0.8470, 0.8474, 0.8408, 1.1468, 0.8478, 1.1528, 1.1525, 0.8469,\n",
       "                      0.8517, 1.1524, 1.1558, 1.1524, 1.1523, 0.8476, 1.1530, 1.1476, 0.8478,\n",
       "                      1.1525, 0.8475, 1.1519, 1.1538, 1.1588, 1.1527, 0.8477, 1.1528, 0.8460,\n",
       "                      1.1536, 1.1531, 0.8484, 0.8478, 0.8475, 0.8477, 1.1523, 0.8479, 0.8476,\n",
       "                      0.8497, 0.8493, 1.1537, 0.8425, 1.1512, 1.1527, 0.8472, 0.8478, 1.1523,\n",
       "                      0.8484, 1.1500, 1.1530, 1.1526, 0.8415, 1.1527, 1.1539, 0.8469, 0.8472,\n",
       "                      0.8467, 0.8476, 0.8474, 1.1527, 1.1524, 0.8475, 1.1528, 0.8178],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm2.beta',\n",
       "              tensor([ 0.1509,  0.1529, -0.1523, -0.1509,  0.1526,  0.1527,  0.1460, -0.1479,\n",
       "                       0.1520,  0.1519,  0.1552, -0.1524,  0.1543, -0.1574,  0.1529, -0.1516,\n",
       "                       0.1015, -0.1523, -0.1525, -0.1535, -0.1522, -0.1518, -0.1528, -0.1515,\n",
       "                      -0.1522,  0.1515, -0.1526, -0.1522,  0.1506, -0.1454, -0.1524,  0.1507,\n",
       "                       0.1633, -0.1520,  0.1525,  0.1505, -0.1521,  0.1535, -0.1516,  0.1518,\n",
       "                       0.1517,  0.1529, -0.1523, -0.1746,  0.1527, -0.1510, -0.1525,  0.1518,\n",
       "                       0.1516,  0.1527, -0.1599, -0.1519,  0.1525,  0.1572,  0.1517,  0.1524,\n",
       "                      -0.1705, -0.1526,  0.1527,  0.1552,  0.1522, -0.1582,  0.1526, -0.1521,\n",
       "                       0.1550,  0.1506,  0.1528, -0.1526,  0.1531, -0.1526, -0.1448, -0.1549,\n",
       "                       0.1531, -0.1524,  0.1534,  0.1526,  0.1530, -0.1531, -0.1523, -0.1519,\n",
       "                       0.1517,  0.1498, -0.1482,  0.1524, -0.1520, -0.1631, -0.1521, -0.1507,\n",
       "                       0.1525, -0.1517, -0.1520,  0.1527, -0.1519,  0.1535, -0.1505,  0.1531,\n",
       "                      -0.1333, -0.1529,  0.1536,  0.1513,  0.1521,  0.1527,  0.1525, -0.1515,\n",
       "                      -0.1516, -0.1542, -0.1547, -0.1576, -0.1527,  0.1526,  0.1522,  0.1519,\n",
       "                      -0.1518, -0.1529, -0.1530, -0.1547,  0.1518, -0.1507,  0.1508,  0.1521,\n",
       "                       0.1543,  0.1520,  0.1523,  0.1528, -0.1525, -0.1410, -0.1519,  0.1497,\n",
       "                       0.1517,  0.1520, -0.1518,  0.1520,  0.1525,  0.1522,  0.1528, -0.1515,\n",
       "                       0.1544,  0.1518, -0.1526,  0.1530,  0.1527, -0.1516,  0.1520,  0.1518,\n",
       "                      -0.1542, -0.1521, -0.1523, -0.1527, -0.1513, -0.1587, -0.1521, -0.1520,\n",
       "                       0.1528, -0.1519, -0.1523,  0.1506,  0.1498, -0.1469, -0.1534, -0.1527,\n",
       "                      -0.1513, -0.1520, -0.1523, -0.1628, -0.1517,  0.1521, -0.1521, -0.1618,\n",
       "                       0.1525, -0.1533,  0.1516,  0.1534,  0.1518,  0.1533,  0.1524,  0.1523,\n",
       "                      -0.1542,  0.1527, -0.1524, -0.1550, -0.1525,  0.1525, -0.1524,  0.1528,\n",
       "                       0.1518,  0.1518,  0.1523,  0.1517, -0.1525,  0.1515,  0.1515,  0.1531,\n",
       "                      -0.1414, -0.1525,  0.1475,  0.1523, -0.1527, -0.1518,  0.1520,  0.1529,\n",
       "                       0.1521,  0.1533, -0.1560, -0.1507,  0.1519,  0.1524,  0.1520, -0.1509,\n",
       "                      -0.1518, -0.1521, -0.1526, -0.1523,  0.1524, -0.1517, -0.1530, -0.1543,\n",
       "                      -0.1507, -0.1514, -0.1522, -0.1520, -0.1521, -0.1722, -0.1532,  0.1517,\n",
       "                       0.1520,  0.1519,  0.1522,  0.1521,  0.1523,  0.1525, -0.1521,  0.1525,\n",
       "                       0.1525, -0.1508,  0.1519, -0.1537, -0.1544,  0.1519,  0.1525,  0.1346,\n",
       "                       0.1519, -0.1524, -0.1523, -0.1529, -0.1517, -0.1547,  0.1529,  0.1530,\n",
       "                       0.1522,  0.1519, -0.1535,  0.1565,  0.1514, -0.1516,  0.1472,  0.1474,\n",
       "                       0.1523, -0.1523, -0.1521,  0.1526,  0.1517,  0.1530, -0.1537, -0.1530,\n",
       "                       0.1530,  0.1537,  0.1565, -0.1546,  0.1519,  0.1518, -0.1529, -0.1535,\n",
       "                       0.1525,  0.1525,  0.1521, -0.1534, -0.1529,  0.1524,  0.1523, -0.1547,\n",
       "                       0.1525,  0.1498, -0.1531,  0.1509, -0.1427, -0.1524, -0.1526,  0.1529,\n",
       "                      -0.1525, -0.1525,  0.1528, -0.1528,  0.1521, -0.1527,  0.1547, -0.1518,\n",
       "                      -0.1529, -0.1516,  0.1521, -0.1513,  0.1527, -0.1522, -0.1525,  0.1522,\n",
       "                      -0.1525, -0.1529,  0.1533,  0.1523,  0.1512, -0.1527,  0.1521,  0.1521,\n",
       "                      -0.1519,  0.1531, -0.1533,  0.1519, -0.1515, -0.1557,  0.1527,  0.1522,\n",
       "                      -0.1520,  0.1523,  0.1529,  0.1586,  0.1545,  0.1437, -0.1499,  0.1524,\n",
       "                       0.1518, -0.1532, -0.1528, -0.1597, -0.1554,  0.1526,  0.1524, -0.1528,\n",
       "                       0.1520, -0.1501,  0.1522, -0.1523, -0.1525, -0.1532,  0.1510, -0.1509,\n",
       "                      -0.1530, -0.1514,  0.1527,  0.1533, -0.1558, -0.1570,  0.1522, -0.1516,\n",
       "                      -0.1527,  0.1523,  0.1498,  0.1523, -0.1525,  0.1090,  0.1519, -0.1521,\n",
       "                       0.1526, -0.1524, -0.1525,  0.1520, -0.1521, -0.1521, -0.1533, -0.1527,\n",
       "                      -0.1525,  0.1522,  0.1521,  0.1500, -0.1520, -0.1522, -0.1528,  0.1508,\n",
       "                       0.1519,  0.1577, -0.1523, -0.1525, -0.1521, -0.1526,  0.1526, -0.1521,\n",
       "                      -0.1526, -0.1520, -0.1539,  0.1529,  0.1591,  0.1526,  0.1521,  0.1517,\n",
       "                       0.1525,  0.1535,  0.1545,  0.1587,  0.1521, -0.1468,  0.1520, -0.1524,\n",
       "                       0.1519,  0.1523,  0.1533, -0.1513,  0.1428,  0.1522,  0.1506, -0.1516,\n",
       "                       0.1523, -0.1644, -0.1539,  0.1515,  0.1518, -0.1566, -0.1524, -0.1523,\n",
       "                       0.0360,  0.1489,  0.1522, -0.1529, -0.1529,  0.1526, -0.1522, -0.1536,\n",
       "                       0.1549, -0.1522,  0.1532, -0.1520,  0.1519,  0.1533, -0.1520, -0.1520,\n",
       "                      -0.1522, -0.1520,  0.1525,  0.1531,  0.1520,  0.1529,  0.1513,  0.1529,\n",
       "                       0.1463, -0.1525,  0.1524, -0.1518,  0.1516, -0.1525, -0.1519, -0.1506,\n",
       "                      -0.1551,  0.1523, -0.1531, -0.1522,  0.1519,  0.1510, -0.1521,  0.1523,\n",
       "                      -0.1527,  0.1527, -0.1499,  0.1534,  0.1528, -0.1545,  0.1523,  0.1526,\n",
       "                       0.1521,  0.1534, -0.1457,  0.1525, -0.1526, -0.1520,  0.1522,  0.1525,\n",
       "                      -0.0872,  0.1526,  0.1526, -0.1529, -0.1517,  0.1535,  0.1530,  0.1524,\n",
       "                       0.1539, -0.1521,  0.1518,  0.1521, -0.1514,  0.1485, -0.1525,  0.1527,\n",
       "                       0.1526,  0.1499, -0.1508, -0.1528, -0.1519,  0.1783, -0.1525, -0.1484,\n",
       "                      -0.1519,  0.1490, -0.1526, -0.1522, -0.1527,  0.1531, -0.1510,  0.1522,\n",
       "                       0.1519,  0.1525, -0.1517, -0.1525,  0.1528,  0.1522,  0.1584, -0.1712],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.ffn.layer1.weight',\n",
       "              tensor([[-0.1110, -0.1781,  0.1243,  ..., -0.1808,  0.1329, -0.1816],\n",
       "                      [ 0.1107,  0.1278, -0.1575,  ...,  0.1054, -0.1309,  0.1243],\n",
       "                      [ 0.0259,  0.0437,  0.0037,  ...,  0.0333, -0.0022, -0.0413],\n",
       "                      ...,\n",
       "                      [ 0.1967, -0.1285,  0.1677,  ..., -0.1655,  0.1128, -0.1678],\n",
       "                      [-0.1396, -0.1237,  0.1115,  ..., -0.1489,  0.1919, -0.1052],\n",
       "                      [ 0.1503,  0.1113, -0.1097,  ..., -0.1189, -0.1370,  0.1056]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.ffn.layer1.bias',\n",
       "              tensor([ 0.1480, -0.1790, -0.0390,  ...,  0.1387,  0.1149, -0.1857],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.ffn.layer2.weight',\n",
       "              tensor([[ 0.1048,  0.1232, -0.0230,  ...,  0.1508,  0.1859,  0.1191],\n",
       "                      [-0.1025,  0.1073, -0.0033,  ...,  0.1054,  0.2022,  0.1368],\n",
       "                      [-0.1460, -0.1498, -0.0231,  ..., -0.1661,  0.1302, -0.1415],\n",
       "                      ...,\n",
       "                      [ 0.1513,  0.1566,  0.0195,  ..., -0.1718,  0.1899,  0.1152],\n",
       "                      [ 0.1819, -0.1197, -0.0460,  ..., -0.1182, -0.1133, -0.1169],\n",
       "                      [-0.1465,  0.1984, -0.0094,  ..., -0.1377, -0.1167,  0.1401]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.ffn.layer2.bias',\n",
       "              tensor([ 0.1357,  0.1731, -0.1529, -0.1514,  0.1690,  0.1597,  0.1718, -0.1468,\n",
       "                       0.1696,  0.1589,  0.1370, -0.1580, -0.1626, -0.1590,  0.1604,  0.1737,\n",
       "                       0.1690,  0.1427, -0.1633, -0.1662, -0.1522, -0.1439, -0.1385, -0.1508,\n",
       "                      -0.1529,  0.1534, -0.1680, -0.1491, -0.1733,  0.1485, -0.1717,  0.1463,\n",
       "                       0.1406, -0.1452,  0.1711,  0.1461, -0.1644,  0.1549, -0.1664,  0.1344,\n",
       "                       0.1418,  0.1659, -0.1408,  0.1348,  0.1415, -0.1533, -0.1557,  0.1324,\n",
       "                       0.1314,  0.1336, -0.1508, -0.1368,  0.1732,  0.1577, -0.1385,  0.1623,\n",
       "                       0.1727,  0.1305,  0.1727, -0.1349, -0.1384, -0.1694,  0.1473, -0.1670,\n",
       "                       0.1626,  0.1454,  0.1597, -0.1577, -0.1658, -0.1359,  0.1547, -0.1308,\n",
       "                      -0.1438, -0.1675,  0.1559, -0.1515,  0.1577, -0.1704, -0.1662, -0.1558,\n",
       "                       0.1514,  0.1527, -0.1591,  0.1346, -0.1611, -0.1622, -0.1695, -0.1629,\n",
       "                      -0.1349, -0.1469, -0.1459,  0.1532, -0.1405,  0.1438, -0.1436, -0.1338,\n",
       "                      -0.1567, -0.1399,  0.1646,  0.1454,  0.1374,  0.1675,  0.1516, -0.1651,\n",
       "                      -0.1746, -0.1605, -0.1351, -0.1642, -0.1514,  0.1635,  0.1530,  0.1517,\n",
       "                      -0.1581, -0.1365, -0.1309, -0.1682,  0.1496, -0.1325,  0.1511,  0.1376,\n",
       "                       0.1709,  0.1398,  0.1663,  0.1444, -0.1580,  0.1446, -0.1318, -0.1522,\n",
       "                       0.1638,  0.1472,  0.1718,  0.1576,  0.1430,  0.1330,  0.1383, -0.1522,\n",
       "                       0.1686, -0.1748, -0.1687,  0.1568,  0.1384, -0.1641,  0.1690,  0.1394,\n",
       "                      -0.1369, -0.1591, -0.1306, -0.1488, -0.1446, -0.1436, -0.1467, -0.1383,\n",
       "                       0.1615, -0.1372, -0.1548, -0.1637, -0.1603,  0.1521, -0.1347, -0.1508,\n",
       "                      -0.1347, -0.1450, -0.1744, -0.1424,  0.1444,  0.1420, -0.1612, -0.1460,\n",
       "                       0.1652, -0.1332,  0.1326,  0.1464,  0.1743,  0.1531,  0.1634, -0.1624,\n",
       "                       0.1532,  0.1321,  0.1412, -0.1456, -0.1695,  0.1337, -0.1517,  0.1670,\n",
       "                       0.1482,  0.1462,  0.1383,  0.1376, -0.1405,  0.1527,  0.1357,  0.1488,\n",
       "                       0.1419, -0.1522, -0.1492,  0.1536, -0.1566, -0.1549,  0.1717,  0.1658,\n",
       "                       0.1344,  0.1579, -0.1436, -0.1701,  0.1551,  0.1695,  0.1368, -0.1500,\n",
       "                      -0.1518, -0.1313, -0.1541, -0.1335,  0.1455, -0.1483, -0.1638, -0.1542,\n",
       "                       0.1542, -0.1615, -0.1669, -0.1372, -0.1528, -0.1503, -0.1608,  0.1558,\n",
       "                       0.1405,  0.1696,  0.1533,  0.1592,  0.1482,  0.1506, -0.1350,  0.1426,\n",
       "                       0.1566, -0.1708, -0.1297, -0.1587,  0.1553,  0.1607,  0.1741,  0.1498,\n",
       "                       0.1479, -0.1637, -0.1586, -0.1705, -0.1615, -0.1687, -0.1723,  0.1494,\n",
       "                       0.1667,  0.1555, -0.1324,  0.1523,  0.1675, -0.1553,  0.1469,  0.1725,\n",
       "                       0.1634, -0.1628, -0.1729,  0.1608,  0.1356,  0.1600, -0.1554, -0.1351,\n",
       "                      -0.1614,  0.1355,  0.1383, -0.1370,  0.1321,  0.1282, -0.1423, -0.1574,\n",
       "                       0.1618,  0.1597,  0.1321, -0.1421, -0.1529,  0.1693,  0.1571, -0.1581,\n",
       "                       0.1416,  0.1632, -0.1637,  0.1399, -0.1633, -0.1633, -0.1636,  0.1369,\n",
       "                      -0.1607, -0.1469,  0.1374, -0.1505,  0.1379, -0.1597, -0.1529, -0.1377,\n",
       "                      -0.1328, -0.1587,  0.1477, -0.1328,  0.1612,  0.1679,  0.1388,  0.1416,\n",
       "                      -0.1514, -0.1373,  0.1499,  0.1344,  0.1329, -0.1668,  0.1537,  0.1380,\n",
       "                      -0.1448,  0.1355, -0.1427,  0.1650, -0.1372, -0.1644,  0.1356,  0.1367,\n",
       "                      -0.1530,  0.1322,  0.1322, -0.1699,  0.1362,  0.1481,  0.1705,  0.1582,\n",
       "                       0.1719, -0.1350, -0.1687,  0.1580, -0.1639,  0.1699,  0.1666, -0.1694,\n",
       "                       0.1401, -0.1440,  0.1589, -0.1711,  0.1303, -0.1441, -0.1312,  0.1528,\n",
       "                      -0.1691,  0.1569,  0.1619, -0.1723,  0.1303, -0.1529, -0.1429,  0.1233,\n",
       "                      -0.1551,  0.1651,  0.1571,  0.1728, -0.1591, -0.1694,  0.1739,  0.1422,\n",
       "                      -0.1597, -0.1618, -0.1602,  0.1593, -0.1459, -0.1532, -0.1373, -0.1344,\n",
       "                      -0.1582,  0.1685,  0.1361,  0.1351, -0.1309, -0.1505, -0.1718, -0.1512,\n",
       "                       0.1317,  0.1678, -0.1657, -0.1520, -0.1544,  0.1723,  0.1652, -0.1620,\n",
       "                      -0.1504, -0.1323,  0.1467,  0.1621, -0.1324,  0.1485,  0.1662, -0.1605,\n",
       "                       0.1546,  0.1702,  0.1493, -0.1650,  0.1449,  0.1337,  0.1440, -0.1376,\n",
       "                       0.1625,  0.1376,  0.1365,  0.1460,  0.1539,  0.1306,  0.1507,  0.1559,\n",
       "                       0.1432, -0.1710, -0.1475, -0.1375,  0.1666,  0.1411, -0.1702, -0.1479,\n",
       "                       0.1369,  0.1516,  0.1689,  0.1441, -0.1519,  0.1570, -0.1616,  0.1438,\n",
       "                       0.1325, -0.1342,  0.1328, -0.1701,  0.1326,  0.1634, -0.1371, -0.1611,\n",
       "                      -0.1504, -0.1471,  0.1480,  0.1440,  0.1586,  0.1631,  0.1724,  0.1681,\n",
       "                       0.1321, -0.1470,  0.1498, -0.1558,  0.1736, -0.1712, -0.1550, -0.1466,\n",
       "                       0.1552,  0.1425, -0.1706, -0.1689,  0.1694,  0.1469, -0.1347,  0.1366,\n",
       "                      -0.1431, -0.1497, -0.1646, -0.1617,  0.1628, -0.1658,  0.1515,  0.1409,\n",
       "                       0.1462,  0.1659,  0.1602,  0.1602, -0.1351, -0.1382,  0.1339, -0.1372,\n",
       "                      -0.1573,  0.1544,  0.1638, -0.1433, -0.1499,  0.1476,  0.1428,  0.1435,\n",
       "                       0.1706, -0.1510,  0.1323,  0.1495,  0.1645,  0.1432, -0.1593,  0.1652,\n",
       "                       0.1655,  0.1398,  0.1615, -0.1567, -0.1602,  0.1517, -0.1737, -0.1552,\n",
       "                      -0.1339,  0.1434, -0.1450, -0.1552, -0.1730, -0.1525, -0.1672,  0.1547,\n",
       "                       0.1665,  0.1550, -0.1363, -0.1312, -0.1347,  0.1536, -0.1666, -0.1394],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm3.gamma',\n",
       "              tensor([1.1476, 0.9372, 1.2017, 0.8370, 1.1572, 1.1568, 0.8266, 1.1472, 0.8304,\n",
       "                      0.8435, 1.1709, 0.8615, 0.8418, 1.1743, 1.1761, 1.1548, 0.8140, 0.8481,\n",
       "                      1.1541, 0.9750, 0.8383, 0.8384, 1.1575, 1.0182, 1.1415, 1.1395, 1.1516,\n",
       "                      1.1462, 0.9040, 0.8477, 1.1589, 0.8454, 1.1682, 0.8271, 1.1563, 0.8447,\n",
       "                      0.8849, 1.1588, 1.1355, 0.8432, 1.1460, 0.8569, 0.8579, 0.8449, 1.1534,\n",
       "                      0.8365, 1.1634, 0.8417, 0.8463, 0.8507, 1.1612, 0.8438, 1.1608, 0.8700,\n",
       "                      1.1516, 0.8554, 0.8240, 1.1492, 1.1468, 1.0930, 1.1655, 0.8658, 1.1161,\n",
       "                      0.8460, 1.1606, 0.8268, 1.1545, 1.1539, 0.8367, 0.8234, 1.1972, 0.8539,\n",
       "                      1.0479, 1.1547, 0.8645, 1.0928, 1.1158, 1.1551, 1.1680, 0.8343, 1.1854,\n",
       "                      0.8514, 0.8488, 1.1392, 0.8542, 0.8403, 1.1445, 0.8095, 0.8585, 0.8126,\n",
       "                      1.1438, 0.8581, 0.8392, 1.0088, 0.8506, 0.8412, 0.8456, 1.1642, 1.0080,\n",
       "                      0.8253, 0.8351, 1.1956, 1.1532, 0.8230, 0.8596, 0.8425, 1.1322, 1.1545,\n",
       "                      1.1569, 1.1562, 0.8308, 0.8846, 0.8424, 1.1760, 0.8567, 1.1598, 1.1521,\n",
       "                      0.8407, 0.8197, 0.8357, 0.7975, 0.8442, 1.1514, 1.1411, 0.8422, 0.8556,\n",
       "                      1.1520, 1.1703, 0.8473, 0.8411, 0.8995, 0.8419, 0.8540, 1.1605, 1.1745,\n",
       "                      0.8456, 1.1552, 1.1814, 1.1524, 1.1581, 0.8587, 0.8444, 0.8459, 1.1511,\n",
       "                      1.1673, 0.8460, 1.1820, 1.1356, 0.8192, 1.1428, 0.8543, 0.8495, 1.1625,\n",
       "                      0.8439, 0.8427, 1.1557, 1.1584, 0.8148, 1.1549, 1.1545, 0.8448, 1.1484,\n",
       "                      1.1823, 0.8470, 1.1594, 1.1465, 0.8425, 1.1600, 1.1570, 1.1539, 1.0345,\n",
       "                      1.1848, 1.1470, 1.1953, 1.1638, 0.8436, 0.8365, 1.1556, 0.8467, 1.1883,\n",
       "                      1.1503, 1.1551, 1.1439, 1.1589, 0.8392, 0.8498, 0.8432, 1.1360, 1.1517,\n",
       "                      0.8385, 0.8525, 1.1530, 1.1714, 1.1550, 1.1522, 1.1599, 0.8477, 0.8682,\n",
       "                      0.8460, 1.1589, 0.8254, 1.2035, 0.8523, 0.8326, 0.8237, 0.8428, 0.8462,\n",
       "                      0.8362, 0.8472, 0.8286, 1.1620, 0.8487, 1.1692, 0.8428, 1.1535, 1.1601,\n",
       "                      1.1560, 1.1401, 1.1597, 0.8337, 1.1544, 0.9303, 0.8480, 0.8251, 1.1297,\n",
       "                      0.8425, 1.0482, 1.1500, 1.1477, 1.1530, 0.8501, 1.1582, 1.1518, 0.8480,\n",
       "                      1.0772, 1.1829, 0.8409, 0.8396, 0.9765, 0.8442, 1.1123, 1.1602, 1.1464,\n",
       "                      1.1750, 1.1158, 1.1577, 0.8446, 1.1712, 0.8422, 0.8465, 1.1672, 1.1743,\n",
       "                      0.8493, 0.8401, 0.8380, 1.1364, 1.1526, 0.8458, 0.8415, 0.8432, 0.8358,\n",
       "                      1.1563, 1.1613, 1.1591, 1.1532, 1.1662, 1.1772, 0.8636, 0.8221, 1.1645,\n",
       "                      1.2040, 1.1512, 1.1527, 0.9315, 0.8629, 1.1646, 1.0807, 1.1464, 0.8473,\n",
       "                      1.1140, 0.8473, 0.7961, 0.8155, 0.8155, 0.8375, 1.1489, 0.8300, 0.8407,\n",
       "                      1.1478, 0.8461, 1.1729, 1.1549, 1.1535, 1.0969, 0.8345, 0.8312, 1.1572,\n",
       "                      0.8396, 1.1458, 0.8336, 1.1611, 0.8272, 0.8348, 0.8469, 1.1559, 0.8583,\n",
       "                      1.1563, 1.1485, 1.1720, 0.8678, 0.8458, 0.8473, 0.8389, 1.1889, 1.1562,\n",
       "                      0.8439, 0.8403, 1.1588, 0.8286, 1.1600, 0.8446, 1.1251, 1.1584, 1.1513,\n",
       "                      1.1525, 0.8376, 0.8415, 1.1539, 0.8548, 0.9668, 1.1523, 0.7994, 1.1574,\n",
       "                      1.1660, 1.1477, 1.1594, 0.8411, 1.0512, 0.8453, 0.8420, 0.8438, 1.1808,\n",
       "                      1.1465, 1.1697, 1.1607, 0.8456, 0.8063, 0.8425, 0.8489, 1.1565, 1.1529,\n",
       "                      1.1519, 1.1583, 0.8467, 1.0025, 0.8429, 1.1521, 0.8542, 0.8466, 1.1508,\n",
       "                      1.1412, 1.1536, 0.8388, 0.8488, 0.8475, 0.8433, 0.8597, 0.8467, 1.2031,\n",
       "                      0.8436, 0.9250, 0.8346, 0.8441, 0.8582, 1.1759, 1.1208, 0.8474, 0.8568,\n",
       "                      1.1590, 1.1589, 1.1387, 0.8486, 1.1555, 0.8430, 0.8522, 0.8423, 0.8503,\n",
       "                      0.8521, 0.8476, 1.1905, 0.8525, 1.1591, 1.1546, 1.1639, 1.1531, 0.8425,\n",
       "                      0.8389, 0.8339, 0.8549, 1.1501, 0.8356, 1.1470, 0.8458, 1.1623, 0.8574,\n",
       "                      0.8468, 0.8459, 1.1508, 1.1738, 0.9515, 0.8481, 1.1334, 0.8447, 0.8476,\n",
       "                      0.8713, 0.8490, 1.1603, 1.1833, 0.8485, 1.1400, 1.1575, 1.1531, 1.1541,\n",
       "                      0.8440, 1.1721, 0.8468, 1.1566, 0.9994, 0.8447, 0.9918, 1.1376, 1.1451,\n",
       "                      0.8411, 1.1376, 0.8872, 1.1607, 1.1088, 1.1597, 0.8240, 1.1573, 0.7960,\n",
       "                      1.1409, 1.1572, 1.1406, 1.1436, 1.1570, 0.8442, 0.8394, 1.1078, 0.8591,\n",
       "                      1.1840, 0.8446, 0.8407, 1.1541, 0.8349, 0.8498, 1.1601, 0.8468, 0.8291,\n",
       "                      0.7964, 1.1547, 1.1396, 1.0311, 1.1536, 0.8503, 1.1603, 1.1752, 1.1646,\n",
       "                      1.1548, 0.8456, 0.9355, 0.8483, 0.9744, 1.1549, 1.2037, 1.1552, 0.7999,\n",
       "                      1.1736, 1.1818, 1.1489, 0.8526, 0.8468, 0.8467, 1.1529, 1.1543, 0.8164,\n",
       "                      1.1685, 1.1597, 0.8747, 0.7979, 1.1543, 1.1615, 0.8343, 0.8664, 1.1546,\n",
       "                      0.8486, 0.8414, 0.8878, 1.1546, 1.0312, 1.1581, 0.8456, 0.8377, 0.8609,\n",
       "                      0.8409, 0.7966, 0.8384, 1.1544, 0.8246, 0.8439, 0.8769, 1.1536],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.3.layer_norm3.beta',\n",
       "              tensor([ 0.1507,  0.1885, -0.1551, -0.1378,  0.1578,  0.1624,  0.1441, -0.1482,\n",
       "                       0.1425,  0.1486,  0.1993, -0.1582, -0.1441, -0.1729,  0.1575,  0.1616,\n",
       "                       0.1011,  0.0429, -0.1549, -0.1674, -0.1514, -0.1389, -0.1560, -0.1145,\n",
       "                      -0.1477,  0.1498, -0.1518, -0.1498, -0.1549, -0.1506, -0.1557,  0.1500,\n",
       "                       0.1771, -0.1372,  0.1553,  0.1497, -0.1575,  0.1733, -0.1245,  0.1483,\n",
       "                       0.1508,  0.1536, -0.1575,  0.1440,  0.1533, -0.1475, -0.1639,  0.1461,\n",
       "                       0.1508,  0.1549, -0.1566, -0.1503,  0.1563,  0.1570, -0.1414,  0.1549,\n",
       "                       0.1388,  0.1516,  0.1472, -0.0803, -0.0496, -0.1568,  0.1490, -0.1516,\n",
       "                       0.1822,  0.1251,  0.1533, -0.1544, -0.1297, -0.1362,  0.1690, -0.1565,\n",
       "                      -0.1295, -0.1536,  0.1873, -0.1306,  0.1496, -0.1608, -0.1570, -0.1428,\n",
       "                       0.1631,  0.1570, -0.1532,  0.1472, -0.1566, -0.1464, -0.1496, -0.1293,\n",
       "                      -0.1575, -0.1432, -0.1515,  0.1597, -0.1460,  0.2043, -0.1610, -0.1454,\n",
       "                      -0.1516, -0.1609,  0.1583,  0.1447,  0.1481,  0.1662,  0.1526, -0.1452,\n",
       "                      -0.1620, -0.1466, -0.1433, -0.1558, -0.1572,  0.1557,  0.1447,  0.1692,\n",
       "                      -0.1510, -0.1589, -0.1552, -0.1662, -0.1640, -0.1332,  0.1400,  0.1360,\n",
       "                       0.1496,  0.1486,  0.1514,  0.1429, -0.1448,  0.2020, -0.1519, -0.1866,\n",
       "                       0.1514,  0.1451,  0.1844,  0.1465,  0.1558,  0.1571,  0.1575, -0.1410,\n",
       "                       0.1593, -0.1717, -0.1521,  0.1598,  0.1542, -0.1487,  0.1490,  0.1507,\n",
       "                      -0.1804, -0.1512, -0.1548, -0.1380, -0.1251, -0.1481, -0.1667, -0.1537,\n",
       "                       0.1538, -0.1492, -0.1513, -0.1599, -0.1602,  0.1409, -0.1539, -0.1557,\n",
       "                      -0.1504, -0.1516, -0.1565, -0.1512,  0.1693,  0.1486, -0.1332, -0.1985,\n",
       "                       0.1550, -0.1529,  0.1428,  0.1624,  0.1510,  0.1576,  0.1556, -0.1331,\n",
       "                       0.1426,  0.1537, -0.1804, -0.1539, -0.1518,  0.1541, -0.1411,  0.1581,\n",
       "                       0.1496,  0.1545,  0.1494,  0.0921, -0.1520,  0.1338,  0.1558,  0.1533,\n",
       "                      -0.0110, -0.1565, -0.1558,  0.1573, -0.1523, -0.1608,  0.1503,  0.1589,\n",
       "                       0.1459,  0.1639, -0.1546, -0.1428,  0.1432,  0.1503,  0.1510, -0.1425,\n",
       "                      -0.1520, -0.1492, -0.1614, -0.1530,  0.1596, -0.1470, -0.1547, -0.1610,\n",
       "                       0.1575, -0.1462, -0.1594, -0.1490, -0.1533, -0.1592, -0.1529,  0.1441,\n",
       "                       0.1473,  0.1441,  0.1475,  0.1438,  0.1517,  0.1531, -0.1541,  0.1542,\n",
       "                       0.1518, -0.1554, -0.0970, -0.1723,  0.1506,  0.1455,  0.1608,  0.1475,\n",
       "                       0.1443, -0.1567, -0.1499, -0.1599, -0.1405, -0.1625,  0.1921,  0.1662,\n",
       "                       0.1503,  0.1514, -0.1720,  0.1602,  0.1531, -0.1438,  0.1495,  0.1421,\n",
       "                       0.1525, -0.1483, -0.1492,  0.1491,  0.1419,  0.1606, -0.1792, -0.1603,\n",
       "                      -0.1670,  0.1603,  0.1700, -0.1638,  0.1465,  0.0435, -0.1747, -0.1509,\n",
       "                       0.0181,  0.1559,  0.1558, -0.1638, -0.1467,  0.1487,  0.1521, -0.1381,\n",
       "                       0.1523, -0.2018, -0.1277,  0.1508, -0.1356, -0.1512, -0.1491,  0.1497,\n",
       "                      -0.1500, -0.1508,  0.1541, -0.1544,  0.1528, -0.1487, -0.1414, -0.1367,\n",
       "                      -0.1568, -0.1378,  0.1459, -0.1421,  0.1603,  0.1434,  0.0315,  0.1514,\n",
       "                      -0.1755, -0.1695,  0.1619,  0.1519,  0.1634, -0.1562,  0.1503,  0.1518,\n",
       "                      -0.1494,  0.1642, -0.1585,  0.1448, -0.1485,  0.0923,  0.1470,  0.1565,\n",
       "                      -0.1475,  0.1455,  0.1555, -0.1503,  0.1532,  0.1244, -0.0303,  0.1549,\n",
       "                       0.1606, -0.1594, -0.1519,  0.1473, -0.1770,  0.1569,  0.1477, -0.1559,\n",
       "                       0.1389, -0.0906,  0.1486, -0.1505,  0.1238, -0.1533, -0.1468,  0.2038,\n",
       "                      -0.1599,  0.1520,  0.1463, -0.1419,  0.1526, -0.1566, -0.1634,  0.1230,\n",
       "                      -0.1549,  0.1515,  0.1468,  0.1508, -0.1520,  0.1340,  0.1500, -0.1924,\n",
       "                      -0.1261, -0.1532, -0.1448,  0.1528, -0.1521, -0.1431, -0.1573, -0.1517,\n",
       "                      -0.1579,  0.1479,  0.1796,  0.1162, -0.1508, -0.1556, -0.1619, -0.0986,\n",
       "                       0.1521,  0.1987, -0.1759, -0.1527, -0.1484,  0.1753,  0.1544, -0.1495,\n",
       "                      -0.1587, -0.1485,  0.1546,  0.1544, -0.1522,  0.1576,  0.1564, -0.1684,\n",
       "                       0.1535,  0.1591,  0.1543, -0.1459,  0.1403,  0.1482,  0.1590, -0.1504,\n",
       "                       0.1456,  0.1492,  0.1511,  0.1759,  0.1554,  0.1519,  0.1487, -0.0137,\n",
       "                       0.1561, -0.1807, -0.1525, -0.1371,  0.1494,  0.1514, -0.1638, -0.1527,\n",
       "                       0.1610,  0.1568,  0.1530,  0.0056, -0.1596,  0.1524, -0.1535,  0.1465,\n",
       "                       0.1730, -0.1515,  0.1550, -0.1518,  0.1502,  0.1575, -0.1485, -0.1508,\n",
       "                      -0.1454, -0.1491,  0.1619,  0.1664,  0.1413,  0.1577,  0.1217,  0.1953,\n",
       "                       0.1180, -0.1478,  0.1559, -0.1484,  0.1515, -0.1571, -0.1493, -0.1457,\n",
       "                       0.1443,  0.1532, -0.1583, -0.1334,  0.1472,  0.1563, -0.1428,  0.1527,\n",
       "                      -0.1566,  0.1673, -0.1395, -0.1457,  0.1577, -0.1462,  0.1876,  0.1530,\n",
       "                       0.1711,  0.1632,  0.1565,  0.1590, -0.1565, -0.1502,  0.1556, -0.0575,\n",
       "                      -0.0991,  0.1543,  0.0126, -0.1555, -0.1480,  0.1652,  0.1577,  0.1496,\n",
       "                       0.0280, -0.1512,  0.1513,  0.1534,  0.1650,  0.1304, -0.1579,  0.1548,\n",
       "                       0.1671,  0.0565,  0.1598, -0.1660, -0.1452,  0.1589, -0.1566, -0.1556,\n",
       "                      -0.1498,  0.1826, -0.1546, -0.1491, -0.1564, -0.1414, -0.1458,  0.1559,\n",
       "                       0.1508,  0.1392, -0.1469, -0.1530,  0.0494,  0.1490, -0.1630, -0.1575],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.self_attention.W_Q.weight',\n",
       "              tensor([[-0.1750,  0.1352,  0.1916,  ..., -0.1720, -0.1187, -0.2115],\n",
       "                      [-0.1257,  0.1161, -0.1244,  ...,  0.1711, -0.1606,  0.1386],\n",
       "                      [ 0.0989,  0.2083,  0.1283,  ..., -0.1598,  0.1475, -0.1531],\n",
       "                      ...,\n",
       "                      [ 0.2289,  0.1806, -0.2146,  ...,  0.1340, -0.1294,  0.1594],\n",
       "                      [-0.2018,  0.2093,  0.0865,  ...,  0.1965, -0.1689,  0.1214],\n",
       "                      [ 0.1940, -0.0950,  0.2249,  ..., -0.1943,  0.1215, -0.1923]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.self_attention.W_K.weight',\n",
       "              tensor([[-0.2215, -0.2100,  0.1164,  ...,  0.1765,  0.0993, -0.1102],\n",
       "                      [ 0.1333,  0.2324, -0.1837,  ..., -0.1280, -0.1753, -0.0853],\n",
       "                      [ 0.1036, -0.1935,  0.1467,  ...,  0.1572,  0.1828, -0.2205],\n",
       "                      ...,\n",
       "                      [-0.0837,  0.1623,  0.2009,  ..., -0.1552,  0.1839, -0.1289],\n",
       "                      [-0.0835,  0.2121, -0.1722,  ..., -0.1741, -0.1856, -0.0975],\n",
       "                      [ 0.0881,  0.0998,  0.2272,  ...,  0.0848, -0.1375,  0.1569]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.self_attention.W_V.weight',\n",
       "              tensor([[-0.1610,  0.1682, -0.2226,  ...,  0.1619, -0.1160,  0.0756],\n",
       "                      [-0.2228, -0.1722,  0.0943,  ..., -0.0854,  0.1023, -0.1178],\n",
       "                      [ 0.1489, -0.2219, -0.1824,  ..., -0.1647,  0.0900, -0.1285],\n",
       "                      ...,\n",
       "                      [ 0.1831,  0.1726,  0.1503,  ..., -0.1984,  0.1531, -0.2264],\n",
       "                      [ 0.1642, -0.1261,  0.1272,  ..., -0.2167,  0.0804, -0.1265],\n",
       "                      [ 0.2358,  0.1389, -0.2551,  ...,  0.2130, -0.1479,  0.1060]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.self_attention.W_T.weight',\n",
       "              tensor([[ 0.1616,  0.2028, -0.2323,  ...,  0.1322, -0.1985,  0.0868],\n",
       "                      [-0.2175, -0.1881,  0.1795,  ..., -0.1205,  0.1235, -0.1857],\n",
       "                      [ 0.1637,  0.1594, -0.2094,  ..., -0.2090,  0.1660,  0.2438],\n",
       "                      ...,\n",
       "                      [ 0.1090,  0.0870, -0.1308,  ...,  0.1355, -0.1614,  0.1309],\n",
       "                      [ 0.1451,  0.1427, -0.1459,  ...,  0.1668, -0.1838,  0.0814],\n",
       "                      [-0.1241, -0.0881,  0.2447,  ..., -0.2248,  0.1219, -0.1165]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm1.gamma',\n",
       "              tensor([0.8657, 1.1222, 1.1452, 1.1520, 1.1158, 0.8460, 1.0305, 1.1393, 0.8457,\n",
       "                      0.8521, 1.1459, 1.0177, 0.8608, 0.9385, 1.1707, 1.1527, 1.1489, 1.1273,\n",
       "                      1.1862, 1.1624, 0.8763, 1.1534, 0.8628, 1.1316, 0.8040, 1.1482, 1.1528,\n",
       "                      1.0148, 0.8383, 0.8344, 1.1608, 0.8399, 0.8457, 0.9930, 1.1604, 0.8435,\n",
       "                      1.1972, 1.1527, 1.1372, 0.8527, 0.8265, 0.8735, 1.1626, 0.8607, 1.1419,\n",
       "                      0.8320, 1.2026, 0.7951, 0.8256, 0.8511, 0.8955, 0.8526, 0.8478, 0.8176,\n",
       "                      1.1444, 0.8590, 1.1375, 1.2043, 1.1497, 0.8253, 1.1477, 0.8538, 0.9569,\n",
       "                      0.8508, 0.8622, 0.9947, 0.8422, 1.1577, 0.8404, 0.8489, 1.1802, 1.1997,\n",
       "                      0.8611, 0.8361, 0.8504, 0.8665, 0.8642, 1.1544, 1.1516, 0.8468, 1.1588,\n",
       "                      0.8532, 0.8385, 0.7951, 1.1658, 1.1611, 1.1925, 0.8398, 1.1383, 0.8426,\n",
       "                      0.9382, 1.1541, 0.9987, 1.1598, 0.8495, 1.1545, 0.8646, 1.1617, 0.8484,\n",
       "                      1.0293, 1.1465, 1.1518, 0.8465, 1.1151, 0.7978, 0.9824, 1.1471, 1.1508,\n",
       "                      1.1609, 0.8054, 0.8455, 0.9078, 1.2038, 0.8430, 1.1730, 0.9951, 0.8209,\n",
       "                      0.8472, 1.1388, 0.8404, 1.1526, 0.8414, 0.8296, 1.1776, 1.1527, 1.1518,\n",
       "                      1.0076, 0.7959, 1.1474, 0.8355, 0.8396, 0.8472, 0.9664, 1.1003, 1.0667,\n",
       "                      0.8844, 1.1511, 0.8373, 1.1760, 1.1480, 1.1293, 0.8472, 0.8511, 1.0197,\n",
       "                      0.8472, 0.8476, 1.1534, 1.1420, 0.8479, 0.7965, 1.1768, 0.8530, 1.1539,\n",
       "                      1.1352, 1.1070, 1.1543, 1.1470, 1.1492, 1.1499, 1.1526, 1.1325, 0.8274,\n",
       "                      1.1400, 0.9253, 1.1485, 1.1227, 0.8533, 1.0795, 0.8202, 1.1489, 1.0912,\n",
       "                      0.8520, 1.1597, 1.1499, 0.8896, 1.1479, 1.1302, 0.8803, 0.8539, 1.1557,\n",
       "                      1.1550, 0.8702, 1.1149, 1.1600, 0.8578, 0.8796, 0.8497, 1.2028, 1.0383,\n",
       "                      0.8528, 0.8533, 1.1378, 1.1642, 1.1969, 0.8256, 1.1516, 1.1569, 1.1242,\n",
       "                      0.9357, 1.1502, 1.1863, 1.1807, 1.0255, 0.9729, 1.1744, 0.8456, 0.9308,\n",
       "                      1.1567, 0.8528, 1.1634, 1.1461, 0.8512, 1.1316, 0.8328, 1.1473, 1.1446,\n",
       "                      1.1468, 1.1698, 1.1821, 1.1326, 0.9468, 0.8461, 0.8244, 1.1462, 1.0503,\n",
       "                      1.2019, 0.8529, 0.8543, 1.1497, 1.1275, 0.8117, 1.1692, 0.8394, 0.8420,\n",
       "                      0.8593, 0.9291, 0.8577, 0.8464, 0.8463, 0.7986, 0.7950, 0.8651, 0.9636,\n",
       "                      0.9943, 1.0706, 0.8557, 1.1397, 1.2044, 1.2046, 0.8567, 0.8477, 1.1290,\n",
       "                      0.9147, 0.8610, 0.8542, 1.1217, 0.8466, 0.8350, 0.8489, 0.8846, 1.1139,\n",
       "                      1.1521, 0.9889, 1.1576, 1.1336, 0.8676, 1.1739, 1.1525, 0.8096, 1.2008,\n",
       "                      0.8652, 1.1486, 1.1791, 0.8505, 0.8658, 1.1723, 1.1855, 1.1249, 1.1668,\n",
       "                      0.7975, 0.8687, 1.1507, 1.0842, 1.1269, 1.1604, 0.8795, 0.9356, 0.8558,\n",
       "                      0.9275, 0.8456, 1.0864, 1.1445, 0.8570, 1.1592, 1.1439, 0.9194, 1.1555,\n",
       "                      0.8482, 0.8722, 0.8552, 0.9290, 1.0037, 1.1436, 0.8318, 0.8473, 0.8473,\n",
       "                      1.1519, 0.8703, 0.9292, 0.8556, 0.8491, 1.1356, 0.8449, 0.9547, 1.1540,\n",
       "                      0.8497, 1.1449, 1.0173, 0.8080, 1.1534, 1.1529, 1.1294, 1.1504, 0.8744,\n",
       "                      1.2045, 0.8456, 0.8474, 1.1538, 1.1831, 0.8693, 1.1571, 1.0004, 0.8781,\n",
       "                      1.1833, 1.1415, 1.1974, 0.8371, 0.8544, 0.8451, 0.8357, 1.1108, 0.8444,\n",
       "                      1.2041, 1.1576, 0.8147, 0.8634, 0.8669, 0.8644, 0.8544, 1.1451, 1.1391,\n",
       "                      1.0355, 1.1861, 0.8461, 0.8502, 1.1431, 1.1486, 1.1759, 0.8434, 0.8570,\n",
       "                      1.1614, 1.1502, 0.8417, 1.0020, 1.1374, 1.1702, 1.1569, 0.8528, 1.2039,\n",
       "                      0.8581, 0.9747, 0.8238, 1.1619, 1.1478, 0.8733, 1.1443, 1.1788, 0.8908,\n",
       "                      1.1617, 0.8503, 0.7949, 0.8434, 1.1285, 0.8472, 0.8619, 0.8048, 1.1158,\n",
       "                      0.8569, 0.8548, 1.1561, 0.8551, 1.1483, 1.1312, 1.1659, 1.1563, 0.8423,\n",
       "                      1.1726, 1.1805, 0.8372, 0.8439, 1.1461, 0.7979, 0.8523, 0.8185, 0.9915,\n",
       "                      0.9411, 0.8847, 1.2002, 0.8519, 0.8253, 1.1593, 1.1200, 0.8585, 1.2027,\n",
       "                      0.7948, 0.8457, 1.1757, 1.1526, 0.8518, 1.1511, 0.8717, 0.8611, 1.1089,\n",
       "                      1.1692, 1.1713, 1.1438, 0.8639, 1.1778, 0.8490, 1.1963, 0.8229, 1.1815,\n",
       "                      1.1467, 1.1533, 1.1453, 1.1597, 1.1543, 1.1846, 0.8268, 0.9020, 0.8296,\n",
       "                      1.1078, 0.8520, 1.1676, 1.1116, 1.1526, 0.8468, 0.8488, 1.1434, 0.8374,\n",
       "                      1.1899, 0.8336, 0.8687, 1.0372, 0.8027, 0.8816, 1.1507, 0.8947, 1.0442,\n",
       "                      1.0465, 1.1547, 0.9270, 0.9578, 1.1354, 0.8118, 0.8464, 0.8718, 1.2049,\n",
       "                      1.1331, 0.8471, 0.8447, 0.8644, 1.1429, 1.1603, 1.1512, 1.1442, 0.8384,\n",
       "                      0.8403, 0.8448, 1.1459, 1.1496, 0.8451, 0.8725, 1.1119, 1.1556, 1.0124,\n",
       "                      1.1453, 0.8491, 1.0794, 1.1521, 1.1276, 0.7951, 0.8438, 0.8490, 1.1502,\n",
       "                      0.8711, 0.8138, 1.1525, 0.9917, 0.8038, 1.1528, 0.8477, 0.8857, 0.8727,\n",
       "                      0.8184, 1.1456, 1.1689, 1.1520, 1.0969, 0.8481, 0.9856, 1.1626],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm1.beta',\n",
       "              tensor([ 0.1685, -0.1385,  0.2041,  0.1525,  0.1132, -0.1509, -0.0677, -0.0933,\n",
       "                       0.1503,  0.1576,  0.1420, -0.1662, -0.1635, -0.1735,  0.1569,  0.1527,\n",
       "                      -0.1416,  0.1312, -0.1674, -0.1657, -0.1590,  0.1577, -0.1559, -0.1410,\n",
       "                      -0.1387,  0.1484, -0.1215, -0.1401,  0.1689, -0.1494, -0.1597,  0.1439,\n",
       "                      -0.1496,  0.1742,  0.1679,  0.1507, -0.1729,  0.1538,  0.1756,  0.1679,\n",
       "                       0.1471,  0.1557, -0.1627,  0.1806,  0.1400, -0.1427, -0.1667,  0.1325,\n",
       "                       0.1290,  0.1541, -0.1789, -0.1551,  0.1670, -0.1069, -0.1388,  0.1551,\n",
       "                       0.0889,  0.1194,  0.1507,  0.1651,  0.1444, -0.1604, -0.2006, -0.1599,\n",
       "                      -0.1319,  0.1438, -0.1677, -0.1614, -0.1433, -0.1541,  0.1669, -0.1737,\n",
       "                       0.1837,  0.1403, -0.1546, -0.1563, -0.1709, -0.1529, -0.1513, -0.1783,\n",
       "                      -0.1526,  0.1550, -0.1677,  0.1257, -0.1674,  0.1573, -0.1713, -0.1385,\n",
       "                      -0.1435, -0.1467, -0.1884, -0.1502, -0.1916,  0.1610, -0.1670, -0.1569,\n",
       "                      -0.1208, -0.1613,  0.1547,  0.1274,  0.1480, -0.1518, -0.1655, -0.1448,\n",
       "                      -0.2049, -0.1670, -0.1451, -0.1498, -0.1546,  0.1484,  0.1535,  0.1745,\n",
       "                      -0.1576, -0.1656, -0.1642, -0.0958,  0.1425, -0.1489,  0.1485,  0.1469,\n",
       "                       0.1536,  0.1437, -0.1720,  0.0863,  0.1422,  0.1506, -0.1402, -0.0203,\n",
       "                      -0.1464,  0.1446,  0.1466,  0.1510,  0.1412,  0.1484, -0.1088, -0.1703,\n",
       "                       0.1448,  0.1512, -0.1597, -0.1734,  0.1396, -0.1520,  0.1604,  0.1936,\n",
       "                       0.1535, -0.1527, -0.1543, -0.1434, -0.1524, -0.1239, -0.1574, -0.1555,\n",
       "                       0.1552, -0.1460,  0.1172, -0.1607, -0.1777,  0.1492, -0.1499, -0.1525,\n",
       "                       0.1883, -0.1316, -0.1957,  0.0334,  0.1463,  0.1146, -0.1629,  0.1145,\n",
       "                       0.0409, -0.1490,  0.0913,  0.1607,  0.1633,  0.1436, -0.2029,  0.1378,\n",
       "                       0.1466,  0.1597,  0.1711, -0.1546, -0.1546,  0.1631, -0.1134,  0.1589,\n",
       "                       0.1545,  0.1844,  0.1552, -0.1934, -0.1405,  0.1597,  0.1558,  0.0989,\n",
       "                       0.1633, -0.1583, -0.1235,  0.1510, -0.1618,  0.1424,  0.1615,  0.1510,\n",
       "                       0.0522,  0.1684,  0.1793, -0.1180, -0.1878, -0.1488,  0.1565,  0.1476,\n",
       "                      -0.1556, -0.1546, -0.1364, -0.1609, -0.0976, -0.1462, -0.1634, -0.1181,\n",
       "                       0.1452, -0.1579, -0.1830, -0.1496, -0.1654, -0.1515,  0.1373,  0.1442,\n",
       "                       0.0507, -0.1620,  0.1577, -0.1635,  0.1512,  0.1393,  0.1337,  0.1620,\n",
       "                       0.1427,  0.1344,  0.1626,  0.0294,  0.1658,  0.1516,  0.1417,  0.1919,\n",
       "                      -0.1985, -0.1609, -0.1462, -0.1694,  0.1812,  0.1557,  0.1445,  0.1714,\n",
       "                       0.1762,  0.1590,  0.1521,  0.0245, -0.1250, -0.1708,  0.1633,  0.1005,\n",
       "                       0.1514, -0.1477, -0.1563,  0.1591,  0.1305,  0.1434,  0.1308,  0.1638,\n",
       "                      -0.1346,  0.1704,  0.1693,  0.1501,  0.0881, -0.1712, -0.1985, -0.1470,\n",
       "                      -0.1715,  0.1549,  0.1616, -0.1621, -0.1610,  0.1092, -0.1244, -0.1106,\n",
       "                       0.1573, -0.1510, -0.1218, -0.1138,  0.1700,  0.1408, -0.1415,  0.1562,\n",
       "                      -0.1198, -0.1485, -0.1500, -0.1370,  0.1548, -0.1564, -0.1455, -0.1635,\n",
       "                      -0.1576, -0.1359,  0.1614, -0.1579,  0.1607,  0.1185, -0.1491,  0.1676,\n",
       "                       0.1512,  0.1413,  0.1464,  0.1648,  0.1991, -0.1573,  0.1540, -0.2014,\n",
       "                      -0.1508,  0.1642, -0.1524,  0.1611,  0.1850, -0.1884,  0.1447,  0.1558,\n",
       "                      -0.1265,  0.1436,  0.1502,  0.1964, -0.2044,  0.1501, -0.1564,  0.1535,\n",
       "                       0.1594, -0.0989, -0.1644,  0.2040,  0.1915,  0.1647,  0.1449, -0.1717,\n",
       "                       0.1437, -0.1557,  0.1494, -0.1458,  0.1238, -0.1506,  0.0026,  0.1600,\n",
       "                      -0.1179,  0.1594, -0.1748, -0.1715,  0.1567, -0.1355,  0.1569, -0.1675,\n",
       "                      -0.1618,  0.1508, -0.1581,  0.1509, -0.1514, -0.1897,  0.1491, -0.1603,\n",
       "                       0.1311, -0.1475, -0.1492,  0.2028, -0.0659,  0.1953, -0.1572, -0.1645,\n",
       "                      -0.1934,  0.1591,  0.1989,  0.1232, -0.1553,  0.1460, -0.1674, -0.1432,\n",
       "                      -0.1929, -0.1552,  0.1597, -0.1527,  0.1209,  0.1701,  0.1460, -0.1520,\n",
       "                      -0.1657, -0.1405,  0.1833,  0.1956, -0.1666,  0.1546,  0.1643, -0.1377,\n",
       "                       0.1495,  0.1278,  0.1611, -0.1469, -0.1538,  0.1247,  0.2000, -0.1474,\n",
       "                       0.1417,  0.1466,  0.1565, -0.1436,  0.1952,  0.1558,  0.1962, -0.1803,\n",
       "                       0.1801,  0.1245, -0.1550,  0.1855,  0.1538,  0.1694, -0.0713, -0.1500,\n",
       "                       0.1587, -0.1518,  0.1579,  0.1588,  0.1693,  0.1666, -0.1160, -0.0569,\n",
       "                      -0.1293, -0.1514,  0.1570,  0.1157,  0.1573, -0.0174, -0.1490, -0.1598,\n",
       "                       0.1620, -0.1528, -0.1049,  0.1608,  0.1536,  0.1895,  0.1405,  0.2034,\n",
       "                       0.1395, -0.1277, -0.1589, -0.1530,  0.1717, -0.1525, -0.1483, -0.1554,\n",
       "                       0.1236,  0.1513,  0.0381,  0.1685,  0.1589, -0.1837, -0.1152,  0.1787,\n",
       "                      -0.1509, -0.1924,  0.0500, -0.1636,  0.1560,  0.0363, -0.0062,  0.1499,\n",
       "                       0.1945, -0.1586,  0.1654,  0.1767, -0.1464, -0.1514,  0.1508, -0.1833,\n",
       "                      -0.1369,  0.1590, -0.1406, -0.1473, -0.1455, -0.1313, -0.1239,  0.1422,\n",
       "                       0.1502, -0.1507,  0.1643,  0.1233,  0.1548,  0.1645,  0.1308,  0.1527,\n",
       "                      -0.1060, -0.1523,  0.0746,  0.1382, -0.1500, -0.1477, -0.1624, -0.1682,\n",
       "                      -0.1384, -0.1527,  0.0255, -0.1465, -0.1549, -0.1625,  0.1830,  0.1697,\n",
       "                       0.1372,  0.1435, -0.1676, -0.1522,  0.0389,  0.1569,  0.1391, -0.1571],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[-0.1253, -0.1022,  0.1676,  ..., -0.1817,  0.1675, -0.0698],\n",
       "                      [ 0.2027,  0.0960, -0.1794,  ...,  0.0927,  0.1450,  0.0936],\n",
       "                      [-0.1963, -0.2366,  0.2019,  ..., -0.1915,  0.1188, -0.2367],\n",
       "                      ...,\n",
       "                      [ 0.1847,  0.1164,  0.1545,  ...,  0.2100, -0.1463, -0.1610],\n",
       "                      [ 0.1400, -0.1480, -0.1476,  ..., -0.1125,  0.1108,  0.1294],\n",
       "                      [ 0.1478,  0.0868, -0.1709,  ...,  0.1455,  0.0371,  0.1253]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.enc_dec_attention.W_K.weight',\n",
       "              tensor([[ 0.0253, -0.1584,  0.0962,  ..., -0.0657, -0.1205,  0.1539],\n",
       "                      [ 0.1204,  0.1035, -0.0790,  ...,  0.1875,  0.1488, -0.1426],\n",
       "                      [ 0.0872,  0.1399, -0.2138,  ...,  0.0825,  0.1919, -0.2147],\n",
       "                      ...,\n",
       "                      [-0.2115, -0.1879,  0.0938,  ...,  0.1440,  0.1452, -0.0964],\n",
       "                      [ 0.1475,  0.2743, -0.2146,  ...,  0.1291, -0.2205,  0.0165],\n",
       "                      [-0.1301, -0.1770,  0.1381,  ..., -0.1742, -0.1481, -0.1773]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.enc_dec_attention.W_V.weight',\n",
       "              tensor([[-0.1477, -0.0271, -0.1153,  ...,  0.2461,  0.1251, -0.0725],\n",
       "                      [ 0.1741,  0.0362,  0.1106,  ..., -0.0283, -0.0761, -0.0560],\n",
       "                      [-0.0351, -0.2122, -0.2387,  ..., -0.0669,  0.1560,  0.0161],\n",
       "                      ...,\n",
       "                      [ 0.0614,  0.1840,  0.1141,  ...,  0.1657, -0.1905, -0.2571],\n",
       "                      [ 0.1391,  0.2103,  0.1919,  ...,  0.0773, -0.1679,  0.2528],\n",
       "                      [-0.1135, -0.0864, -0.2139,  ..., -0.2318,  0.2027,  0.1639]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.enc_dec_attention.W_T.weight',\n",
       "              tensor([[-0.1656,  0.0564,  0.2426,  ..., -0.1344,  0.0726, -0.1224],\n",
       "                      [ 0.1284, -0.2212, -0.0602,  ...,  0.0770, -0.2288, -0.1538],\n",
       "                      [-0.2100,  0.0028,  0.1953,  ..., -0.2403,  0.1073,  0.0216],\n",
       "                      ...,\n",
       "                      [-0.1399,  0.2084,  0.1219,  ..., -0.2151,  0.1619,  0.0483],\n",
       "                      [-0.0899,  0.1493,  0.1714,  ..., -0.1743,  0.1066, -0.0347],\n",
       "                      [ 0.1165, -0.1899, -0.1400,  ...,  0.1800, -0.0772,  0.2756]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm2.gamma',\n",
       "              tensor([0.8290, 1.1681, 1.1841, 1.1577, 1.1412, 0.8429, 0.9402, 1.1835, 0.8462,\n",
       "                      0.8488, 1.0548, 0.8738, 0.8295, 0.8429, 0.8508, 1.1573, 1.1530, 1.1266,\n",
       "                      1.1936, 1.1245, 0.8890, 1.1577, 0.8499, 1.1517, 1.1385, 1.0800, 0.9009,\n",
       "                      1.1139, 0.7967, 1.1497, 1.1759, 1.1542, 0.8466, 0.8739, 0.7917, 1.0956,\n",
       "                      0.8948, 1.1511, 0.9673, 0.8325, 0.8402, 0.8510, 1.1819, 1.1941, 1.1555,\n",
       "                      0.9099, 1.1970, 1.0213, 1.1113, 0.8516, 0.8215, 0.8524, 0.8377, 1.1722,\n",
       "                      1.1210, 0.8577, 1.1328, 1.1815, 1.1546, 0.9269, 1.1419, 1.1465, 0.9916,\n",
       "                      0.8592, 1.1828, 1.1439, 0.8519, 1.1568, 0.8396, 0.8554, 1.2025, 0.7870,\n",
       "                      0.8116, 0.8796, 0.8532, 1.1602, 0.8478, 0.8436, 1.1512, 0.8719, 1.1525,\n",
       "                      0.8626, 1.0311, 0.9658, 1.0174, 1.2008, 1.1518, 1.1593, 1.1536, 1.0677,\n",
       "                      0.9626, 1.1637, 1.1888, 1.1524, 0.8288, 1.1467, 1.1604, 1.1658, 0.8481,\n",
       "                      0.9329, 1.1401, 1.1525, 1.1575, 1.1369, 1.1754, 0.9722, 1.1380, 1.1547,\n",
       "                      0.8526, 1.1540, 0.8434, 0.8232, 0.8658, 0.7869, 1.1969, 0.9289, 0.8358,\n",
       "                      0.8452, 0.8417, 0.8451, 1.1580, 0.8492, 0.7930, 0.8003, 1.1516, 0.8267,\n",
       "                      1.1547, 0.7973, 1.1410, 0.8254, 1.1424, 0.8473, 1.1037, 0.8085, 0.9258,\n",
       "                      1.1782, 1.1569, 0.8482, 1.1761, 1.1903, 1.1476, 0.8460, 0.8657, 0.8772,\n",
       "                      0.8453, 0.8454, 1.1517, 1.1102, 1.1581, 0.9420, 0.8507, 0.8516, 0.9005,\n",
       "                      1.1512, 1.1656, 1.1984, 1.1825, 0.8387, 1.1515, 1.1542, 0.8542, 1.1566,\n",
       "                      0.8618, 0.9775, 1.1543, 0.9917, 0.8437, 1.0072, 0.9555, 0.8497, 0.9653,\n",
       "                      0.8624, 1.1766, 1.1543, 0.7983, 1.1979, 1.1370, 1.1609, 0.8520, 1.1646,\n",
       "                      1.1559, 1.1680, 1.1249, 0.8927, 0.8520, 1.0328, 0.8488, 0.9886, 1.1547,\n",
       "                      0.8978, 0.8553, 1.1040, 1.1600, 0.8404, 1.1790, 1.1528, 1.1470, 1.1472,\n",
       "                      0.8820, 1.1564, 1.1665, 0.9631, 0.8770, 1.0584, 1.2044, 0.8463, 0.7931,\n",
       "                      0.8561, 0.9071, 1.1575, 1.1598, 0.8936, 0.9514, 0.8385, 1.1438, 1.1114,\n",
       "                      0.8295, 0.8658, 0.9802, 1.1479, 0.8989, 0.8456, 1.1573, 1.0233, 0.9665,\n",
       "                      0.9647, 0.8669, 1.0579, 0.8466, 1.1495, 0.9320, 0.8362, 0.7971, 1.1460,\n",
       "                      0.8997, 1.1740, 0.8210, 0.8455, 1.1736, 0.8203, 0.8104, 0.8424, 0.8170,\n",
       "                      0.9816, 0.9727, 0.8635, 1.1064, 1.0394, 0.7964, 0.8398, 0.8468, 1.1878,\n",
       "                      0.9973, 0.8725, 0.8571, 1.1390, 0.8449, 0.8219, 0.8395, 0.8618, 1.1996,\n",
       "                      1.1365, 1.0897, 1.1579, 1.1379, 1.2119, 1.2081, 1.1489, 1.1775, 0.8811,\n",
       "                      0.9219, 1.1539, 1.1731, 0.8501, 0.8690, 0.8265, 1.0432, 1.1188, 1.1578,\n",
       "                      0.9534, 0.8344, 1.1498, 0.9979, 0.7959, 1.1676, 1.2031, 0.8323, 0.8524,\n",
       "                      0.8851, 1.1319, 1.0909, 1.1629, 0.8784, 1.1019, 1.1515, 0.9004, 1.1589,\n",
       "                      0.8378, 0.8822, 0.7968, 0.8830, 1.1493, 0.8406, 0.7929, 0.8460, 1.1875,\n",
       "                      1.1604, 0.8547, 0.8101, 0.8554, 0.8497, 0.8094, 0.8492, 0.8902, 1.1571,\n",
       "                      0.8498, 1.1032, 1.0502, 0.8319, 1.1548, 1.1585, 0.8445, 1.1524, 0.7950,\n",
       "                      1.0576, 0.8538, 0.8381, 0.8817, 0.8853, 0.8439, 1.1621, 1.0377, 0.8603,\n",
       "                      0.8409, 1.1519, 1.1737, 1.1372, 0.8752, 0.8355, 1.1438, 0.8171, 0.8472,\n",
       "                      1.1802, 1.1620, 0.8336, 1.2049, 0.8966, 0.8485, 0.8840, 0.9630, 0.8356,\n",
       "                      1.0599, 1.2006, 1.1598, 0.8309, 0.8199, 0.8456, 1.1780, 1.1370, 0.8527,\n",
       "                      1.1529, 1.1366, 0.8404, 0.9469, 1.1153, 1.1876, 1.1507, 0.9704, 1.2017,\n",
       "                      0.8471, 1.0532, 0.8169, 0.8899, 1.1491, 0.8374, 0.9357, 1.2067, 0.8551,\n",
       "                      0.8358, 1.1566, 1.0833, 0.7914, 1.1598, 0.8455, 0.8476, 1.1158, 1.0514,\n",
       "                      0.8027, 1.0111, 1.1767, 0.8068, 1.1301, 1.1037, 1.1659, 1.1601, 0.8440,\n",
       "                      1.1616, 1.1901, 0.7938, 0.8524, 1.1148, 1.1387, 1.1927, 0.9364, 0.7901,\n",
       "                      0.8489, 1.0156, 1.0618, 0.8065, 1.0760, 1.1719, 1.1579, 0.8473, 0.9735,\n",
       "                      1.2036, 0.8471, 0.8633, 1.1599, 0.8488, 1.1484, 0.9398, 1.0435, 1.1183,\n",
       "                      1.1397, 1.1286, 1.1527, 0.8592, 1.2052, 0.8392, 1.1815, 1.1424, 0.8539,\n",
       "                      1.1473, 1.1578, 1.1201, 1.1641, 1.1680, 1.1463, 1.1014, 0.9195, 0.8298,\n",
       "                      1.1772, 0.8602, 1.1619, 1.1360, 0.8505, 0.8416, 0.8481, 1.1204, 0.8437,\n",
       "                      1.1921, 1.1908, 0.9689, 0.8798, 0.8069, 0.9135, 1.1527, 0.9512, 0.7904,\n",
       "                      0.8268, 1.1603, 1.2013, 1.1921, 0.8024, 0.9553, 0.8272, 0.8485, 0.8170,\n",
       "                      1.1389, 0.8471, 0.8438, 0.8201, 1.1322, 1.1672, 1.1473, 1.1485, 1.0881,\n",
       "                      0.8307, 0.8979, 1.1580, 0.8477, 1.1468, 0.8507, 1.1461, 1.1596, 0.9722,\n",
       "                      0.8473, 0.8475, 0.8055, 1.1548, 1.1288, 0.8677, 0.8439, 0.8500, 1.1353,\n",
       "                      0.8319, 0.8263, 0.8518, 0.9910, 1.1422, 1.1536, 0.8815, 0.8744, 0.9095,\n",
       "                      1.1480, 1.1435, 0.9057, 1.1578, 1.1936, 0.8383, 0.7949, 1.1644],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm2.beta',\n",
       "              tensor([ 0.1766, -0.1303,  0.2082,  0.1573,  0.0889, -0.1490, -0.0669, -0.0671,\n",
       "                       0.1469,  0.1608,  0.1378, -0.1756, -0.1683, -0.1841,  0.1594,  0.1568,\n",
       "                      -0.1382,  0.1193, -0.1796, -0.1747, -0.1632,  0.1652, -0.1576, -0.1336,\n",
       "                      -0.1332,  0.1478, -0.0915, -0.1341,  0.1751, -0.1503, -0.1687,  0.1380,\n",
       "                      -0.1459,  0.1864,  0.1750,  0.1518, -0.1839,  0.1561,  0.1872,  0.1741,\n",
       "                       0.1419,  0.1571, -0.1700,  0.1914,  0.1327, -0.1309, -0.1772,  0.1224,\n",
       "                       0.1163,  0.1542, -0.1868, -0.1564,  0.1718, -0.0845, -0.1347,  0.1557,\n",
       "                       0.0539,  0.0979,  0.1538,  0.1748,  0.1421, -0.1663, -0.1810, -0.1647,\n",
       "                      -0.1146,  0.1415, -0.1752, -0.1708, -0.1334, -0.1553,  0.1807, -0.1843,\n",
       "                       0.1899,  0.1236, -0.1555, -0.1629, -0.1795, -0.1544, -0.1550, -0.1925,\n",
       "                      -0.1540,  0.1558, -0.1691,  0.1064, -0.1776,  0.1592, -0.1820, -0.1296,\n",
       "                      -0.1409, -0.1469, -0.1996, -0.1616, -0.1944,  0.1680, -0.1760, -0.1601,\n",
       "                      -0.1058, -0.1720,  0.1541,  0.1063,  0.1490, -0.1555, -0.1807, -0.1440,\n",
       "                      -0.2022, -0.1764, -0.1441, -0.1528, -0.1558,  0.1468,  0.1518,  0.1843,\n",
       "                      -0.1613, -0.1747, -0.1729, -0.0705,  0.1314, -0.1436,  0.1442,  0.1342,\n",
       "                       0.1589,  0.1324, -0.1817,  0.0372,  0.1382,  0.1490, -0.1352, -0.0260,\n",
       "                      -0.1465,  0.1368,  0.1477,  0.1440,  0.1340,  0.1443, -0.0843, -0.1745,\n",
       "                       0.1419,  0.1456, -0.1650, -0.1817,  0.1319, -0.1507,  0.1653,  0.2029,\n",
       "                       0.1495, -0.1539, -0.1569, -0.1412, -0.1563, -0.1092, -0.1609, -0.1572,\n",
       "                       0.1589, -0.1464,  0.0921, -0.1680, -0.1941,  0.1456, -0.1528, -0.1562,\n",
       "                       0.1875, -0.1193, -0.2034,  0.0633,  0.1437,  0.0943, -0.1603,  0.0923,\n",
       "                      -0.0257, -0.1422,  0.0630,  0.1618,  0.1704,  0.1407, -0.1942,  0.1267,\n",
       "                       0.1455,  0.1659,  0.1792, -0.1605, -0.1593,  0.1697, -0.0916,  0.1633,\n",
       "                       0.1553,  0.1967,  0.1556, -0.1969, -0.1361,  0.1650,  0.1534,  0.0312,\n",
       "                       0.1786, -0.1587, -0.1047,  0.1533, -0.1693,  0.1407,  0.1683,  0.1494,\n",
       "                       0.0122,  0.1776,  0.1947, -0.0985, -0.2004, -0.1450,  0.1575,  0.1381,\n",
       "                      -0.1573, -0.1600, -0.1266, -0.1668, -0.0557, -0.1387, -0.1718, -0.0969,\n",
       "                       0.1375, -0.1605, -0.1976, -0.1523, -0.1717, -0.1503,  0.1247,  0.1408,\n",
       "                       0.0165, -0.1767,  0.1633, -0.1696,  0.1486,  0.1339,  0.1175,  0.1662,\n",
       "                       0.1335,  0.1234,  0.1683,  0.0074,  0.1701,  0.1466,  0.1360,  0.1997,\n",
       "                      -0.2005, -0.1641, -0.1401, -0.1800,  0.1776,  0.1578,  0.1423,  0.1830,\n",
       "                       0.1873,  0.1604,  0.1472, -0.0116, -0.0997, -0.1827,  0.1697,  0.0765,\n",
       "                       0.1492, -0.1437, -0.1561,  0.1625,  0.1158,  0.1417,  0.1163,  0.1743,\n",
       "                      -0.1258,  0.1859,  0.1832,  0.1525,  0.0524, -0.1822, -0.2002, -0.1485,\n",
       "                      -0.1813,  0.1558,  0.1677, -0.1653, -0.1752,  0.0744, -0.1079, -0.0825,\n",
       "                       0.1577, -0.1530, -0.1098, -0.0823,  0.1821,  0.1321, -0.1349,  0.1572,\n",
       "                      -0.0956, -0.1493, -0.1508, -0.1286,  0.1575, -0.1566, -0.1449, -0.1711,\n",
       "                      -0.1655, -0.1179,  0.1682, -0.1581,  0.1653,  0.0976, -0.1466,  0.1759,\n",
       "                       0.1485,  0.1331,  0.1445,  0.1724,  0.2007, -0.1604,  0.1544, -0.2031,\n",
       "                      -0.1478,  0.1700, -0.1560,  0.1678,  0.1879, -0.1994,  0.1334,  0.1590,\n",
       "                      -0.1597,  0.1344,  0.1525,  0.2012, -0.2003,  0.1416, -0.1546,  0.1556,\n",
       "                       0.1616, -0.1231, -0.1821,  0.2026,  0.2028,  0.1709,  0.1433, -0.1844,\n",
       "                       0.1408, -0.1582,  0.1442, -0.1452,  0.1026, -0.1479,  0.0433,  0.1687,\n",
       "                      -0.1144,  0.1631, -0.1870, -0.1828,  0.1591, -0.1290,  0.1544, -0.1794,\n",
       "                      -0.1668,  0.1503, -0.1628,  0.1491, -0.1501, -0.2052,  0.1503, -0.1632,\n",
       "                       0.1209, -0.1480, -0.1447,  0.2046, -0.2026,  0.2073, -0.1629, -0.1735,\n",
       "                      -0.2015,  0.1629,  0.2044,  0.0987, -0.1585,  0.1455, -0.1744, -0.1329,\n",
       "                      -0.2042, -0.1563,  0.1618, -0.1575,  0.1027,  0.1804,  0.1417, -0.1509,\n",
       "                      -0.1724, -0.1351,  0.1812,  0.2004, -0.1722,  0.1574,  0.1686, -0.1327,\n",
       "                       0.1492,  0.1214,  0.1740, -0.1405, -0.1582,  0.1053,  0.1637, -0.1403,\n",
       "                       0.1371,  0.1464,  0.1583, -0.1374,  0.2012,  0.1569,  0.2033, -0.1946,\n",
       "                       0.1908,  0.1094, -0.1607,  0.2011,  0.1538,  0.1840, -0.1668, -0.1473,\n",
       "                       0.1608, -0.1601,  0.1608,  0.1639,  0.1750,  0.1748, -0.0973, -0.0961,\n",
       "                      -0.1186, -0.1547,  0.1598,  0.0941,  0.1556,  0.0406, -0.1503, -0.1646,\n",
       "                       0.1700, -0.1568, -0.0913,  0.1707,  0.1608,  0.2033,  0.1360,  0.1980,\n",
       "                       0.1262, -0.1137, -0.1626, -0.1581,  0.1631, -0.1531, -0.1428, -0.1556,\n",
       "                       0.1056,  0.1495, -0.0376,  0.1761,  0.1648, -0.1977, -0.0884,  0.1930,\n",
       "                      -0.1556, -0.2021,  0.0668, -0.1730,  0.1622,  0.0540,  0.0146,  0.1509,\n",
       "                       0.2009, -0.1593,  0.1733,  0.1879, -0.1456, -0.1495,  0.1471, -0.1927,\n",
       "                      -0.1334,  0.1669, -0.1398, -0.1496, -0.1433, -0.1149, -0.1015,  0.1346,\n",
       "                       0.1442, -0.1528,  0.1713,  0.1040,  0.1603,  0.1728,  0.1087,  0.1521,\n",
       "                      -0.0688, -0.1545,  0.0132,  0.1211, -0.1471, -0.1388, -0.1740, -0.1759,\n",
       "                      -0.1237, -0.1537,  0.0492, -0.1464, -0.1598, -0.1690,  0.1996,  0.1807,\n",
       "                       0.1296,  0.1401, -0.1758, -0.1570,  0.0276,  0.1528,  0.1266, -0.1659],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.ffn.layer1.weight',\n",
       "              tensor([[-0.2072, -0.1773,  0.1976,  ..., -0.1797, -0.1103, -0.1777],\n",
       "                      [ 0.1683, -0.2127, -0.0148,  ...,  0.0226, -0.0677,  0.0067],\n",
       "                      [ 0.1037,  0.2196, -0.0530,  ...,  0.0072,  0.1805,  0.0227],\n",
       "                      ...,\n",
       "                      [ 0.1489, -0.0720, -0.0798,  ...,  0.1305, -0.0716,  0.1284],\n",
       "                      [ 0.0820,  0.1579, -0.0907,  ...,  0.0248,  0.1936,  0.0077],\n",
       "                      [-0.1061,  0.1575,  0.1558,  ..., -0.0746,  0.0817, -0.0887]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.ffn.layer1.bias',\n",
       "              tensor([ 0.1389, -0.0010, -0.0544,  ..., -0.1406, -0.0296,  0.1584],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.ffn.layer2.weight',\n",
       "              tensor([[ 0.1577, -0.0053,  0.0173,  ...,  0.1250,  0.0859,  0.1582],\n",
       "                      [-0.1545, -0.1906, -0.1645,  ..., -0.0960, -0.1635, -0.1143],\n",
       "                      [ 0.2396,  0.0801,  0.0213,  ...,  0.1191,  0.1132,  0.1659],\n",
       "                      ...,\n",
       "                      [ 0.1682,  0.0802,  0.1184,  ..., -0.1659, -0.1588, -0.1604],\n",
       "                      [ 0.1459,  0.0025,  0.0213,  ..., -0.1149, -0.0733, -0.1521],\n",
       "                      [-0.1845, -0.1823, -0.1888,  ..., -0.1353, -0.1688, -0.1659]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.ffn.layer2.bias',\n",
       "              tensor([-0.1260, -0.1535,  0.1652,  0.1871, -0.1449, -0.1301, -0.1746, -0.1431,\n",
       "                       0.1702,  0.1372,  0.1485, -0.1706, -0.1412, -0.1374,  0.1427,  0.1606,\n",
       "                      -0.1598,  0.1321, -0.1593, -0.1871, -0.1591,  0.1849, -0.1428, -0.0903,\n",
       "                      -0.1423,  0.1468,  0.0960, -0.1390,  0.1245, -0.1488, -0.1499,  0.1519,\n",
       "                      -0.1622,  0.1734,  0.1567,  0.1716, -0.1365,  0.1974,  0.2122,  0.1521,\n",
       "                       0.1635,  0.1352, -0.1247,  0.1719,  0.1457, -0.1865, -0.1619,  0.1463,\n",
       "                       0.1611,  0.1586, -0.1582, -0.1554,  0.1503,  0.0103, -0.1331,  0.1616,\n",
       "                      -0.1622,  0.1442,  0.1699,  0.0531,  0.1707, -0.1878, -0.1897, -0.1739,\n",
       "                      -0.1725,  0.1591,  0.1682, -0.1967, -0.0777, -0.1228,  0.1689, -0.1559,\n",
       "                      -0.0634,  0.1097, -0.1371,  0.0836, -0.1846, -0.1740, -0.1687,  0.1594,\n",
       "                      -0.1345,  0.1298, -0.1469,  0.1330, -0.1598,  0.1558, -0.2091, -0.1230,\n",
       "                      -0.1422, -0.1461,  0.1333, -0.1646,  0.1716,  0.1825, -0.1197, -0.1626,\n",
       "                      -0.2016, -0.1768,  0.1113, -0.1932,  0.1672, -0.1530, -0.1770, -0.1484,\n",
       "                      -0.1144, -0.1536, -0.1564, -0.1454, -0.1468,  0.1140,  0.1511,  0.0710,\n",
       "                      -0.1474, -0.1734, -0.1761, -0.2095,  0.1208, -0.0895,  0.1461,  0.1168,\n",
       "                       0.1620,  0.1423, -0.1658,  0.1307,  0.1800,  0.1475, -0.1561, -0.1873,\n",
       "                      -0.1553,  0.1593,  0.1668,  0.1895,  0.1562,  0.1496, -0.1512,  0.0497,\n",
       "                       0.1515, -0.1859, -0.1470, -0.1406,  0.1311, -0.1456, -0.1603, -0.1896,\n",
       "                      -0.1565, -0.1773, -0.1808, -0.1630, -0.1772, -0.1979, -0.1054, -0.1690,\n",
       "                       0.1571, -0.1649,  0.0377, -0.1305, -0.0749,  0.1303, -0.1671, -0.1708,\n",
       "                       0.1417, -0.1368,  0.0613,  0.1390,  0.1598,  0.1368, -0.1196,  0.1412,\n",
       "                      -0.1349, -0.1353,  0.0196,  0.0495,  0.1591,  0.2203, -0.1024,  0.1821,\n",
       "                       0.1381,  0.1618,  0.1355, -0.1595, -0.1530,  0.1360, -0.1584, -0.0458,\n",
       "                       0.1402,  0.1916,  0.1394,  0.2021, -0.1354,  0.1430,  0.1361,  0.1909,\n",
       "                       0.1519, -0.1535, -0.1841,  0.1776, -0.1525,  0.1523,  0.1015,  0.1396,\n",
       "                      -0.1549,  0.1245, -0.2105, -0.1444, -0.1643, -0.1373,  0.1702, -0.1913,\n",
       "                      -0.1442, -0.1939, -0.1703, -0.1610, -0.0796, -0.1422, -0.1565, -0.1733,\n",
       "                       0.1996, -0.1501,  0.1801, -0.1457, -0.1421, -0.1266, -0.1473,  0.1526,\n",
       "                       0.1825, -0.1707, -0.1035, -0.1907,  0.1640,  0.1894,  0.1585,  0.1463,\n",
       "                       0.1193,  0.1785,  0.0325, -0.1232, -0.1135, -0.1791,  0.1817, -0.1448,\n",
       "                       0.2221, -0.1556, -0.1335, -0.1702, -0.0451, -0.2198,  0.1425,  0.1414,\n",
       "                       0.1581,  0.1374,  0.1291,  0.1613,  0.1268, -0.1365,  0.1301,  0.1851,\n",
       "                       0.1480, -0.1548, -0.1263,  0.1550,  0.1340,  0.1752,  0.1469,  0.1877,\n",
       "                      -0.1814,  0.2181,  0.1444,  0.0691,  0.1565, -0.1629, -0.0148, -0.1766,\n",
       "                      -0.1605,  0.1332,  0.1338, -0.0275,  0.2000, -0.1605, -0.1645, -0.0868,\n",
       "                       0.1072, -0.1334, -0.1623,  0.2160, -0.0968,  0.1689, -0.1556,  0.1539,\n",
       "                      -0.1501, -0.1566, -0.1542, -0.1511,  0.1699, -0.1361, -0.1717, -0.1541,\n",
       "                      -0.1921, -0.1418,  0.1373,  0.1800,  0.1364,  0.1673, -0.1246,  0.1661,\n",
       "                       0.1513,  0.1445, -0.1583,  0.1835,  0.1919, -0.1515,  0.1364,  0.1519,\n",
       "                      -0.1415,  0.1344,  0.1588,  0.1710,  0.1429, -0.1711,  0.0645, -0.0338,\n",
       "                       0.2013,  0.1314,  0.1976,  0.1321, -0.1565,  0.1122, -0.1459,  0.1665,\n",
       "                       0.1378, -0.1550,  0.1423, -0.1528,  0.1636,  0.1745,  0.1662, -0.1686,\n",
       "                       0.1503, -0.1291,  0.1370, -0.1489,  0.1300, -0.1320, -0.2179,  0.1784,\n",
       "                       0.0544,  0.1595, -0.1113, -0.1301,  0.1418,  0.0720,  0.1625, -0.1729,\n",
       "                      -0.1283,  0.1383, -0.1562,  0.1601, -0.0894,  0.0389,  0.1773,  0.1929,\n",
       "                       0.0819, -0.1498, -0.1534,  0.1328, -0.1521,  0.1844, -0.1933, -0.1512,\n",
       "                       0.1708,  0.1516,  0.1646, -0.1668, -0.1452,  0.1693, -0.1576,  0.0206,\n",
       "                       0.0336, -0.1343,  0.1365, -0.1607,  0.1736,  0.1470,  0.1370, -0.1186,\n",
       "                      -0.1493, -0.1752, -0.0819,  0.1472, -0.1293,  0.1591,  0.1431, -0.1639,\n",
       "                       0.1374,  0.1508,  0.1878, -0.1425, -0.1745, -0.1251,  0.1685, -0.1518,\n",
       "                       0.1266,  0.1632,  0.1607, -0.1379, -0.1017,  0.1655,  0.1030, -0.1677,\n",
       "                       0.1762,  0.1295, -0.1917, -0.0931,  0.1560,  0.1785, -0.1853, -0.1392,\n",
       "                      -0.1544, -0.1800,  0.1525,  0.1826,  0.1240,  0.0696,  0.2016, -0.1537,\n",
       "                      -0.1784, -0.1700,  0.1350,  0.1569,  0.1212, -0.1553, -0.1476, -0.1528,\n",
       "                       0.2068, -0.1740, -0.2035,  0.1741,  0.1270, -0.0756,  0.1393, -0.0740,\n",
       "                       0.1086, -0.1581, -0.1416, -0.1873,  0.1045, -0.0637, -0.1335,  0.0192,\n",
       "                       0.1546,  0.1281,  0.1336, -0.0310,  0.1332, -0.1670, -0.1330,  0.1425,\n",
       "                      -0.2069, -0.1317,  0.1313, -0.1569,  0.1825,  0.1821, -0.1262,  0.1155,\n",
       "                       0.1882, -0.1554,  0.1651,  0.1403, -0.1597, -0.1424,  0.1423,  0.1525,\n",
       "                      -0.1549,  0.1503,  0.0894, -0.1821, -0.1431, -0.1167, -0.1402,  0.1466,\n",
       "                       0.1434, -0.1705,  0.1796,  0.1631,  0.1620,  0.1361,  0.1393,  0.1443,\n",
       "                      -0.1074, -0.1858,  0.1135, -0.1659, -0.1469, -0.1279,  0.1170, -0.1842,\n",
       "                      -0.0143, -0.1406,  0.1596, -0.1679, -0.1878, -0.1318,  0.1740,  0.0929,\n",
       "                       0.2011,  0.1377,  0.0921, -0.1536, -0.0666,  0.1413,  0.1363, -0.1465],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm3.gamma',\n",
       "              tensor([0.9497, 1.1933, 1.1758, 1.2063, 0.8217, 0.8328, 1.1771, 0.9939, 0.8003,\n",
       "                      0.8027, 1.1595, 1.0586, 0.8112, 0.9172, 1.2094, 1.2064, 1.1970, 1.2034,\n",
       "                      1.1878, 1.2037, 0.8060, 1.2028, 0.7904, 0.8298, 0.9468, 0.7968, 0.8322,\n",
       "                      0.8177, 0.8172, 1.0721, 1.1932, 1.1512, 0.7958, 1.0276, 1.1913, 1.1687,\n",
       "                      1.0242, 1.2126, 1.1679, 0.8076, 0.8714, 1.0429, 0.8035, 1.1875, 1.1961,\n",
       "                      1.0567, 1.1724, 1.0754, 0.9820, 0.8011, 1.0602, 0.7914, 0.7946, 0.8218,\n",
       "                      1.1302, 0.8242, 1.1948, 0.8219, 1.0644, 0.8303, 1.1834, 1.1950, 1.1486,\n",
       "                      1.0370, 1.1733, 1.2109, 1.0655, 1.1898, 0.8368, 0.8088, 1.1850, 1.0564,\n",
       "                      1.1045, 0.8303, 0.7941, 0.8205, 1.1881, 1.2109, 1.1987, 0.7960, 0.9644,\n",
       "                      0.8093, 1.0655, 0.8223, 0.8845, 0.9484, 1.2301, 0.9529, 1.0202, 0.8285,\n",
       "                      0.8268, 1.0386, 1.0779, 1.1729, 0.8153, 1.2028, 1.2034, 1.0818, 0.8123,\n",
       "                      1.1883, 1.1859, 1.1986, 1.1790, 1.2173, 0.8103, 0.8057, 1.1865, 1.1933,\n",
       "                      0.8170, 0.8281, 0.8155, 0.8207, 0.7645, 1.0400, 1.1595, 1.1770, 0.7964,\n",
       "                      0.8139, 0.8104, 0.8179, 1.1789, 0.8214, 1.1794, 0.8325, 1.1834, 0.9556,\n",
       "                      1.1874, 1.1830, 1.1840, 0.8108, 1.0473, 1.1758, 0.7888, 1.0527, 1.1920,\n",
       "                      0.9686, 1.1482, 1.2016, 0.7991, 0.9221, 1.1732, 0.7981, 1.1802, 1.1860,\n",
       "                      0.7984, 1.1813, 1.2017, 0.7903, 1.1744, 1.1701, 0.8065, 0.7885, 1.0340,\n",
       "                      1.2304, 0.8229, 0.9385, 0.8286, 0.9298, 1.2221, 1.1903, 0.8193, 0.7964,\n",
       "                      0.8139, 0.8024, 1.2092, 0.8242, 0.8335, 0.9600, 0.9480, 0.8162, 0.8366,\n",
       "                      0.9272, 1.2084, 1.1616, 0.9256, 1.1650, 0.7791, 1.1498, 0.8051, 1.1924,\n",
       "                      1.2268, 0.8097, 0.7943, 1.1222, 0.8013, 1.0369, 0.8252, 1.0855, 1.1722,\n",
       "                      1.1773, 0.8077, 1.1906, 1.1948, 0.8299, 1.0807, 1.1811, 1.2066, 1.2258,\n",
       "                      0.8167, 1.2174, 1.0673, 0.8117, 1.1786, 1.1641, 1.1842, 0.7998, 0.7969,\n",
       "                      1.2020, 0.7930, 1.1935, 1.1923, 0.8094, 0.8312, 0.8254, 1.1929, 1.1688,\n",
       "                      1.1992, 0.9355, 1.1449, 1.2089, 0.9356, 0.8141, 0.8019, 1.0845, 1.1939,\n",
       "                      1.1688, 0.9316, 1.1649, 0.9331, 1.1857, 1.1422, 1.0199, 0.8167, 1.1779,\n",
       "                      0.9647, 0.8166, 0.8260, 1.1804, 1.2024, 0.9372, 1.1926, 1.0253, 0.7955,\n",
       "                      0.7995, 1.0242, 1.2080, 0.9809, 0.8071, 1.1397, 0.8001, 0.8005, 1.0071,\n",
       "                      0.8281, 0.9322, 0.7993, 1.1879, 0.8092, 0.9973, 0.8136, 0.8063, 0.9891,\n",
       "                      1.1819, 0.8183, 1.1806, 1.2020, 1.1660, 1.1933, 0.8374, 0.8249, 1.2155,\n",
       "                      0.8551, 1.2189, 0.7906, 0.8077, 0.8248, 1.1346, 1.1610, 0.8006, 1.1879,\n",
       "                      0.8283, 0.8122, 1.0517, 1.1826, 1.1683, 0.8176, 0.8047, 0.9005, 0.7840,\n",
       "                      0.8037, 1.0839, 1.1957, 1.2044, 0.8721, 0.9428, 1.1929, 0.9702, 1.1642,\n",
       "                      0.8476, 0.8202, 1.2010, 0.9155, 1.0199, 0.7931, 0.9611, 0.7948, 0.8317,\n",
       "                      1.0364, 1.1789, 0.9604, 0.9689, 0.9338, 1.2032, 0.8413, 0.7979, 1.1738,\n",
       "                      0.7994, 1.2057, 1.1798, 0.8190, 1.0240, 1.1841, 0.8163, 1.0436, 0.7994,\n",
       "                      0.7995, 0.8337, 0.8040, 1.2034, 0.8097, 0.9465, 0.8197, 1.1406, 0.9160,\n",
       "                      1.0465, 1.1981, 1.1849, 1.1543, 0.8199, 0.8171, 1.1899, 0.8038, 0.8076,\n",
       "                      1.1784, 1.1914, 1.1909, 0.9274, 0.8254, 0.8090, 0.8324, 0.8115, 0.7979,\n",
       "                      0.8684, 1.1156, 0.8099, 1.1763, 0.8243, 0.8001, 0.9554, 1.1940, 1.1716,\n",
       "                      0.9724, 0.7947, 0.9351, 0.8047, 0.9265, 1.1734, 1.1706, 1.1651, 1.1886,\n",
       "                      0.7954, 1.0242, 1.1654, 1.1882, 1.1982, 0.9187, 1.0334, 0.8247, 0.8091,\n",
       "                      0.8230, 1.1753, 1.1687, 1.0317, 1.1873, 0.8184, 0.9273, 1.1993, 0.9570,\n",
       "                      0.8004, 0.8260, 1.1955, 0.9412, 1.1996, 0.8425, 1.2316, 1.2271, 0.8017,\n",
       "                      1.1795, 0.8255, 0.7932, 0.8145, 0.9453, 0.9248, 0.8060, 0.8244, 0.8299,\n",
       "                      0.7972, 0.8432, 1.0593, 1.0352, 0.8288, 1.1858, 0.9264, 0.7869, 1.1785,\n",
       "                      1.1876, 0.7881, 1.1984, 1.2221, 1.1770, 1.0695, 0.8243, 0.9240, 1.1972,\n",
       "                      1.1950, 1.2146, 1.2126, 0.9401, 0.9481, 0.9334, 1.0393, 1.1229, 0.8042,\n",
       "                      1.2109, 1.2000, 1.2127, 1.2145, 0.9673, 0.8137, 1.1988, 0.8262, 0.8116,\n",
       "                      0.9519, 0.8051, 1.2067, 0.8118, 0.8068, 0.8082, 0.8057, 0.9271, 0.7917,\n",
       "                      0.8236, 0.8454, 1.2196, 1.1878, 0.8073, 0.9342, 1.2081, 0.8124, 0.8105,\n",
       "                      0.8076, 1.2163, 1.1810, 0.8175, 0.8362, 1.1693, 1.0447, 0.9803, 0.9951,\n",
       "                      1.0824, 0.7952, 0.7922, 0.8197, 1.2169, 1.1939, 0.8018, 1.2207, 0.9173,\n",
       "                      0.8161, 0.9435, 1.2172, 0.9459, 1.0628, 1.1686, 1.1912, 1.1923, 1.2081,\n",
       "                      0.9275, 0.8128, 0.8307, 1.1755, 1.0864, 1.1718, 0.8099, 0.7944, 0.9264,\n",
       "                      1.0670, 0.8100, 0.8009, 1.0319, 1.1831, 1.2059, 0.7903, 1.0204, 0.9213,\n",
       "                      1.2157, 0.9635, 0.9559, 1.1992, 0.8143, 0.8151, 0.8076, 1.1817],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.4.layer_norm3.beta',\n",
       "              tensor([ 0.0347, -0.2109,  0.0719,  0.1893, -0.0062,  0.0693, -0.1742,  0.0347,\n",
       "                       0.0562, -0.0132,  0.2021, -0.1824,  0.0199, -0.1961,  0.1874,  0.2092,\n",
       "                      -0.1918,  0.1881, -0.1905, -0.1918, -0.0162,  0.1941,  0.0379,  0.0725,\n",
       "                       0.0123,  0.0450, -0.0675,  0.0169, -0.0460, -0.1994, -0.1928,  0.1614,\n",
       "                      -0.0300,  0.1699,  0.2072,  0.1976, -0.1937,  0.1795,  0.1702, -0.0405,\n",
       "                       0.1519,  0.1968, -0.0391,  0.2053,  0.2110, -0.1958, -0.1930,  0.1975,\n",
       "                       0.0032,  0.0482, -0.1816, -0.0359,  0.0840, -0.0873, -0.0919, -0.0384,\n",
       "                      -0.1870, -0.0162,  0.1914, -0.0815,  0.1747, -0.1780, -0.1624, -0.1869,\n",
       "                      -0.1798,  0.2011,  0.0704, -0.1842,  0.0536,  0.0480,  0.1870, -0.0798,\n",
       "                      -0.1797, -0.0599,  0.0050, -0.0777, -0.1923, -0.2039, -0.1884, -0.0711,\n",
       "                      -0.0977,  0.0762, -0.1872,  0.0037, -0.1665,  0.0229, -0.1760,  0.0631,\n",
       "                      -0.0970, -0.0349, -0.0619, -0.1683,  0.1844,  0.2047,  0.0688, -0.1936,\n",
       "                      -0.1844, -0.1835, -0.0652, -0.1737,  0.2092, -0.1948, -0.1851, -0.2079,\n",
       "                       0.0627, -0.1191, -0.2094, -0.2077,  0.0396, -0.0588, -0.0556, -0.0864,\n",
       "                      -0.1167, -0.1867, -0.2030, -0.1847, -0.0290,  0.0648, -0.0487, -0.0579,\n",
       "                       0.1879,  0.1371, -0.2073, -0.0158,  0.1817,  0.1910, -0.2013, -0.1783,\n",
       "                      -0.1955, -0.0358,  0.1971,  0.0685,  0.1038,  0.1926, -0.1751,  0.0800,\n",
       "                       0.1498, -0.1739, -0.1082,  0.0544,  0.1597,  0.0380, -0.1843, -0.1759,\n",
       "                       0.0334, -0.2005, -0.1949, -0.1247, -0.1905, -0.1796,  0.0674, -0.0920,\n",
       "                       0.1985, -0.1984,  0.0707,  0.0041,  0.0714, -0.0116, -0.2083, -0.2100,\n",
       "                      -0.0132,  0.0444,  0.0734, -0.0746,  0.2098, -0.0634,  0.0361, -0.0582,\n",
       "                       0.0806,  0.0498, -0.0779, -0.1745,  0.2032,  0.1680,  0.0685,  0.2068,\n",
       "                       0.0873,  0.1478, -0.0464, -0.2065, -0.2106, -0.0427,  0.0461, -0.0561,\n",
       "                       0.0265,  0.1720, -0.0243,  0.1720, -0.1770,  0.2077, -0.0315,  0.1700,\n",
       "                       0.1875, -0.1477, -0.0855,  0.1804, -0.1879,  0.2067, -0.0720,  0.1943,\n",
       "                      -0.1871, -0.0381, -0.1752, -0.2054, -0.1792,  0.0456,  0.0409, -0.1772,\n",
       "                      -0.0590, -0.1829, -0.2024,  0.0212,  0.0751,  0.0413, -0.1877, -0.1912,\n",
       "                       0.1709, -0.0034,  0.1699, -0.1889,  0.0079,  0.0313,  0.0579,  0.2091,\n",
       "                       0.1859, -0.1868,  0.0613, -0.1846,  0.1871,  0.1729,  0.1519,  0.1843,\n",
       "                      -0.0477,  0.1934, -0.0709,  0.0644,  0.0398, -0.1841,  0.1971,  0.0430,\n",
       "                       0.0775, -0.1989,  0.0045, -0.1207,  0.0682, -0.0622,  0.0309, -0.0155,\n",
       "                       0.1946,  0.0817,  0.0828,  0.0480, -0.0558, -0.1093,  0.0171,  0.1885,\n",
       "                      -0.0340, -0.1904,  0.0314,  0.0671,  0.1760,  0.1993, -0.0551,  0.1698,\n",
       "                      -0.1746,  0.1646,  0.2007,  0.0754, -0.0530, -0.1939,  0.0816, -0.2085,\n",
       "                       0.0161,  0.1093,  0.1035,  0.1878,  0.1746,  0.0066, -0.1784,  0.0743,\n",
       "                      -0.0534, -0.1934, -0.1972,  0.1719,  0.0689,  0.1340, -0.1670, -0.0049,\n",
       "                       0.0514, -0.1419, -0.2043, -0.2007,  0.1758,  0.0323, -0.2033, -0.1832,\n",
       "                      -0.1824, -0.0380, -0.0362,  0.1851,  0.1758,  0.0660,  0.0045,  0.1218,\n",
       "                      -0.0213, -0.0485,  0.0007,  0.1849,  0.1635, -0.1884,  0.1775,  0.1927,\n",
       "                      -0.1516,  0.0796,  0.0643,  0.0607,  0.1918, -0.1991, -0.0739,  0.0744,\n",
       "                       0.1806, -0.0501,  0.0759,  0.0816,  0.0091, -0.0654,  0.0194,  0.1860,\n",
       "                      -0.0651, -0.1981, -0.0597, -0.1185,  0.2009,  0.2052,  0.2044, -0.2036,\n",
       "                       0.1578, -0.0085, -0.0421, -0.2027, -0.0582,  0.0348, -0.1647,  0.2069,\n",
       "                       0.1643, -0.0297,  0.0712,  0.0556,  0.0592,  0.1640, -0.0313, -0.1582,\n",
       "                      -0.0516, -0.0350, -0.1948,  0.0930,  0.0579, -0.1694,  0.1883,  0.1633,\n",
       "                      -0.0560, -0.0904, -0.0109,  0.0214,  0.0413,  0.1749, -0.1805, -0.1984,\n",
       "                       0.1925,  0.0679,  0.1826, -0.1871, -0.2022,  0.2092, -0.1637,  0.0689,\n",
       "                      -0.0789,  0.0399,  0.1427, -0.2116,  0.2019,  0.1874,  0.2084,  0.0556,\n",
       "                      -0.1867, -0.2044,  0.0706,  0.0049, -0.1247,  0.1944, -0.0434, -0.2026,\n",
       "                       0.0903,  0.2015,  0.1951,  0.0158, -0.1903,  0.0562,  0.0108,  0.0239,\n",
       "                       0.0526,  0.1521,  0.0888,  0.0584,  0.0225,  0.0831, -0.0843, -0.1866,\n",
       "                       0.1864, -0.0729, -0.1914,  0.0691,  0.0939,  0.1895, -0.1599,  0.0161,\n",
       "                      -0.1793, -0.1890,  0.2062,  0.1579, -0.0725, -0.0726,  0.0760, -0.1881,\n",
       "                      -0.2054, -0.1883,  0.1704, -0.0502, -0.0307, -0.1049, -0.1331,  0.0245,\n",
       "                       0.1728, -0.2031, -0.1856,  0.1945,  0.0546,  0.0780,  0.2005,  0.0740,\n",
       "                      -0.0623,  0.0144, -0.0397, -0.1889, -0.0699,  0.0691,  0.0419,  0.0799,\n",
       "                      -0.0387,  0.0376, -0.0168, -0.1751,  0.1587, -0.1870,  0.0205, -0.0510,\n",
       "                      -0.1758,  0.0575, -0.0593,  0.0415,  0.1982,  0.1779,  0.0468, -0.0497,\n",
       "                       0.1689, -0.1446,  0.1920, -0.0289, -0.1291, -0.0728, -0.0306, -0.0402,\n",
       "                      -0.1965,  0.1914, -0.0690, -0.1972,  0.0112,  0.0663,  0.0286,  0.2068,\n",
       "                       0.0178, -0.1850,  0.1940,  0.1973,  0.2056,  0.2111, -0.0434, -0.0400,\n",
       "                       0.0606, -0.1754, -0.0499, -0.1634,  0.0391,  0.0443, -0.0671, -0.1829,\n",
       "                       0.0726, -0.0170,  0.1895, -0.1902, -0.1953, -0.0184,  0.1892, -0.0697,\n",
       "                       0.1749,  0.0214, -0.1611, -0.1972,  0.0676,  0.1027, -0.0162, -0.2028],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.self_attention.W_Q.weight',\n",
       "              tensor([[ 0.1939,  0.2203, -0.1654,  ...,  0.0198,  0.1085,  0.1839],\n",
       "                      [ 0.0884, -0.2479, -0.0939,  ..., -0.0253,  0.2053,  0.0494],\n",
       "                      [-0.1147,  0.0485,  0.1046,  ..., -0.1086,  0.0495,  0.1190],\n",
       "                      ...,\n",
       "                      [ 0.0428,  0.2031,  0.1819,  ...,  0.0042, -0.0297,  0.2119],\n",
       "                      [-0.0837,  0.0133,  0.0944,  ..., -0.1650,  0.0113, -0.1357],\n",
       "                      [ 0.1868,  0.2621, -0.2010,  ...,  0.0069, -0.1126,  0.2260]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.self_attention.W_K.weight',\n",
       "              tensor([[ 0.1854,  0.2573,  0.1704,  ...,  0.1708, -0.1131, -0.1123],\n",
       "                      [-0.1495, -0.1895, -0.1552,  ..., -0.2425,  0.1927,  0.0773],\n",
       "                      [ 0.0292,  0.0148, -0.1484,  ..., -0.2095,  0.1780,  0.1678],\n",
       "                      ...,\n",
       "                      [-0.2517, -0.2539,  0.1905,  ...,  0.1008,  0.0283,  0.2014],\n",
       "                      [-0.2405, -0.0393,  0.1073,  ...,  0.0088, -0.1802, -0.0630],\n",
       "                      [-0.0050, -0.2903,  0.0631,  ..., -0.1807,  0.0419,  0.0881]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.self_attention.W_V.weight',\n",
       "              tensor([[ 0.2046,  0.1094,  0.0104,  ..., -0.2220, -0.0126, -0.0030],\n",
       "                      [-0.2338, -0.1423,  0.1009,  ...,  0.0286,  0.1268, -0.1295],\n",
       "                      [-0.0579, -0.0931, -0.1385,  ...,  0.1247,  0.1559, -0.1033],\n",
       "                      ...,\n",
       "                      [ 0.0056, -0.0970, -0.1111,  ...,  0.1404,  0.1164, -0.0343],\n",
       "                      [ 0.2709,  0.2398, -0.1333,  ...,  0.0984,  0.0683, -0.0635],\n",
       "                      [ 0.0856,  0.0102, -0.1211,  ...,  0.1321,  0.1905, -0.0819]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.self_attention.W_T.weight',\n",
       "              tensor([[ 0.2029, -0.0199,  0.1519,  ...,  0.2058, -0.0291,  0.0156],\n",
       "                      [-0.1913, -0.0657, -0.1760,  ..., -0.0870, -0.1933, -0.1533],\n",
       "                      [-0.0670, -0.1743, -0.0419,  ...,  0.0792, -0.0385,  0.0275],\n",
       "                      ...,\n",
       "                      [ 0.0846,  0.2270, -0.2875,  ..., -0.1238, -0.0130, -0.1101],\n",
       "                      [ 0.0148,  0.2044,  0.1244,  ..., -0.0024,  0.1698, -0.2426],\n",
       "                      [ 0.1369,  0.2225, -0.0614,  ..., -0.1123,  0.1732, -0.1131]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm1.gamma',\n",
       "              tensor([1.1497, 0.8379, 0.9434, 1.1857, 0.8157, 0.7278, 1.1585, 1.1684, 1.2026,\n",
       "                      0.7598, 1.1226, 0.9781, 1.2405, 0.8678, 1.1867, 1.1431, 1.1991, 0.8462,\n",
       "                      1.1702, 1.2461, 0.8003, 1.2330, 0.9061, 1.1995, 0.8213, 0.7944, 0.9806,\n",
       "                      1.1442, 1.3250, 0.8034, 1.2104, 1.1680, 0.7981, 1.2490, 0.7618, 1.2810,\n",
       "                      0.8116, 1.1565, 1.2887, 0.9134, 1.1280, 0.8914, 0.7551, 0.7872, 1.2400,\n",
       "                      1.1568, 1.2066, 1.2397, 1.0318, 0.8258, 1.2003, 0.8788, 0.7289, 0.7469,\n",
       "                      1.1628, 0.8852, 1.1504, 0.7774, 0.8219, 0.8049, 1.0290, 1.1069, 0.9333,\n",
       "                      0.8059, 1.2078, 1.2231, 1.2656, 1.1994, 0.7550, 1.0315, 1.2306, 1.1720,\n",
       "                      0.7115, 0.7442, 1.2717, 1.0028, 1.2232, 0.9519, 1.2254, 1.0565, 1.1702,\n",
       "                      1.2217, 1.0716, 0.7431, 1.0381, 1.0950, 0.8489, 1.0442, 0.8584, 1.0641,\n",
       "                      1.2150, 0.9684, 1.0467, 0.7228, 0.8887, 1.2220, 1.2156, 1.0020, 1.1440,\n",
       "                      0.6888, 1.2684, 1.0223, 0.7763, 0.8893, 0.7810, 0.8125, 1.2413, 0.9868,\n",
       "                      0.7406, 0.8987, 1.1556, 0.9083, 1.1854, 0.8265, 1.0547, 1.2321, 0.9833,\n",
       "                      0.7843, 0.7376, 0.8013, 1.1559, 1.0602, 0.8355, 1.2307, 1.0830, 0.7336,\n",
       "                      0.7312, 1.2138, 0.8832, 0.7516, 0.9754, 1.0232, 0.9658, 1.0762, 1.0065,\n",
       "                      0.8071, 1.2760, 0.8674, 0.8053, 0.7836, 0.9572, 0.9351, 0.9163, 0.7865,\n",
       "                      0.7729, 1.0917, 0.9517, 0.9225, 0.8100, 1.0007, 0.9724, 0.8111, 1.0674,\n",
       "                      1.1975, 0.9471, 0.7601, 0.8546, 0.9580, 1.3156, 1.2139, 0.8506, 1.0100,\n",
       "                      1.1967, 1.0022, 1.2151, 0.7644, 0.8114, 1.2871, 1.2405, 1.0307, 0.8179,\n",
       "                      0.9819, 0.7983, 1.2635, 0.7701, 0.7097, 0.7390, 1.2444, 0.7816, 1.1941,\n",
       "                      0.7653, 0.7604, 1.1541, 0.8062, 0.8291, 1.1050, 0.7499, 1.0020, 1.1095,\n",
       "                      1.0094, 0.7832, 1.2618, 1.1969, 0.7288, 0.8229, 1.2691, 1.0989, 0.7596,\n",
       "                      0.8246, 1.0827, 1.1016, 0.8123, 1.2061, 0.7712, 1.1762, 0.7446, 0.8619,\n",
       "                      1.1630, 0.7848, 0.8620, 1.2006, 0.7640, 1.1890, 0.7372, 1.1865, 0.8105,\n",
       "                      1.1255, 0.8635, 1.0848, 1.2540, 0.9058, 1.1996, 1.1404, 1.0351, 0.7985,\n",
       "                      1.2062, 0.8615, 1.1959, 0.7522, 1.1687, 1.1739, 0.7544, 0.9310, 1.2805,\n",
       "                      1.1983, 0.7398, 0.7845, 0.9693, 1.3099, 1.2323, 0.7576, 0.8038, 0.8339,\n",
       "                      1.1755, 1.1661, 0.7972, 0.9263, 0.7913, 1.2842, 0.9886, 0.7463, 1.1682,\n",
       "                      0.9579, 1.1419, 1.2345, 0.9982, 0.7277, 1.0490, 0.8195, 1.0551, 0.7505,\n",
       "                      0.8932, 1.1385, 1.1866, 1.2216, 0.7481, 1.2270, 0.8685, 0.7672, 0.7759,\n",
       "                      0.7853, 1.1576, 0.9330, 0.7704, 1.0097, 0.9235, 0.7805, 0.8112, 1.1025,\n",
       "                      0.7767, 1.1430, 0.9208, 1.1713, 1.1946, 1.0761, 0.7857, 0.9179, 0.9232,\n",
       "                      1.2586, 1.2130, 1.1137, 1.1289, 0.8071, 0.7032, 1.2286, 0.8883, 1.2253,\n",
       "                      1.2672, 0.9267, 0.9347, 0.9016, 1.2205, 0.7497, 0.7180, 0.8493, 1.1951,\n",
       "                      1.0771, 1.2974, 0.8072, 1.1792, 0.7930, 1.0172, 0.7611, 1.0720, 1.2922,\n",
       "                      0.7692, 0.7392, 1.2068, 0.7931, 1.1993, 1.0023, 0.8355, 0.7539, 0.8480,\n",
       "                      0.8122, 0.8636, 1.2285, 1.3089, 0.7461, 0.8720, 1.1151, 1.1288, 1.1059,\n",
       "                      1.0175, 0.9873, 1.0789, 1.1064, 0.8580, 0.8092, 1.0255, 0.7829, 1.2105,\n",
       "                      0.9800, 1.1486, 0.7806, 1.1525, 0.7774, 0.7828, 0.8139, 0.8867, 1.0662,\n",
       "                      1.1844, 1.0947, 0.9380, 0.7566, 0.9436, 0.7788, 1.1421, 1.2099, 0.7500,\n",
       "                      1.1903, 0.9996, 1.2194, 0.7331, 0.8753, 1.2666, 1.2012, 0.9640, 1.0077,\n",
       "                      0.7071, 1.2256, 0.7713, 0.7938, 0.8242, 1.0235, 1.1844, 0.6790, 0.9260,\n",
       "                      1.0150, 0.8611, 1.2490, 0.7453, 0.7518, 0.8215, 0.7646, 1.1672, 0.6993,\n",
       "                      1.2080, 1.0526, 1.2360, 0.7528, 1.1557, 1.0296, 1.2110, 1.0288, 1.0080,\n",
       "                      0.7863, 1.1892, 1.1798, 0.9628, 0.9320, 0.7423, 0.8079, 0.8067, 0.7902,\n",
       "                      0.7687, 0.7347, 1.2045, 1.0561, 0.8326, 0.7565, 0.9254, 0.7566, 0.8626,\n",
       "                      1.1617, 0.7998, 0.8638, 1.0010, 1.1093, 0.7548, 0.7949, 1.0152, 0.8077,\n",
       "                      1.2350, 0.7938, 0.9049, 0.8335, 1.0607, 1.1768, 0.8418, 0.7932, 1.0150,\n",
       "                      0.8174, 0.9757, 0.9990, 1.2187, 0.7726, 1.0512, 1.1787, 1.1515, 0.7855,\n",
       "                      0.8301, 0.7811, 1.1975, 0.8895, 1.3281, 0.7163, 0.8196, 0.8854, 0.9780,\n",
       "                      1.1572, 0.7171, 0.7976, 0.7654, 0.7529, 1.0128, 1.1832, 0.9262, 0.7509,\n",
       "                      1.1398, 1.2738, 1.2257, 0.7714, 0.6540, 1.0054, 1.1748, 0.7421, 1.1673,\n",
       "                      0.9635, 0.8129, 0.7623, 1.2346, 1.2355, 1.2055, 0.8809, 1.2506, 1.1350,\n",
       "                      0.8121, 1.1428, 0.7682, 1.0242, 1.1619, 1.1487, 1.0266, 0.9659, 1.2268,\n",
       "                      1.2322, 0.8093, 1.1818, 1.1733, 1.0745, 1.1743, 0.8215, 1.0070, 1.2744,\n",
       "                      0.8122, 0.7378, 0.8257, 0.9385, 1.2401, 1.2785, 0.8317, 0.7830, 0.9593,\n",
       "                      1.1959, 0.7378, 0.9513, 1.0933, 1.0683, 0.7548, 0.7691, 1.1747],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm1.beta',\n",
       "              tensor([ 1.8272e-01, -1.2787e-01,  2.6223e-02,  2.1620e-01, -3.2311e-02,\n",
       "                      -1.5838e-01, -2.0738e-01, -1.3480e-01,  4.5194e-02,  1.1127e-01,\n",
       "                       1.3491e-01,  1.0287e-01,  2.2768e-01,  2.2191e-01,  1.9462e-01,\n",
       "                       1.8137e-01, -1.7515e-01,  1.3109e-01, -2.1706e-01, -2.7352e-01,\n",
       "                      -8.9312e-02,  2.2491e-01, -2.0910e-01,  1.9906e-01,  8.1397e-02,\n",
       "                       1.1290e-01,  5.8160e-02, -1.2193e-01, -3.0196e-01, -3.2169e-02,\n",
       "                      -2.1744e-01,  5.4956e-02, -4.0633e-02,  2.3300e-01,  4.1290e-02,\n",
       "                       2.6736e-01, -1.3954e-03,  1.8986e-01,  2.8238e-01, -1.8662e-01,\n",
       "                      -1.8741e-01,  1.7615e-01,  5.0891e-02, -1.0335e-01,  2.0279e-01,\n",
       "                      -1.7094e-01, -1.9633e-01,  2.5581e-01,  2.7475e-02,  3.1811e-03,\n",
       "                      -2.2718e-01, -1.7466e-01,  8.0136e-02,  6.3705e-02, -6.5244e-02,\n",
       "                      -1.4980e-01, -2.2017e-01, -1.0710e-01,  3.2457e-02,  3.7867e-02,\n",
       "                       2.1707e-01, -1.0971e-01,  4.7341e-02, -1.4185e-01, -2.0262e-01,\n",
       "                       2.2409e-01,  2.3838e-01, -2.2048e-01,  2.2840e-02,  2.0418e-01,\n",
       "                       2.2240e-01, -2.1013e-01,  1.0319e-01, -1.4934e-01,  2.5036e-01,\n",
       "                      -1.3296e-01, -2.2704e-01,  9.4752e-02, -2.5593e-01,  1.0000e-01,\n",
       "                      -1.5999e-01,  2.3409e-01,  9.4076e-02,  2.9250e-02,  2.2226e-01,\n",
       "                      -2.2015e-01,  2.0435e-01,  1.6285e-01, -1.3421e-01, -1.7142e-01,\n",
       "                      -2.1723e-01, -1.6606e-01,  1.7303e-01, -4.4762e-02,  1.0273e-01,\n",
       "                      -2.3823e-01, -2.5027e-01, -1.3200e-01, -1.7674e-01,  1.1736e-02,\n",
       "                       2.4809e-01, -8.1849e-02,  3.5114e-02, -9.6140e-02,  1.3456e-01,\n",
       "                      -6.7679e-02, -2.4571e-01, -1.1620e-01,  1.2193e-01, -2.1521e-02,\n",
       "                       7.1856e-02,  5.2692e-02, -1.9570e-01,  1.3755e-03, -2.2332e-01,\n",
       "                       1.1363e-01, -2.3998e-01, -4.9090e-02, -5.4267e-02,  6.4461e-02,\n",
       "                       1.9488e-02,  1.9670e-01, -1.5119e-01,  2.5519e-01,  5.3502e-02,\n",
       "                      -5.2137e-02, -4.4372e-02, -2.2160e-01,  7.4944e-02, -2.0653e-05,\n",
       "                       3.8021e-02,  1.2197e-01,  1.1274e-01, -8.6208e-02, -1.2050e-01,\n",
       "                       1.0966e-01,  2.4939e-01,  1.4420e-01,  9.8618e-03, -3.7977e-02,\n",
       "                       7.4847e-02,  9.1574e-02, -1.1068e-01, -7.8967e-02, -7.9629e-02,\n",
       "                      -2.2747e-01,  1.1251e-01, -1.5228e-01,  1.1157e-01, -1.3302e-01,\n",
       "                      -1.1945e-01, -1.7828e-01,  1.8757e-01, -2.3253e-01, -1.5362e-01,\n",
       "                       1.7502e-02,  1.4158e-01,  6.2850e-02, -3.0748e-01, -2.2645e-01,\n",
       "                       1.1139e-01, -2.0827e-01,  2.6051e-01,  9.3681e-02,  2.3217e-01,\n",
       "                      -5.6512e-02,  1.7262e-01, -2.4525e-01,  2.3823e-01,  2.2788e-01,\n",
       "                       1.2424e-02, -1.9406e-01, -1.6646e-01,  2.8820e-01, -4.7575e-02,\n",
       "                      -9.6297e-02, -1.0192e-01,  2.5040e-01,  3.7027e-02, -2.2201e-01,\n",
       "                      -7.0302e-02, -1.1735e-01, -1.8725e-01, -5.5528e-02,  1.5585e-01,\n",
       "                       2.2174e-01, -8.0982e-02, -1.4200e-01, -2.6052e-02,  7.8694e-02,\n",
       "                      -4.7964e-02, -2.4662e-01, -1.7306e-01, -5.6504e-02,  1.0008e-01,\n",
       "                       2.3405e-01, -2.6322e-02, -1.1222e-01, -2.0499e-02,  2.1139e-01,\n",
       "                      -1.4875e-01, -3.2217e-02, -2.4249e-01,  1.0642e-03, -1.8309e-01,\n",
       "                       5.3367e-02,  7.5337e-02, -1.7868e-01, -6.7865e-02,  1.9008e-01,\n",
       "                      -1.5033e-01, -1.2830e-01,  1.9167e-01,  1.3946e-01, -1.6651e-01,\n",
       "                      -8.7468e-02,  1.4119e-01, -4.6227e-02,  1.4964e-01, -2.3877e-01,\n",
       "                      -1.2288e-01, -2.0051e-01,  2.4900e-01,  2.2989e-01, -1.4486e-02,\n",
       "                      -1.8623e-01,  7.4425e-02, -1.0923e-01, -4.8517e-02, -2.1303e-01,\n",
       "                       2.0641e-01, -4.9930e-02, -2.4066e-02,  2.9160e-01, -8.9289e-02,\n",
       "                       6.3458e-02, -9.3142e-02, -2.4898e-01,  2.9649e-01, -2.4124e-01,\n",
       "                       8.6252e-03, -7.3773e-02,  2.8301e-02, -2.0565e-01, -2.1217e-01,\n",
       "                      -1.2802e-01,  3.7584e-02,  1.3893e-01,  2.6015e-01,  2.0977e-01,\n",
       "                       1.6952e-02,  2.0754e-01,  2.0879e-01,  2.6435e-01,  2.3675e-01,\n",
       "                       1.2859e-01, -1.0692e-01,  5.2735e-02, -1.3175e-01,  1.1970e-01,\n",
       "                      -1.1000e-02, -7.4572e-02,  1.5830e-01,  2.0104e-01, -2.2758e-01,\n",
       "                       1.0474e-01,  2.1718e-01, -7.4852e-02, -8.6602e-02, -6.2565e-02,\n",
       "                       4.8900e-02, -1.7783e-01, -1.1542e-01, -8.8323e-02,  1.3540e-01,\n",
       "                      -1.0950e-01, -5.6748e-03,  1.0666e-01, -1.1017e-01,  7.0941e-02,\n",
       "                      -2.1318e-01,  2.1416e-01, -1.6933e-01,  2.2384e-01,  2.2698e-01,\n",
       "                       1.1723e-01, -1.8251e-01,  1.4136e-01,  2.3128e-01, -2.2855e-01,\n",
       "                      -2.0387e-01, -1.1706e-01,  3.1934e-02,  9.9604e-02, -2.1290e-01,\n",
       "                       7.0347e-02, -2.2180e-01,  2.8752e-01, -7.7995e-02, -5.7284e-02,\n",
       "                       1.2583e-01,  2.2277e-01,  8.3677e-02,  3.3740e-02,  1.1721e-01,\n",
       "                      -1.8791e-01,  2.1297e-01, -2.7479e-01, -9.5689e-02, -1.8934e-01,\n",
       "                       6.9223e-02,  1.6126e-01,  4.6337e-02,  2.0768e-01, -2.5723e-01,\n",
       "                      -2.0084e-02,  1.1151e-01, -2.3895e-01, -9.3287e-02,  4.4142e-03,\n",
       "                       1.4727e-01, -1.8249e-02,  8.3503e-02,  1.4633e-01, -1.0667e-01,\n",
       "                       1.5197e-01, -8.3800e-02,  2.8269e-01, -7.5163e-02,  1.2143e-01,\n",
       "                       1.2781e-01, -1.3580e-01, -2.0705e-01,  2.0952e-01,  1.3984e-01,\n",
       "                      -1.6258e-01,  1.3582e-01,  1.2490e-01,  1.9131e-04, -8.7471e-02,\n",
       "                      -4.5870e-02, -2.3806e-01, -7.8977e-02,  2.1104e-01, -1.1383e-02,\n",
       "                       1.7544e-01,  7.5342e-02, -7.7205e-02, -1.8880e-01,  1.5806e-01,\n",
       "                      -5.5747e-03, -2.0642e-01,  2.3611e-01,  1.6869e-01,  1.7350e-02,\n",
       "                      -2.3561e-01,  1.5132e-01, -1.8075e-01,  2.1286e-01,  7.4652e-02,\n",
       "                       2.2239e-01,  1.3693e-01, -1.9872e-01, -2.1260e-02,  2.3531e-01,\n",
       "                       2.4391e-01, -2.3357e-01, -1.7818e-01,  2.4527e-01, -8.4937e-02,\n",
       "                      -2.4397e-01, -5.2173e-02,  1.3560e-02, -2.1293e-02, -1.6817e-01,\n",
       "                      -2.4623e-01, -1.5677e-01,  6.2590e-02, -2.2331e-01, -1.1756e-01,\n",
       "                       2.3726e-01,  1.0633e-02, -5.1857e-02,  8.4221e-02,  5.3284e-02,\n",
       "                      -1.4883e-01,  1.3253e-02,  2.0581e-01,  2.8546e-01,  2.1659e-01,\n",
       "                      -8.6640e-02, -1.2315e-01,  1.5912e-01,  2.2823e-01, -1.8923e-01,\n",
       "                      -1.3671e-01, -4.5919e-02,  2.4798e-01,  5.9234e-02,  2.7323e-02,\n",
       "                       1.3842e-01, -1.0726e-01,  1.2516e-01, -8.8601e-02,  1.0322e-01,\n",
       "                      -2.5955e-02, -4.8995e-02,  1.9960e-01,  2.0298e-01,  1.3552e-01,\n",
       "                       1.8792e-02, -3.9736e-02, -2.7467e-02, -1.0116e-01, -1.9843e-01,\n",
       "                      -1.1710e-03,  1.1148e-01, -1.8420e-01,  1.9049e-01,  7.4159e-02,\n",
       "                      -1.7392e-02, -1.7898e-01, -1.3192e-01,  2.4006e-01,  2.4455e-02,\n",
       "                      -4.3959e-02, -2.6722e-03,  1.0034e-01,  1.9324e-01, -2.7615e-01,\n",
       "                       1.0801e-02,  2.0070e-01,  4.3955e-02, -1.1032e-01, -1.1792e-01,\n",
       "                       2.1283e-01,  5.9773e-02,  2.3129e-01,  2.2770e-01,  1.9927e-01,\n",
       "                       9.9187e-02, -2.7543e-02, -7.5674e-02, -2.1826e-01, -9.1627e-02,\n",
       "                      -2.9996e-01,  8.2655e-02,  3.0538e-02, -1.5083e-01,  1.7388e-01,\n",
       "                       1.8441e-01, -1.6877e-01, -5.4535e-02,  1.4329e-01,  1.5936e-01,\n",
       "                      -2.0780e-01, -2.3529e-01,  1.8494e-01, -8.7241e-02, -1.4392e-02,\n",
       "                       2.7412e-01,  2.2912e-01, -9.5234e-02, -1.2591e-01,  1.7186e-01,\n",
       "                       1.7645e-01, -9.0011e-02, -1.7173e-01, -4.7246e-02, -1.4483e-01,\n",
       "                      -4.4970e-02,  2.4590e-01, -2.4546e-01,  1.8972e-01, -2.4762e-01,\n",
       "                      -2.6801e-01, -1.5448e-01,  3.2174e-02, -2.0957e-01,  1.2658e-01,\n",
       "                       5.6929e-02, -2.0080e-01,  1.6103e-01,  2.2234e-01,  3.7291e-02,\n",
       "                       2.5591e-01,  2.3801e-01, -4.2701e-03,  2.0738e-01, -1.7812e-01,\n",
       "                      -1.7762e-01,  1.2314e-01,  1.0087e-01, -1.4814e-01,  2.9343e-01,\n",
       "                       6.2754e-02,  9.8516e-02, -1.3937e-01, -1.8408e-01, -2.3462e-01,\n",
       "                      -2.4538e-01, -1.1605e-01, -4.4421e-03,  5.2244e-02,  2.0597e-01,\n",
       "                      -1.0387e-01, -1.6085e-01, -1.3810e-01,  2.3678e-01, -7.1618e-02,\n",
       "                       2.0020e-03, -1.7498e-01], device='cuda:0')),\n",
       "             ('decoder.layers.5.enc_dec_attention.W_Q.weight',\n",
       "              tensor([[ 0.0730,  0.2884, -0.1029,  ..., -0.1028,  0.1331, -0.0218],\n",
       "                      [ 0.0881, -0.0438,  0.2052,  ..., -0.2516, -0.2370,  0.0166],\n",
       "                      [ 0.1509, -0.0106, -0.1463,  ..., -0.0240,  0.0291, -0.2241],\n",
       "                      ...,\n",
       "                      [ 0.1476,  0.1063, -0.2097,  ...,  0.1092,  0.0950, -0.1602],\n",
       "                      [ 0.1980, -0.0904, -0.1819,  ...,  0.1801,  0.1501, -0.2357],\n",
       "                      [ 0.0355,  0.2093,  0.1730,  ..., -0.2296, -0.1804,  0.1356]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.enc_dec_attention.W_K.weight',\n",
       "              tensor([[-0.1211, -0.0434, -0.1429,  ..., -0.1630, -0.0421, -0.1325],\n",
       "                      [ 0.3724,  0.2493, -0.0109,  ...,  0.0805, -0.0731,  0.0742],\n",
       "                      [-0.4103, -0.1387, -0.1210,  ..., -0.1235,  0.2412, -0.1133],\n",
       "                      ...,\n",
       "                      [ 0.1327, -0.0092,  0.0811,  ..., -0.0190,  0.0519, -0.0619],\n",
       "                      [-0.1422,  0.2739,  0.2427,  ...,  0.0709, -0.1169,  0.4013],\n",
       "                      [-0.0927, -0.3118, -0.0834,  ..., -0.0722,  0.1606, -0.2337]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.enc_dec_attention.W_V.weight',\n",
       "              tensor([[ 6.3840e-02, -2.1054e-01, -2.4892e-01,  ..., -2.3782e-01,\n",
       "                        3.2400e-03,  4.7399e-02],\n",
       "                      [ 3.6285e-01,  1.0948e-01,  1.0905e-01,  ..., -1.3021e-01,\n",
       "                       -3.2784e-01,  2.3063e-03],\n",
       "                      [-7.6722e-02, -1.1920e-01, -1.7048e-01,  ..., -1.5871e-01,\n",
       "                        2.2420e-01,  7.7792e-02],\n",
       "                      ...,\n",
       "                      [-2.2874e-01, -1.1651e-01, -2.0189e-01,  ...,  5.8729e-02,\n",
       "                        2.3534e-01, -2.8007e-02],\n",
       "                      [ 6.3847e-02,  2.6618e-01,  3.7666e-01,  ...,  2.8550e-01,\n",
       "                       -3.2407e-01, -1.9717e-01],\n",
       "                      [ 1.0318e-01, -3.5291e-02,  1.3552e-04,  ..., -2.7666e-01,\n",
       "                       -1.3894e-01,  2.5135e-01]], device='cuda:0')),\n",
       "             ('decoder.layers.5.enc_dec_attention.W_T.weight',\n",
       "              tensor([[-0.0897, -0.2860,  0.1901,  ...,  0.2341, -0.2798,  0.0521],\n",
       "                      [ 0.0336,  0.1043, -0.2938,  ..., -0.1026,  0.2523, -0.1522],\n",
       "                      [ 0.0080,  0.0104, -0.0235,  ..., -0.0031,  0.0107, -0.1335],\n",
       "                      ...,\n",
       "                      [-0.1895,  0.0544, -0.0802,  ..., -0.0468,  0.1145,  0.1069],\n",
       "                      [-0.1862,  0.0174, -0.1112,  ...,  0.0399,  0.0707,  0.1035],\n",
       "                      [ 0.2392,  0.1182, -0.0620,  ..., -0.0862,  0.1943,  0.1067]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm2.gamma',\n",
       "              tensor([1.2120, 1.2102, 0.4937, 1.0330, 0.6927, 1.1005, 1.0606, 1.1793, 0.9637,\n",
       "                      0.9761, 1.1761, 1.0596, 1.1879, 0.7298, 1.1199, 1.1593, 1.0775, 0.8151,\n",
       "                      1.1279, 1.2850, 0.7504, 1.1578, 1.0081, 0.9801, 0.5389, 0.7749, 0.7483,\n",
       "                      0.6648, 1.2641, 0.6685, 1.1683, 0.7456, 0.6937, 1.1009, 0.6654, 1.2708,\n",
       "                      0.7372, 1.1020, 1.2819, 1.1226, 1.0316, 0.9119, 0.6051, 0.9658, 1.0358,\n",
       "                      0.9839, 1.0731, 1.0573, 0.9268, 0.5997, 0.9075, 1.0746, 0.7227, 0.7827,\n",
       "                      1.2152, 0.9438, 1.1972, 0.8572, 1.0193, 0.8617, 0.9429, 0.7289, 0.6453,\n",
       "                      1.1013, 1.1649, 1.1900, 1.1001, 1.2089, 0.5283, 1.0389, 1.0837, 1.0746,\n",
       "                      0.9099, 0.8451, 1.2097, 1.1507, 1.2239, 0.9865, 1.0605, 0.8482, 1.0258,\n",
       "                      0.9334, 0.6125, 0.7231, 1.0695, 1.2798, 0.9323, 1.1113, 0.7929, 1.2326,\n",
       "                      1.1338, 0.9643, 1.0616, 1.1348, 0.9829, 1.0918, 1.0481, 1.2602, 1.1096,\n",
       "                      0.4576, 1.1394, 0.5653, 0.7400, 0.7080, 1.0660, 1.1362, 1.2181, 1.0060,\n",
       "                      0.8462, 0.7317, 1.0749, 0.4480, 1.1704, 0.5452, 1.0962, 1.0351, 1.0129,\n",
       "                      0.7120, 0.5193, 0.8203, 1.1479, 0.9850, 1.1862, 1.1771, 0.7949, 0.8081,\n",
       "                      0.7607, 1.0522, 1.0102, 0.5473, 0.7525, 0.9616, 1.1533, 0.8399, 0.6677,\n",
       "                      0.8001, 1.1077, 1.1256, 0.7212, 0.7794, 0.7820, 0.6745, 1.1418, 0.7293,\n",
       "                      0.6317, 1.1265, 1.1402, 1.1821, 0.9838, 1.2168, 1.0829, 1.1801, 0.9678,\n",
       "                      1.1762, 0.9992, 0.3965, 0.8928, 0.7822, 1.1749, 1.0746, 1.2142, 1.1661,\n",
       "                      1.2262, 0.7862, 1.1972, 0.4903, 0.8703, 1.1331, 1.1201, 1.0786, 0.4719,\n",
       "                      1.1031, 0.9238, 0.9937, 0.7995, 0.9516, 0.8765, 1.0178, 0.6640, 1.1733,\n",
       "                      1.0093, 1.0982, 0.7069, 0.8057, 0.8546, 1.2218, 0.5826, 1.0188, 0.5408,\n",
       "                      0.7198, 0.5152, 1.0662, 0.9993, 0.8012, 0.9575, 1.1305, 0.6088, 0.8549,\n",
       "                      0.7274, 1.1427, 1.1475, 0.5521, 1.1882, 0.6801, 1.1941, 0.5242, 0.9597,\n",
       "                      1.1596, 0.6240, 0.9139, 1.0914, 1.0282, 1.1149, 0.8301, 1.1545, 0.7508,\n",
       "                      0.9066, 0.6309, 1.0911, 1.1360, 1.1422, 1.1620, 1.2845, 1.0372, 0.6348,\n",
       "                      1.1508, 0.6131, 0.9566, 0.4848, 1.2167, 0.8690, 0.5826, 0.5903, 1.2574,\n",
       "                      0.6177, 0.7924, 1.0621, 0.9720, 0.9859, 0.9406, 0.7278, 0.8421, 0.7465,\n",
       "                      0.9840, 1.0247, 1.0725, 0.8416, 1.0245, 1.1794, 1.0498, 0.5484, 1.2679,\n",
       "                      1.0333, 1.2370, 1.2361, 0.9180, 0.6951, 1.0978, 1.0266, 0.6231, 0.5298,\n",
       "                      0.6035, 1.0570, 1.1326, 1.1194, 0.7360, 1.1724, 1.0599, 0.7236, 0.7530,\n",
       "                      0.5464, 1.2148, 1.0837, 0.8065, 0.9882, 0.9219, 0.7363, 0.9261, 0.9943,\n",
       "                      0.7972, 1.1146, 0.8819, 1.0628, 1.1611, 1.2089, 1.0092, 1.1138, 1.0903,\n",
       "                      1.1785, 1.0620, 1.1155, 1.1256, 0.5210, 0.6211, 1.2427, 0.9593, 1.2668,\n",
       "                      0.9293, 1.0192, 0.8312, 0.9821, 1.1355, 0.3764, 0.5155, 0.8202, 1.0558,\n",
       "                      1.1256, 0.9481, 0.7296, 1.1696, 0.4699, 1.1636, 0.5482, 1.0122, 0.9682,\n",
       "                      0.5902, 0.7715, 1.1366, 0.7946, 0.9211, 1.1841, 0.6286, 0.5537, 0.9261,\n",
       "                      0.8363, 1.0261, 1.0874, 1.1319, 0.8613, 0.9384, 1.1245, 1.0868, 1.0941,\n",
       "                      1.0980, 0.9003, 0.7007, 1.0507, 1.0157, 0.5920, 0.7584, 0.5223, 1.1319,\n",
       "                      0.4559, 1.1547, 0.8777, 1.2106, 0.6280, 0.8215, 1.0796, 0.9308, 0.5412,\n",
       "                      1.1639, 1.0761, 1.1244, 0.4903, 1.1854, 0.8861, 1.0205, 1.0931, 0.7798,\n",
       "                      1.1294, 1.0226, 1.1382, 0.5207, 1.0131, 1.2415, 1.1554, 0.9897, 1.0646,\n",
       "                      0.6869, 1.0993, 0.6223, 0.7672, 0.5021, 1.1827, 1.0183, 0.8730, 0.7254,\n",
       "                      1.1171, 0.8499, 1.1170, 0.4568, 0.4328, 0.8134, 0.7222, 0.8425, 0.8050,\n",
       "                      1.0975, 1.0336, 1.1690, 0.7939, 0.8786, 0.9092, 1.1729, 1.0484, 1.1691,\n",
       "                      0.6011, 1.0939, 1.0934, 1.2367, 1.1579, 1.0939, 0.8083, 0.7989, 0.9000,\n",
       "                      0.5268, 0.6062, 0.9553, 1.1149, 0.9624, 0.6598, 0.7189, 0.4718, 0.8377,\n",
       "                      1.0200, 0.6836, 0.9272, 1.0817, 1.3309, 0.9018, 0.4122, 1.1213, 1.0593,\n",
       "                      1.1030, 1.0939, 1.2102, 0.9626, 0.6073, 1.2128, 1.0129, 0.6882, 1.1419,\n",
       "                      0.6234, 1.0345, 0.8442, 1.0992, 0.6640, 1.0557, 0.9551, 1.1331, 0.8996,\n",
       "                      0.8104, 1.1866, 1.1416, 0.7349, 0.9903, 0.4781, 0.7720, 0.9861, 1.1471,\n",
       "                      1.1663, 0.9809, 0.4512, 1.1109, 1.0124, 1.0079, 0.9841, 1.1602, 0.8587,\n",
       "                      0.3866, 1.1547, 1.1655, 0.5806, 0.7551, 1.1710, 0.8617, 0.5634, 1.1955,\n",
       "                      0.8352, 0.8276, 0.4243, 1.2405, 1.1755, 1.1578, 0.9776, 1.0381, 1.1729,\n",
       "                      0.9185, 1.1545, 0.8631, 1.1361, 1.2653, 1.1743, 0.7824, 0.7783, 1.0496,\n",
       "                      1.1952, 0.6601, 1.1497, 1.1494, 1.1050, 1.0452, 0.8743, 1.0489, 1.1976,\n",
       "                      0.7921, 0.8047, 0.8077, 1.2576, 1.1521, 1.1638, 0.8758, 0.8254, 0.4044,\n",
       "                      1.1363, 0.8459, 1.0465, 1.0323, 1.1492, 0.8068, 0.5868, 1.1426],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm2.beta',\n",
       "              tensor([ 0.2850, -0.2155, -0.0341,  0.1298, -0.0550, -0.2037, -0.1989, -0.1570,\n",
       "                      -0.0287,  0.0989,  0.1658,  0.1235,  0.2512,  0.1106,  0.1601,  0.1659,\n",
       "                      -0.1415,  0.1072, -0.1640, -0.3038, -0.0600,  0.0724, -0.1453,  0.0976,\n",
       "                      -0.1172,  0.1278,  0.0370, -0.0778, -0.3825, -0.0490, -0.1757,  0.0557,\n",
       "                      -0.0535,  0.1539, -0.0907,  0.3337, -0.0504,  0.1379,  0.3024, -0.1366,\n",
       "                      -0.1489,  0.1617, -0.0455, -0.1664,  0.0821, -0.1685, -0.1343,  0.1834,\n",
       "                       0.0603, -0.1381, -0.1728, -0.1642,  0.1375,  0.0766, -0.2153, -0.0194,\n",
       "                      -0.2339, -0.1264, -0.0979,  0.1065,  0.1228, -0.1714, -0.0350, -0.2594,\n",
       "                      -0.1580,  0.1790,  0.0990, -0.2486,  0.0877,  0.1915,  0.1990, -0.1341,\n",
       "                       0.2122, -0.1661,  0.2590, -0.1729, -0.2139,  0.1544, -0.1772,  0.1198,\n",
       "                      -0.0964,  0.1315, -0.0260,  0.0437,  0.1191, -0.3852,  0.3028,  0.1745,\n",
       "                      -0.0949, -0.2069, -0.1784, -0.1656,  0.1801,  0.0510,  0.1518, -0.1943,\n",
       "                      -0.1821, -0.2615, -0.1228, -0.0548,  0.1784, -0.0276, -0.0534, -0.2009,\n",
       "                       0.1443, -0.1587, -0.2559, -0.0504,  0.1220, -0.1414,  0.0732, -0.0637,\n",
       "                      -0.1603,  0.0658, -0.1590,  0.1009, -0.1639, -0.0698, -0.1192,  0.1196,\n",
       "                       0.0389,  0.2050, -0.2085,  0.2033,  0.0327, -0.1454, -0.0533, -0.0884,\n",
       "                       0.1687, -0.1219,  0.1560,  0.1646,  0.1431, -0.1365, -0.0265,  0.1262,\n",
       "                       0.2034,  0.2188,  0.0198, -0.1108,  0.0771,  0.1531, -0.1453, -0.1503,\n",
       "                      -0.1632, -0.2719,  0.2714, -0.1844,  0.1188, -0.0691, -0.1778, -0.2798,\n",
       "                       0.1393, -0.1747, -0.0946, -0.0121,  0.1686,  0.0461, -0.2149, -0.1439,\n",
       "                       0.2489, -0.2288,  0.2517,  0.1558,  0.2121, -0.0899,  0.2790, -0.2052,\n",
       "                       0.2414,  0.1868, -0.1315, -0.2473, -0.3453,  0.2327, -0.0639, -0.1522,\n",
       "                      -0.1648,  0.0782,  0.0671, -0.2184, -0.0924, -0.1312, -0.0554, -0.1351,\n",
       "                       0.1414,  0.2222, -0.1551, -0.1584,  0.0225, -0.0202, -0.1767, -0.1377,\n",
       "                      -0.0163, -0.0885,  0.1204,  0.2008, -0.0523, -0.0697,  0.0325,  0.2231,\n",
       "                      -0.1986, -0.1019, -0.2168, -0.0571, -0.2416,  0.1116,  0.0896, -0.1636,\n",
       "                      -0.0572,  0.1369, -0.1469, -0.1675,  0.1662,  0.1368, -0.1482, -0.0405,\n",
       "                      -0.0037, -0.0464,  0.1999, -0.1677, -0.2108, -0.1919,  0.3084,  0.2063,\n",
       "                      -0.0366, -0.1864,  0.1131, -0.0582, -0.0844, -0.2278,  0.1345, -0.1208,\n",
       "                      -0.1459,  0.2592, -0.0463,  0.1588, -0.0457, -0.1985,  0.1346, -0.1520,\n",
       "                      -0.0683, -0.1033, -0.0578, -0.1182, -0.1213, -0.1618, -0.0353,  0.2520,\n",
       "                       0.3308,  0.1304,  0.0193,  0.1412,  0.2071,  0.2945,  0.2437,  0.0595,\n",
       "                      -0.1369,  0.1708, -0.2531, -0.0249, -0.0857, -0.1137,  0.1177,  0.2201,\n",
       "                      -0.1794, -0.0236,  0.1933, -0.2023, -0.1332, -0.0481, -0.0130, -0.2318,\n",
       "                      -0.1464, -0.1465,  0.2931, -0.1238, -0.0702,  0.2090, -0.1838,  0.1407,\n",
       "                      -0.1622,  0.0238, -0.1317,  0.1975,  0.1904,  0.2351, -0.1819,  0.1369,\n",
       "                       0.1842, -0.1521, -0.1851, -0.1321, -0.0974,  0.1128, -0.2681, -0.0561,\n",
       "                      -0.2857,  0.1120, -0.0938, -0.1409,  0.1953,  0.1605, -0.0082, -0.0940,\n",
       "                       0.1053, -0.1751,  0.2529, -0.1976, -0.1044, -0.1884, -0.1004,  0.1885,\n",
       "                       0.0732,  0.2079, -0.2891, -0.1479,  0.1968, -0.1787, -0.1234, -0.1077,\n",
       "                       0.2045, -0.1309,  0.1143,  0.1325, -0.0983,  0.1201, -0.1122,  0.1744,\n",
       "                      -0.1088,  0.1569,  0.1375, -0.1787, -0.1764,  0.1330,  0.1792, -0.0695,\n",
       "                       0.0888,  0.2336, -0.1147, -0.0321, -0.0946, -0.1849, -0.1268,  0.2371,\n",
       "                      -0.0931,  0.2022,  0.0985, -0.1020, -0.1963,  0.1972,  0.0443, -0.1671,\n",
       "                       0.1991,  0.3098, -0.0610, -0.3064,  0.1904, -0.1184,  0.1858,  0.1296,\n",
       "                       0.2177,  0.1438, -0.1618, -0.1351,  0.3103,  0.2686, -0.2462, -0.0655,\n",
       "                       0.2195, -0.1600, -0.1792, -0.0937, -0.0268, -0.0855, -0.2020, -0.1679,\n",
       "                      -0.1053,  0.0067, -0.3175, -0.1203,  0.1771, -0.0966, -0.0840,  0.1259,\n",
       "                      -0.0629, -0.0953, -0.1094,  0.1443,  0.2760,  0.1982, -0.1398, -0.1154,\n",
       "                       0.1305,  0.2400, -0.1231, -0.1594, -0.1590,  0.2593,  0.0147, -0.1085,\n",
       "                       0.2432, -0.1828,  0.1067, -0.1207,  0.1520, -0.1038, -0.1065,  0.0915,\n",
       "                       0.2476,  0.1725, -0.0440,  0.0622, -0.0638, -0.1467, -0.0619, -0.0554,\n",
       "                       0.1532, -0.1512,  0.3498,  0.2667, -0.1201, -0.1731, -0.1045,  0.1360,\n",
       "                       0.1746, -0.1661,  0.0685,  0.0825,  0.2279, -0.1769, -0.0469,  0.1686,\n",
       "                      -0.0410, -0.2679, -0.1408,  0.1375,  0.0440,  0.1742,  0.0920,  0.1411,\n",
       "                       0.0907, -0.0928, -0.2114, -0.2059, -0.1984, -0.2175,  0.0219, -0.0831,\n",
       "                      -0.1926,  0.1935,  0.1747, -0.1612, -0.1241,  0.2145,  0.2439, -0.1391,\n",
       "                      -0.0737,  0.1625, -0.1865, -0.0196,  0.1748,  0.2014, -0.0761, -0.1582,\n",
       "                       0.1944,  0.0938, -0.1198, -0.1844, -0.1218, -0.1171, -0.0949,  0.2586,\n",
       "                      -0.1731,  0.1591, -0.2365, -0.1515, -0.1543, -0.0596, -0.1603,  0.1000,\n",
       "                       0.0677, -0.2648,  0.1337, -0.1646, -0.1282,  0.1158,  0.2354, -0.0097,\n",
       "                       0.1782, -0.1737, -0.1388,  0.0018, -0.0472, -0.1237,  0.3374,  0.1192,\n",
       "                       0.1456, -0.1220, -0.2853, -0.1701, -0.1915, -0.1138, -0.1499,  0.0028,\n",
       "                       0.1814, -0.1642, -0.2156, -0.1082,  0.2143, -0.1200, -0.1130, -0.1743],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.ffn.layer1.weight',\n",
       "              tensor([[ 0.2987, -0.2929,  0.1929,  ..., -0.1005, -0.1068, -0.3288],\n",
       "                      [ 0.3166, -0.3031,  0.1704,  ..., -0.2192, -0.1059, -0.3196],\n",
       "                      [ 0.1956,  0.1204, -0.1190,  ...,  0.1502,  0.1819,  0.1668],\n",
       "                      ...,\n",
       "                      [ 0.0101,  0.0728, -0.1205,  ...,  0.0782,  0.1769,  0.0975],\n",
       "                      [-0.0370,  0.0987, -0.1468,  ...,  0.1573,  0.1684,  0.0680],\n",
       "                      [-0.0762, -0.0966, -0.1251,  ..., -0.0911, -0.1059,  0.1410]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.ffn.layer1.bias',\n",
       "              tensor([ 0.2476,  0.3744, -0.1508,  ..., -0.0038, -0.0272, -0.0743],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.ffn.layer2.weight',\n",
       "              tensor([[ 0.0956,  0.0513,  0.2180,  ...,  0.2327,  0.1677,  0.1414],\n",
       "                      [-0.1207,  0.0365, -0.1185,  ..., -0.0814, -0.0187,  0.1553],\n",
       "                      [ 0.0321, -0.0757, -0.0574,  ...,  0.0949,  0.0141,  0.0919],\n",
       "                      ...,\n",
       "                      [ 0.1608,  0.1530, -0.2112,  ..., -0.1173,  0.0716, -0.0721],\n",
       "                      [ 0.1315,  0.0733,  0.1009,  ..., -0.0555,  0.1044, -0.1118],\n",
       "                      [-0.0395,  0.0338, -0.1738,  ..., -0.1561, -0.1891, -0.1154]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.ffn.layer2.bias',\n",
       "              tensor([ 1.5565e-01,  2.0562e-01,  4.1882e-02,  5.0435e-02, -5.9576e-01,\n",
       "                      -1.3782e-02, -6.0227e-02, -2.0151e-01,  1.6974e-02, -4.7002e-02,\n",
       "                       2.1843e-01,  1.7359e-02,  2.0828e-01,  1.5396e-02,  2.1148e-01,\n",
       "                       1.2265e-01, -2.9672e-02, -9.3939e-02, -1.7591e-01, -6.8377e-02,\n",
       "                      -6.8675e-01,  1.7080e-01, -1.3691e-01,  2.2727e-01, -1.4813e-01,\n",
       "                       7.2986e-01,  1.2498e-01,  5.0869e-02, -1.6995e-01,  4.8167e-02,\n",
       "                      -2.1100e-01,  9.4047e-02,  4.6586e-02,  1.6060e-01,  8.2169e-01,\n",
       "                       2.1135e-01, -5.9171e-01,  1.2282e-01,  5.3973e-02, -7.2130e-02,\n",
       "                      -5.2096e-02,  6.9822e-01,  4.6013e-03, -1.7291e-01, -1.4333e-01,\n",
       "                      -2.3989e-01, -2.1993e-01, -2.1334e-01, -4.0425e-02,  2.4631e-03,\n",
       "                      -2.0418e-01, -1.8983e-01, -2.0146e-01, -1.3900e-01, -1.9041e-01,\n",
       "                      -4.3758e-02,  3.4141e-02, -4.9872e-02,  2.7708e-02, -4.3807e-02,\n",
       "                       1.6808e-01, -1.9186e-01,  6.7606e-02, -8.2175e-02, -2.0579e-01,\n",
       "                       1.7577e-01,  1.7475e-01, -5.8151e-02,  3.7440e-02,  3.4034e-02,\n",
       "                      -4.8079e-03, -1.9398e-01,  6.0888e-02, -1.8096e-01,  5.7859e-02,\n",
       "                      -2.1302e-01, -2.5441e-01, -9.6547e-04, -1.8663e-01,  2.4050e-02,\n",
       "                      -2.9874e-03,  1.3035e-01,  2.1125e-01, -8.1807e-02,  2.0794e-01,\n",
       "                      -1.9682e-01,  7.8421e-02,  2.0967e-01,  2.7320e-02, -2.1255e-01,\n",
       "                      -1.6920e-01, -4.6227e-02,  1.5750e-02,  2.1899e-01,  4.1713e-02,\n",
       "                      -1.8595e-01, -1.8404e-01, -2.1867e-01,  5.1559e-02,  1.8039e-01,\n",
       "                       2.1241e-01, -1.2479e-01, -7.3967e-01, -2.0845e-01,  2.0254e-01,\n",
       "                      -1.7705e-01, -1.6146e-01,  2.1815e-02,  2.5812e-01,  1.1450e-01,\n",
       "                       5.2982e-03, -2.6517e-02, -2.2642e-01,  1.0756e-01, -2.0845e-01,\n",
       "                       4.0107e-01,  1.0284e-01,  1.0546e-02, -7.1251e-02,  2.2378e-01,\n",
       "                       4.4813e-01,  2.5366e-01, -7.3551e-02,  7.0841e-02,  4.3812e-02,\n",
       "                      -2.1109e-01, -1.3271e-01,  3.8095e-02,  1.9907e-01, -6.7953e-02,\n",
       "                       1.7615e-01,  4.8456e-02, -4.7170e-02, -5.5570e-02, -1.4373e-01,\n",
       "                       2.5214e-02,  2.2417e-01, -2.4055e-02,  1.0672e-01,  1.6454e-01,\n",
       "                       6.2929e-02,  2.3017e-02, -5.3930e-02, -5.8135e-02, -3.6312e-02,\n",
       "                      -1.8644e-01,  3.0378e-02, -1.6387e-01,  2.0221e-01,  2.1450e-01,\n",
       "                      -1.9861e-01, -2.4215e-01,  1.1857e-02, -3.9769e-02, -2.1909e-01,\n",
       "                      -3.7703e-02,  2.2404e-01, -5.5055e-02, -1.7292e-01,  4.8821e-02,\n",
       "                       2.6197e-01,  8.4376e-02,  2.3433e-01,  7.5634e-02,  1.4287e-01,\n",
       "                      -1.6002e-01, -1.8831e-01,  3.7066e-02,  1.8506e-01,  2.1706e-01,\n",
       "                      -1.3713e-01, -1.9375e-01, -2.1237e-01,  2.2192e-01, -8.6829e-01,\n",
       "                       5.1100e-01, -2.6233e-02,  1.1289e-01,  8.6830e-01, -1.7566e-01,\n",
       "                      -1.4852e-01, -1.2854e-01,  2.2328e-02, -4.0803e-02,  4.9779e-02,\n",
       "                       1.4437e-01, -6.5988e-02, -1.8204e-01, -2.1866e-02,  8.6366e-02,\n",
       "                      -9.7690e-02, -1.8438e-01, -2.1894e-01, -2.0423e-01,  2.1126e-01,\n",
       "                       9.6984e-02,  1.5287e-03, -1.9288e-01, -1.2115e-01,  2.1240e-01,\n",
       "                      -1.8034e-01, -3.0323e-02, -1.9980e-01, -8.6068e-03,  1.6484e-02,\n",
       "                       2.2883e-02, -9.2547e-02, -2.2071e-01, -2.1768e-02,  3.4451e-03,\n",
       "                       2.3099e-02, -8.0990e-02,  2.5996e-01,  5.5802e-03, -5.4400e-02,\n",
       "                      -6.8189e-02, -6.1851e-02, -8.2935e-01,  6.6587e-02, -2.0358e-01,\n",
       "                      -1.7801e-01, -1.5644e-01,  1.9374e-01,  8.2032e-02,  8.8227e-02,\n",
       "                      -1.9096e-01, -3.3240e-02, -1.5954e-02, -5.4815e-02, -8.4673e-01,\n",
       "                       8.4593e-03,  1.7374e-01, -2.8402e-02,  2.1247e-01, -2.4067e-01,\n",
       "                      -2.8083e-02, -2.1098e-01, -1.0361e-01,  2.0927e-01, -1.1722e-01,\n",
       "                      -1.0127e+00, -2.3325e-01,  6.9542e-02, -2.0685e-01, -8.4890e-01,\n",
       "                      -2.0750e-01,  7.0214e-01, -1.4581e-01,  1.2730e-01,  7.9293e-02,\n",
       "                      -1.3389e-01, -6.7683e-02,  2.1451e-01,  1.1282e-01,  1.2882e-01,\n",
       "                       2.0363e-03,  2.0448e-02, -7.9155e-02, -1.8852e-01, -3.1872e-02,\n",
       "                      -1.5068e-01, -3.4067e-02,  1.4565e-01, -5.4356e-02, -2.3393e-01,\n",
       "                       1.8113e-01,  1.7798e-01,  1.7332e-02, -4.9324e-02, -2.3112e-01,\n",
       "                      -7.0267e-01, -1.8549e-01, -2.3346e-01,  1.5099e-01,  2.5205e-02,\n",
       "                      -1.5228e-01, -2.2273e-01, -1.4085e-03, -7.4937e-02,  3.2062e-02,\n",
       "                      -2.3545e-01,  4.5386e-02, -1.9722e-01,  9.3750e-02,  1.4677e-01,\n",
       "                       1.1251e-01, -1.9535e-01,  2.2547e-01,  2.3563e-01, -5.1283e-02,\n",
       "                      -1.5940e-01,  2.1680e-03, -5.3645e-02,  6.5027e-02, -1.2144e-01,\n",
       "                       3.8158e-02, -6.0991e-02, -2.8410e-01, -1.2711e-02,  3.6123e-02,\n",
       "                       1.6706e-01,  1.7661e-01,  4.8012e-02,  1.9931e-01,  2.0628e-01,\n",
       "                      -1.9735e-01,  2.3865e-01, -1.9454e-01, -5.9874e-02, -1.8458e-01,\n",
       "                      -3.4680e-02,  2.0318e-01,  4.6546e-02,  1.6137e-01, -4.4959e-02,\n",
       "                      -8.9187e-02,  2.3661e-01, -1.5934e-01, -1.5548e-01, -7.9768e-02,\n",
       "                       1.0355e-01,  6.0239e-01,  1.7176e-02,  2.1728e-01, -1.7895e-01,\n",
       "                       2.2277e-01, -6.5211e-02,  2.2224e-01,  1.5362e-01, -1.0200e-01,\n",
       "                       2.2798e-01, -8.8515e-02, -1.7705e-01,  2.2302e-01, -7.7542e-01,\n",
       "                      -8.9125e-03,  1.8019e-01,  2.2375e-01, -3.4095e-02, -1.4251e-01,\n",
       "                      -1.8385e-02, -2.5480e-03, -1.8078e-01,  2.1791e-01,  7.9920e-02,\n",
       "                       1.4311e-01,  1.3332e-01, -5.6402e-01, -2.1073e-01,  8.4590e-03,\n",
       "                       6.4033e-02, -2.1573e-01,  3.8404e-02,  2.2677e-01,  6.9549e-02,\n",
       "                      -4.7494e-02,  2.1068e-01, -1.9438e-01,  1.9043e-02, -1.1644e-01,\n",
       "                      -1.3103e-01, -6.5226e-02, -7.7177e-03, -7.0410e-02, -5.4039e-02,\n",
       "                       2.0781e-01,  2.2087e-01, -1.8813e-02,  2.2712e-01, -2.9988e-02,\n",
       "                      -7.3083e-02, -3.3190e-02, -2.7827e-02, -2.9269e-02, -1.9805e-01,\n",
       "                      -1.7034e-01,  2.5142e-02, -6.7265e-01, -2.0775e-01, -6.9281e-01,\n",
       "                       2.4651e-01, -1.3018e-01,  8.4032e-01,  1.2339e-02,  6.3751e-02,\n",
       "                      -1.0739e-01, -1.5556e-01,  2.2922e-01,  2.0472e-01,  2.3465e-01,\n",
       "                       3.7555e-01, -2.0153e-01,  8.9292e-02,  2.0193e-01, -1.5704e-01,\n",
       "                      -2.2885e-01, -1.7145e-01,  2.2030e-01, -2.6890e-01, -9.3723e-02,\n",
       "                       1.6450e-01, -5.6110e-02, -1.1054e-01,  2.6187e-02,  6.7356e-02,\n",
       "                      -8.9006e-02, -9.0343e-02,  1.6389e-01,  2.2206e-01,  2.1508e-01,\n",
       "                      -9.1261e-01,  1.5970e-02,  1.0216e+00,  5.0977e-02, -2.1215e-01,\n",
       "                       5.9036e-02,  1.6797e-01, -2.1758e-01,  9.8242e-02,  5.1870e-02,\n",
       "                      -9.3784e-02, -2.2232e-01, -3.2554e-01,  2.4416e-01,  2.1141e-01,\n",
       "                       2.7280e-02, -8.1194e-02,  4.7477e-02,  1.9443e-01, -2.1540e-01,\n",
       "                      -8.6523e-02,  1.9601e-01, -6.6071e-01, -6.5788e-02, -5.0005e-02,\n",
       "                       2.2691e-01,  4.5891e-01,  7.7459e-02,  2.1074e-01,  8.7484e-02,\n",
       "                      -3.5449e-02,  1.3009e-02, -5.7669e-01, -1.7707e-01, -6.5891e-02,\n",
       "                      -1.5022e-01,  7.3973e-02, -5.0612e-01, -1.7524e-01,  2.4981e-01,\n",
       "                       2.0197e-01, -5.6089e-02,  8.8136e-01,  2.1320e-01, -1.9774e-01,\n",
       "                       2.1831e-01, -2.1016e-01,  2.3982e-01,  1.9452e-01,  5.4255e-02,\n",
       "                       1.9644e-01,  2.0526e-01,  2.3115e-02, -2.2691e-01,  1.9450e-01,\n",
       "                       7.4661e-01, -1.2596e-01, -1.7988e-01,  5.1297e-02,  6.7613e-02,\n",
       "                       1.0897e+00,  2.2104e-01, -1.5606e-01,  1.6358e-01, -5.8148e-02,\n",
       "                      -7.2629e-02, -1.8532e-01,  4.8302e-02, -4.0832e-02, -1.1861e-01,\n",
       "                      -3.1749e-02, -1.7679e-01,  1.2631e-01, -2.1445e-01, -3.9119e-02,\n",
       "                      -4.2569e-02,  2.2948e-01, -1.6536e-01,  7.7186e-02, -2.1916e-01,\n",
       "                      -1.9833e-01,  1.8297e-01,  3.7427e-02, -1.6497e-01,  2.5876e-01,\n",
       "                       9.2433e-03,  2.8867e-02,  6.6028e-02, -2.0055e-01, -1.9256e-01,\n",
       "                      -2.1100e-01, -2.3013e-01, -2.6969e-02, -1.0517e-01,  2.0794e-01,\n",
       "                      -5.9939e-02, -1.5308e-01, -1.8348e-01,  2.3205e-01, -3.8828e-02,\n",
       "                      -1.5398e-01, -1.6937e-01], device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm3.gamma',\n",
       "              tensor([0.6375, 0.7617, 0.5365, 0.4187, 0.0661, 0.1543, 0.4941, 0.5025, 0.4560,\n",
       "                      0.4781, 0.7843, 0.4351, 0.7303, 0.4953, 0.7336, 0.7291, 0.4946, 0.1678,\n",
       "                      0.7112, 0.4573, 0.0683, 0.5364, 0.6271, 0.5110, 0.1651, 0.0837, 0.5093,\n",
       "                      0.3102, 0.6675, 0.4205, 0.6393, 0.2242, 0.1835, 0.6813, 0.0776, 0.7749,\n",
       "                      0.0859, 0.7506, 0.4689, 0.5480, 0.3526, 0.0745, 0.4040, 0.6306, 0.1167,\n",
       "                      0.6147, 0.5269, 0.5022, 0.6034, 0.5231, 0.5727, 0.7170, 0.5512, 0.3155,\n",
       "                      0.6012, 0.5391, 0.5389, 0.4640, 0.4267, 0.4593, 0.6838, 0.3032, 0.4463,\n",
       "                      0.3301, 0.5517, 0.7431, 0.7296, 0.3648, 0.5326, 0.3924, 0.2661, 0.2954,\n",
       "                      0.1996, 0.4426, 0.3687, 0.4560, 0.6695, 0.1385, 0.5850, 0.5181, 0.5860,\n",
       "                      0.7496, 0.5209, 0.1418, 0.7372, 0.5670, 0.4258, 0.4781, 0.2975, 0.4970,\n",
       "                      0.6935, 0.5325, 0.4443, 0.5844, 0.4139, 0.2622, 0.6457, 0.7199, 0.6249,\n",
       "                      0.7411, 0.6538, 0.0740, 0.0880, 0.7241, 0.3809, 0.5305, 0.6641, 0.6512,\n",
       "                      0.2283, 0.6305, 0.4731, 0.3558, 0.5319, 0.1709, 0.5546, 0.0949, 0.4544,\n",
       "                      0.1049, 0.5172, 0.7421, 0.0548, 0.3958, 0.1393, 0.5202, 0.5057, 0.4905,\n",
       "                      0.0960, 0.4527, 0.6396, 0.3205, 0.7358, 0.4124, 0.3804, 0.4453, 0.6496,\n",
       "                      0.5349, 0.7231, 0.3376, 0.0427, 0.4319, 0.0119, 0.4292, 0.4857, 0.3315,\n",
       "                      0.5259, 0.7250, 0.4887, 0.6596, 0.4037, 0.4315, 0.6385, 0.5522, 0.4011,\n",
       "                      0.4271, 0.4941, 0.1171, 0.5385, 0.5172, 0.6073, 0.0653, 0.2732, 0.1613,\n",
       "                      0.3467, 0.5722, 0.4360, 0.1149, 0.5991, 0.5551, 0.7374, 0.6729, 0.1721,\n",
       "                      0.5510, 0.4462, 0.3472, 0.0899, 0.0867, 0.5195, 0.7646, 0.0792, 0.6788,\n",
       "                      0.6389, 0.0641, 0.5267, 0.5220, 0.6724, 0.7276, 0.5474, 0.5119, 0.0765,\n",
       "                      0.6859, 0.2384, 0.7111, 0.5028, 0.1481, 0.7588, 0.3024, 0.6113, 0.3648,\n",
       "                      0.2920, 0.7791, 0.6942, 0.5470, 0.5335, 0.2948, 0.3814, 0.3691, 0.1401,\n",
       "                      0.6324, 0.0941, 0.0422, 0.4859, 0.4926, 0.3548, 0.5557, 0.5309, 0.0834,\n",
       "                      0.4535, 0.0760, 0.3407, 0.6833, 0.6466, 0.6519, 0.6228, 0.4369, 0.1107,\n",
       "                      0.5090, 0.2971, 0.6564, 0.4886, 0.0770, 0.4243, 0.6745, 0.5290, 0.7559,\n",
       "                      0.7217, 0.3697, 0.6954, 0.6747, 0.7350, 0.5959, 0.0754, 0.4847, 0.3001,\n",
       "                      0.6651, 0.0729, 0.6348, 0.0746, 0.1654, 0.6996, 0.6790, 0.1654, 0.4864,\n",
       "                      0.7205, 0.5020, 0.6875, 0.3755, 0.2775, 0.4769, 0.6116, 0.5319, 0.0763,\n",
       "                      0.6471, 0.6805, 0.6134, 0.6573, 0.6183, 0.7069, 0.1328, 0.4673, 0.5894,\n",
       "                      0.0672, 0.3865, 0.4008, 0.6166, 0.4097, 0.2095, 0.6164, 0.3708, 0.4049,\n",
       "                      0.5446, 0.6145, 0.3915, 0.7047, 0.7088, 0.6661, 0.7024, 0.4379, 0.6477,\n",
       "                      0.3342, 0.3962, 0.6716, 0.3933, 0.4298, 0.4325, 0.6635, 0.6002, 0.3612,\n",
       "                      0.4752, 0.4778, 0.6245, 0.7243, 0.6831, 0.4079, 0.7025, 0.7464, 0.7063,\n",
       "                      0.3976, 0.5174, 0.5177, 0.7280, 0.5009, 0.6920, 0.5826, 0.6550, 0.5098,\n",
       "                      0.2151, 0.4832, 0.6830, 0.4373, 0.4947, 0.7639, 0.0728, 0.4468, 0.5319,\n",
       "                      0.0920, 0.2336, 0.4427, 0.4681, 0.5801, 0.3626, 0.7751, 0.4111, 0.4471,\n",
       "                      0.7526, 0.0817, 0.0871, 0.6975, 0.4330, 0.5307, 0.1462, 0.3503, 0.3886,\n",
       "                      0.2149, 0.7656, 0.1528, 0.6462, 0.4863, 0.0629, 0.5293, 0.2969, 0.5032,\n",
       "                      0.5926, 0.4913, 0.7492, 0.3218, 0.4198, 0.4553, 0.7202, 0.4580, 0.1514,\n",
       "                      0.1709, 0.3957, 0.0338, 0.5035, 0.4534, 0.5456, 0.6414, 0.4950, 0.6675,\n",
       "                      0.5301, 0.4301, 0.3957, 0.0861, 0.4845, 0.6631, 0.4350, 0.3092, 0.0897,\n",
       "                      0.7129, 0.0712, 0.5566, 0.1082, 0.0741, 0.4288, 0.4411, 0.5422, 0.3473,\n",
       "                      0.7627, 0.5856, 0.4762, 0.0649, 0.3043, 0.5042, 0.7419, 0.5615, 0.1548,\n",
       "                      0.2844, 0.4620, 0.6616, 0.2724, 0.7033, 0.4513, 0.0892, 0.1299, 0.1858,\n",
       "                      0.0788, 0.3150, 0.4725, 0.7494, 0.4788, 0.0819, 0.4617, 0.0707, 0.6441,\n",
       "                      0.6906, 0.2634, 0.6720, 0.3475, 0.6922, 0.3030, 0.1986, 0.5318, 0.0591,\n",
       "                      0.7281, 0.6944, 0.4969, 0.2797, 0.4180, 0.7327, 0.6378, 0.1447, 0.4904,\n",
       "                      0.0646, 0.3085, 0.4749, 0.7407, 0.0265, 0.4133, 0.7259, 0.2860, 0.4691,\n",
       "                      0.4782, 0.0981, 0.6856, 0.3855, 0.6621, 0.2032, 0.0647, 0.3235, 0.6919,\n",
       "                      0.7412, 0.5496, 0.0779, 0.7427, 0.5240, 0.5221, 0.3839, 0.5827, 0.4424,\n",
       "                      0.2598, 0.7493, 0.7712, 0.3426, 0.5327, 0.7601, 0.0926, 0.3015, 0.6907,\n",
       "                      0.5285, 0.2241, 0.0730, 0.7696, 0.6315, 0.6817, 0.3330, 0.5240, 0.6830,\n",
       "                      0.4247, 0.5426, 0.3789, 0.5040, 0.6987, 0.4684, 0.6340, 0.5507, 0.3161,\n",
       "                      0.6888, 0.2327, 0.3573, 0.4983, 0.6598, 0.5347, 0.4021, 0.6307, 0.3532,\n",
       "                      0.3645, 0.4832, 0.1675, 0.3773, 0.6791, 0.4845, 0.1514, 0.5214, 0.2514,\n",
       "                      0.5967, 0.3014, 0.6592, 0.1528, 0.5761, 0.4907, 0.1641, 0.6813],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.layers.5.layer_norm3.beta',\n",
       "              tensor([-6.9910e-02, -1.2577e-02, -8.3990e-03, -2.2847e-03,  2.0121e-01,\n",
       "                       2.0924e-01, -8.1766e-03, -1.9697e-02, -1.4503e-02, -5.9055e-02,\n",
       "                      -7.7055e-02,  3.7942e-03, -3.9799e-02,  1.5763e-02, -1.1434e-01,\n",
       "                      -5.3779e-02, -3.4499e-02, -1.6955e-01, -6.1558e-03, -9.0077e-03,\n",
       "                       2.2460e-01, -1.4197e-01,  2.4322e-02, -3.0210e-02, -7.8527e-02,\n",
       "                      -3.8563e-01, -1.3669e-01,  8.2729e-03, -1.5179e-02,  1.2493e-02,\n",
       "                      -8.0367e-02,  2.5255e-02,  6.2409e-02, -8.0538e-02, -3.9135e-01,\n",
       "                      -7.6219e-02,  2.8799e-01, -6.3170e-02,  3.5549e-03, -4.4793e-02,\n",
       "                      -2.9916e-02, -3.0719e-01, -2.5450e-02, -7.2543e-02, -1.5551e-01,\n",
       "                      -6.7747e-02, -3.5089e-02, -2.8860e-02,  2.3233e-02, -3.9992e-02,\n",
       "                      -2.4763e-02,  3.4588e-02, -3.5240e-02, -4.0115e-02, -7.7234e-03,\n",
       "                      -4.4658e-02, -9.1558e-03, -8.1950e-02, -7.6033e-03, -3.1511e-02,\n",
       "                      -1.1101e-01, -1.7341e-02, -2.3970e-02, -1.1274e-01, -3.4381e-02,\n",
       "                      -5.7640e-02, -5.7711e-02, -2.2378e-02, -1.2309e-02, -1.7318e-02,\n",
       "                      -6.5963e-02, -2.2976e-02,  2.8503e-02, -2.8583e-02, -1.3912e-02,\n",
       "                      -2.2897e-02, -2.7236e-02, -2.0845e-02, -2.2528e-02,  6.2942e-02,\n",
       "                      -1.4195e-02, -7.1233e-02, -4.3931e-02, -1.7380e-01, -4.9659e-02,\n",
       "                      -9.8024e-03, -3.1152e-02, -1.0298e-01,  9.9914e-03, -2.7633e-02,\n",
       "                      -1.4718e-02, -4.8512e-02, -1.1829e-02, -3.0946e-02, -2.7361e-02,\n",
       "                      -2.9396e-02, -1.3434e-02, -1.0941e-02, -3.6604e-02, -6.7033e-02,\n",
       "                      -5.7457e-02,  1.0814e-01,  3.6905e-01, -9.1454e-03, -3.7940e-02,\n",
       "                      -2.9120e-02,  1.9141e-02, -6.8543e-03, -3.3573e-02, -3.3066e-02,\n",
       "                      -2.8484e-02, -1.4169e-01, -2.3215e-02,  3.7120e-02, -2.6820e-02,\n",
       "                      -2.5482e-01,  1.1524e-01,  1.0614e-01, -4.9276e-02, -7.6760e-02,\n",
       "                      -1.3615e-01, -2.8138e-02,  1.8734e-01, -7.4996e-02, -2.7248e-02,\n",
       "                      -2.0248e-02,  1.8181e-01, -1.4417e-02, -1.3203e-01, -4.0647e-02,\n",
       "                      -4.8562e-02, -2.1948e-02, -4.5825e-02, -5.2990e-02,  1.8111e-02,\n",
       "                      -1.0829e-02, -4.4613e-02, -5.1255e-02, -3.5994e-01, -4.8854e-02,\n",
       "                      -2.4504e-01, -8.5833e-02, -5.7909e-02, -1.9098e-02, -3.6875e-02,\n",
       "                       1.2673e-02, -1.2919e-02,  2.3987e-02, -2.8911e-02, -2.8199e-02,\n",
       "                      -2.1064e-02, -4.1752e-02, -6.2240e-02,  4.6937e-03, -1.9283e-02,\n",
       "                       1.3228e-01, -2.9412e-02, -5.6048e-02, -1.9096e-02, -1.1828e-02,\n",
       "                      -4.5787e-02,  4.8705e-02, -4.0814e-02,  5.3400e-03, -1.7285e-01,\n",
       "                      -1.4276e-01, -4.1275e-02, -1.0456e-02, -9.2495e-02, -5.5716e-02,\n",
       "                      -1.2221e-01, -3.0432e-02, -2.5353e-02, -2.4282e-02,  4.3342e-01,\n",
       "                      -2.5739e-01, -3.9202e-02, -5.5085e-02, -3.9325e-01, -1.6736e-02,\n",
       "                       2.4837e-02, -1.3784e-01, -7.0033e-03, -3.7278e-02, -3.9693e-02,\n",
       "                      -6.9218e-02, -4.3221e-02, -5.1593e-02,  7.4584e-02, -5.4020e-02,\n",
       "                      -1.6210e-01, -8.0965e-03, -2.8677e-02,  3.9527e-01, -7.0255e-02,\n",
       "                      -2.0709e-02, -1.0814e-02, -2.3233e-01, -4.8515e-02, -5.6829e-02,\n",
       "                       2.3327e-02, -5.1708e-02, -2.7273e-02, -9.1251e-02, -1.7060e-02,\n",
       "                      -1.4883e-02, -8.2065e-02, -1.8487e-02,  7.0773e-02, -3.0068e-01,\n",
       "                      -7.0995e-03, -4.3877e-02, -4.0015e-02, -1.3482e-03, -4.1678e-02,\n",
       "                       8.7214e-02, -2.8275e-02,  2.8404e-01, -1.4754e-02, -1.1526e-02,\n",
       "                       1.2467e-02,  2.1563e-03, -2.6975e-02, -4.8299e-02,  5.8793e-02,\n",
       "                      -4.3619e-02, -1.2299e-01,  5.9216e-02, -3.2194e-02,  2.9030e-01,\n",
       "                      -7.4800e-02, -1.0224e-01, -4.0534e-02, -3.8317e-02, -3.2440e-03,\n",
       "                      -1.7210e-01, -5.7444e-02, -5.4673e-02, -5.4161e-02,  2.4632e-02,\n",
       "                       3.4790e-01, -3.6877e-02,  5.1056e-03, -1.8201e-02,  3.2835e-01,\n",
       "                      -1.7900e-02, -3.1394e-01, -7.5166e-02, -8.8916e-02, -4.8547e-02,\n",
       "                      -1.3645e-01, -4.7405e-02, -5.6109e-02, -2.9643e-02, -6.5721e-02,\n",
       "                      -6.9146e-03, -8.5411e-02, -4.6251e-02, -1.9333e-02, -4.6346e-02,\n",
       "                      -1.5033e-01, -5.6864e-02, -1.0186e-01, -6.1796e-02, -1.4833e-02,\n",
       "                       2.1620e-01, -4.6720e-02,  8.3850e-02, -2.9214e-02, -3.8758e-02,\n",
       "                       2.4615e-01, -1.6783e-02, -2.2302e-02, -9.3034e-02, -1.6195e-02,\n",
       "                      -5.8731e-02, -3.7505e-02, -1.6946e-01, -2.5604e-02, -8.2529e-03,\n",
       "                      -4.9315e-02, -2.8871e-02, -4.3450e-03, -4.6648e-02, -1.0833e-01,\n",
       "                      -5.1602e-02, -2.4153e-02, -5.5291e-02, -2.7055e-02,  6.2188e-02,\n",
       "                      -9.7037e-03, -2.6577e-03, -3.8934e-02,  7.6198e-03,  2.3988e-02,\n",
       "                      -9.3109e-03, -1.7857e-02, -1.8847e-01, -5.4081e-02, -8.0813e-02,\n",
       "                      -6.8471e-02, -7.2888e-02, -3.1537e-02, -9.5446e-02, -6.2462e-02,\n",
       "                      -5.5325e-02, -3.0601e-02, -3.0889e-02, -4.0901e-02,  3.1921e-02,\n",
       "                      -4.3502e-02, -5.7026e-02, -7.2050e-03, -1.2420e-01, -3.7279e-02,\n",
       "                      -2.0438e-01, -3.8035e-02,  3.8950e-04, -2.3413e-01, -4.2838e-02,\n",
       "                      -4.4072e-02, -3.2924e-01, -2.6912e-02, -2.8280e-02,  1.5068e-01,\n",
       "                      -3.2712e-02, -3.2476e-02, -3.1234e-02, -9.4932e-02,  9.5324e-02,\n",
       "                      -7.2681e-02, -2.4197e-02, -3.5352e-02, -5.8212e-02,  3.3060e-01,\n",
       "                       7.0074e-02, -9.5449e-02, -4.6595e-02, -6.1117e-02,  2.7409e-01,\n",
       "                      -5.0708e-02, -1.8451e-03, -6.8141e-02, -5.8322e-02,  4.5738e-02,\n",
       "                      -1.1943e-01,  1.7607e-01,  1.7982e-01, -1.1320e-01, -9.5018e-02,\n",
       "                      -1.3430e-02, -2.0140e-02, -2.2111e-02, -6.1609e-02,  3.0488e-03,\n",
       "                       2.5812e-02, -2.5899e-02, -5.9171e-03, -1.4700e-02,  2.5838e-01,\n",
       "                      -7.0438e-02, -4.0102e-02, -1.0904e-01, -3.7936e-02, -4.0807e-02,\n",
       "                      -4.7419e-02, -2.1835e-02,  1.7024e-03, -2.1658e-02, -5.1567e-02,\n",
       "                      -1.8982e-02, -4.1927e-02,  8.5632e-02, -7.2820e-02, -1.5479e-02,\n",
       "                      -4.4228e-03, -1.9648e-02,  3.4356e-01, -4.8141e-03,  2.4109e-01,\n",
       "                      -5.1685e-02, -1.8479e-01, -3.7306e-01, -1.6912e-03, -1.8161e-02,\n",
       "                       5.6402e-02, -1.5721e-02, -7.7478e-02, -2.8600e-02, -2.7379e-02,\n",
       "                      -1.7514e-01, -2.3883e-02, -2.4605e-02, -5.0075e-02, -3.8169e-02,\n",
       "                       3.3162e-01, -1.7384e-02, -3.3539e-02, -1.8813e-01,  1.2418e-01,\n",
       "                      -8.3462e-02, -2.9993e-02, -2.4878e-01,  6.2723e-02,  4.4518e-02,\n",
       "                      -2.4284e-01, -4.0986e-02, -1.7053e-01, -6.0168e-02, -2.4227e-02,\n",
       "                       3.8254e-01, -6.0165e-03, -4.1621e-01, -8.2988e-02, -4.8227e-02,\n",
       "                       1.3267e-02, -1.1106e-01, -1.6861e-02, -4.1084e-02, -3.1318e-02,\n",
       "                      -8.3689e-02, -2.1694e-02,  1.2753e-01, -6.2267e-02, -5.1585e-02,\n",
       "                      -1.3025e-02, -5.6785e-02, -1.9804e-02, -6.3684e-02, -6.0903e-02,\n",
       "                       2.1158e-01, -1.1044e-02,  2.1018e-01, -1.2706e-02, -3.8592e-02,\n",
       "                      -5.0392e-02, -2.1588e-02, -1.6666e-01, -1.1690e-01, -2.9778e-02,\n",
       "                      -5.6878e-02, -1.0550e-02,  4.0882e-01,  1.1353e-02, -4.1555e-02,\n",
       "                      -7.7162e-03,  2.5519e-02,  1.6524e-01, -1.7358e-02, -6.3310e-02,\n",
       "                      -5.9343e-02, -4.7053e-02, -3.6106e-01, -5.7632e-02, -3.1946e-02,\n",
       "                      -3.7003e-02, -2.2036e-02, -2.5042e-02, -2.5754e-02, -1.4709e-02,\n",
       "                      -7.7702e-02, -6.4854e-02, -1.2882e-02, -2.1159e-02, -5.4533e-02,\n",
       "                      -4.1745e-01, -4.9224e-02,  2.1478e-02, -9.8426e-03,  2.1918e-02,\n",
       "                      -4.1793e-01, -4.8243e-02,  1.0298e-01, -1.0098e-01,  9.4054e-02,\n",
       "                      -4.7859e-02,  4.8504e-03, -2.9531e-03, -1.5998e-02, -4.3627e-02,\n",
       "                      -5.5010e-02,  7.9932e-03,  8.8389e-02, -1.9838e-02, -4.3646e-02,\n",
       "                      -5.0837e-02, -5.3289e-02, -6.3410e-02, -4.3798e-02, -1.9299e-02,\n",
       "                      -4.5141e-02, -3.0543e-02, -3.5698e-02, -3.3913e-03, -4.0753e-02,\n",
       "                      -3.0075e-02, -9.4846e-03,  5.0328e-02, -1.4561e-02, -1.3521e-02,\n",
       "                      -3.0846e-02,  3.7965e-01, -3.5498e-02, -5.5412e-02, -2.8678e-02,\n",
       "                      -1.3582e-02,  1.1611e-02,  3.6143e-01, -2.7891e-02, -6.0541e-02,\n",
       "                      -8.6496e-02,  9.7985e-02], device='cuda:0')),\n",
       "             ('decoder.linear.weight',\n",
       "              tensor([[ 0.1693,  0.0725,  0.0122,  ..., -0.0572, -0.0461, -0.1013],\n",
       "                      [ 0.4939,  0.0909, -0.3539,  ..., -0.3913,  2.8039, -0.1372],\n",
       "                      [ 0.4463, -0.4067, -0.4302,  ...,  0.1810,  1.1056, -0.1184],\n",
       "                      ...,\n",
       "                      [ 0.5858, -0.4753, -0.5066,  ...,  0.1878,  1.3510,  0.0151],\n",
       "                      [ 0.6012, -0.5569, -0.4687,  ...,  0.2303,  1.4147,  0.2818],\n",
       "                      [ 0.6465, -0.5146, -0.6257,  ...,  0.1807,  1.4165,  0.1256]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.linear.bias',\n",
       "              tensor([ 0.3751, -7.6235, -2.9287,  ..., -2.3634, -2.3457, -2.2857],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4695f90-eb58-4fb1-81ed-a7ce40576a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a094f4-5871-48df-8d86-00c824a5a046",
   "metadata": {},
   "source": [
    "## Decoding Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2fce07d-9248-49b0-bc17-c00ada5ebe6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greedy_decoding(src, start_symbol):\n",
    "    src_mask = model.make_src_mask(src)\n",
    "    enc_rep = model.encoder(src, src_mask)\n",
    "    \n",
    "    ys = torch.zeros(1, 1).fill_(en_vocab[start_symbol]).type_as(src.data).cuda()\n",
    "    max_len = src.size()[1] + 20 - 2\n",
    "    for i in range(max_len - 1):\n",
    "        trg_mask = model.make_trg_mask(ys)\n",
    "        output = model.decoder(ys, enc_rep, trg_mask, src_mask)\n",
    "        # if i == 0:\n",
    "        # output_idx = torch.randint(len(output[0]), (1,)).item()\n",
    "        # else:\n",
    "        output_idx = output[0].max(dim=1)[1][-1]\n",
    "        if output_idx == en_vocab['<eos>']:\n",
    "            break\n",
    "        else:\n",
    "            ys = torch.cat([ys, torch.zeros(1, 1).type_as(src.data).fill_(output_idx)], dim=1)\n",
    "        \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c75e40-bedf-414e-bfb7-b016681577cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SOURCE TEXT ###\n",
      "Eine Person in einer rosa Jacke sitzt auf einer hölzernen Bank . \n",
      "\n",
      "### TARGET TEXT ###\n",
      "An individual wearing rose jacket site idle on a wooden bench . \n",
      "\n",
      "### GENERATED TEXT ###\n",
      "A person in a blue shirt is sitting on a bench in a blue jacket . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 150\n",
    "\n",
    "de_sample = test_data[idx][0]\n",
    "en_sample = test_data[idx][1]\n",
    "src = torch.cat([torch.tensor(de_vocab['<sos>']).reshape(1), de_sample, torch.tensor(de_vocab['<eos>']).reshape(1)], dim=0).unsqueeze(0).cuda()\n",
    "\n",
    "print('### SOURCE TEXT ###')\n",
    "print(idx_to_words(src[0], src_itos))\n",
    "\n",
    "print('### TARGET TEXT ###')\n",
    "print(idx_to_words(en_sample, trg_itos))\n",
    "\n",
    "start_symbol = '<sos>'\n",
    "ys = greedy_decoding(src, start_symbol)\n",
    "print('### GENERATED TEXT ###')\n",
    "print(idx_to_words(ys.squeeze(), trg_itos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251929fa",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1f3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(logits, token_id):\n",
    "    # Compute the softmax of the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # Get the log probability of the token\n",
    "    token_log_probability = log_probabilities[token_id].item()\n",
    "    return token_log_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e69f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def greedy_sampling(logits, beams):\n",
    "    return torch.topk(logits, beams).indices\n",
    "    \n",
    "def beam_search(input_ids, node, bar, length, beams, sampling, enc_rep, src_mask, temperature=0.1):\n",
    "    if length == 0:\n",
    "        return None\n",
    "    \n",
    "    trg_mask = model.make_trg_mask(input_ids)\n",
    "    outputs = model.decoder(input_ids, enc_rep, trg_mask, src_mask)\n",
    "    predictions = outputs\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    if sampling == 'greedy':\n",
    "        top_token_ids = greedy_sampling(logits, beams)\n",
    "    elif sampling == 'top_k':\n",
    "        top_token_ids = top_k_sampling(logits, temperature, 20, beams)\n",
    "    elif sampling == 'nucleus':\n",
    "        top_token_ids = nucleus_sampling(logits, temperature, 0.5, beams)\n",
    "\n",
    "    for j, token_id in enumerate(top_token_ids):\n",
    "        bar.update(1)\n",
    "\n",
    "        # Compute the score of the predicted token\n",
    "        token_score = get_log_prob(logits, token_id)\n",
    "        cumulative_score = graph.nodes[node]['cumscore'] + token_score\n",
    "\n",
    "        # Add the predicted token to the list of input ids\n",
    "        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "        # Add node and edge to graph\n",
    "        token = idx_to_words([token_id], trg_itos)#tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "        current_node = list(graph.successors(node))[j]\n",
    "        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
    "        graph.nodes[current_node]['cumscore'] = cumulative_score\n",
    "        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score\n",
    "        graph.nodes[current_node]['token'] = token + f\"_{length}_{j}\"\n",
    "\n",
    "        # Recursive call\n",
    "        beam_search(new_input_ids, current_node, bar, length-1, beams, sampling, enc_rep, src_mask, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60e4425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 300\n",
    "\n",
    "de_sample = test_data[idx][0]\n",
    "en_sample = test_data[idx][1]\n",
    "src = torch.cat([torch.tensor(de_vocab['<sos>']).reshape(1), de_sample, torch.tensor(de_vocab['<eos>']).reshape(1)], dim=0).unsqueeze(0).cuda()\n",
    "\n",
    "src_mask = model.make_src_mask(src)\n",
    "enc_rep = model.encoder(src, src_mask)\n",
    "\n",
    "start_symbol = '<sos>'\n",
    "ys = torch.zeros(1, 1).fill_(en_vocab[start_symbol]).type_as(src.data).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae50963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SOURCE TEXT ###\n",
      "Ein Junger Mann rutscht mit dem Skateboard über ein rosa Geländer . \n",
      "\n",
      "### TARGET TEXT ###\n",
      "A young man skateboards off a pink railing . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('### SOURCE TEXT ###')\n",
    "print(idx_to_words(src[0], src_itos))\n",
    "\n",
    "print('### TARGET TEXT ###')\n",
    "print(idx_to_words(en_sample, trg_itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f727c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18888a8ef0d489ab24923c84b6b1c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "length = 10\n",
    "beams = 2\n",
    "\n",
    "# Create a balanced tree with height 'length' and branching factor 'k'\n",
    "graph = nx.balanced_tree(beams, length, create_using=nx.DiGraph())\n",
    "bar = tqdm(total=len(graph.nodes))\n",
    "\n",
    "# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\n",
    "for node in graph.nodes:\n",
    "    graph.nodes[node]['tokenscore'] = 100\n",
    "    graph.nodes[node]['cumscore'] = 0\n",
    "    graph.nodes[node]['sequencescore'] = 0\n",
    "    graph.nodes[node]['token'] = '<sos>'\n",
    "\n",
    "# Start generating text\n",
    "beam_search(ys, 0, bar, length, beams, 'greedy', enc_rep, src_mask, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a5edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  A young man is jumping on a skateboard with his\n"
     ]
    }
   ],
   "source": [
    "def get_best_sequence(G):\n",
    "    # Create a list of leaf nodes\n",
    "    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]\n",
    "\n",
    "    # Get the leaf node with the highest cumscore\n",
    "    max_score_node = None\n",
    "    max_score = float('-inf')\n",
    "    for node in leaf_nodes:\n",
    "        if G.nodes[node]['sequencescore'] > max_score:\n",
    "            max_score = G.nodes[node]['sequencescore']\n",
    "            max_score_node = node\n",
    "\n",
    "    # Retrieve the sequence of nodes from this leaf node to the root node in a list\n",
    "    path = nx.shortest_path(G, source=0, target=max_score_node)\n",
    "\n",
    "    # Return the string of token attributes of this sequence\n",
    "    sequence = \" \".join([G.nodes[node]['token'].split('_')[0] for node in path])\n",
    "    \n",
    "    return sequence, max_score\n",
    "\n",
    "sequence, max_score = get_best_sequence(graph)\n",
    "print(f\"Generated text: {sequence[5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b66776-13a2-4a03-a109-da60ec5041e5",
   "metadata": {},
   "source": [
    "## Attention Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0ec460e-8bff-4385-8a43-bbce62f4c926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af97e1dd-326a-4926-be85-983164c7f4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def idx_to_words_new(x, itos):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        word = itos[i.item()]\n",
    "        words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bac793cc-53da-412c-b513-0828ebb9be6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = sample[0].T.cuda()\n",
    "trg = sample[1].T.cuda()\n",
    "\n",
    "output = model(src, trg[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04660818-0778-4da1-a4a3-edf0d9c1cd95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_labels = idx_to_words_new(src.squeeze(), src_itos)\n",
    "target_labels = idx_to_words_new(trg.squeeze(), trg_itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "692a9e1b-6962-4721-917e-e2f7e0efca70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "att = model.decoder.layers[4].enc_dec_attention.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2d38d9f-4780-4db3-bf92-53be4842078a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "472f2137-ddd2-4376-ab60-1e8c2fcd4dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(att.squeeze()[0].cpu().detach().numpy(), cmap=plt.cm.Blues)\n",
    "\n",
    "ax.set_xticks(np.arange(att.squeeze()[0].shape[1]) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(att.squeeze()[0].shape[0]) + 0.5, minor=False)\n",
    "\n",
    "ax.set_xlim(0, int(att.squeeze()[0].shape[1]))\n",
    "ax.set_ylim(0, int(att.squeeze()[0].shape[0]))\n",
    "\n",
    "source_labels = idx_to_words_new(src.squeeze(), src_itos)\n",
    "target_labels = idx_to_words_new(trg.squeeze(), trg_itos)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# source words -> column labels\n",
    "ax.set_xticklabels(source_labels, minor=False)\n",
    "# target words -> row labels\n",
    "ax.set_yticklabels(target_labels[:-1], minor=False)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07c967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "neuro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
